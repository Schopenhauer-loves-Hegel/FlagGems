{
  "batch": 2,
  "threshold_new": 0.8,
  "threshold_existing": 1.2,
  "criterion": "speedup_vs_cuda_and_flaggems",
  "total_gpt_operators": 87,
  "new_operators": {
    "total": 63,
    "selected": 51,
    "operators": {
      "aten::hinge_embedding_loss": {
        "gpt_speedup_vs_cuda": 4.040925402258748,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef hinge_embedding_loss(\n    input_ptr,      # *Pointer* to input tensor.\n    target_ptr,     # *Pointer* to target tensor.\n    output_ptr,     # *Pointer* to output tensor (per-element losses).\n    n_elements,     # Number of elements.\n    margin,         # Margin (float).\n    BLOCK_SIZE: tl.constexpr,  # Number of elements per program.\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(input_ptr + offsets, mask=mask, other=0)\n    t = tl.load(target_ptr + offsets, mask=mask, other=0)\n\n    # Cast to float32 for stable math\n    x_f = tl.cast(x, tl.float32)\n    t_f = tl.cast(t, tl.float32)\n    margin_f = tl.full([1], margin, tl.float32)\n\n    # HingeEmbeddingLoss:\n    # if target == 1:  loss = input\n    # if target == -1: loss = max(0, margin - input)\n    # else: 0 (undefined target values)\n    is_pos = t_f == 1.0\n    is_neg = t_f == -1.0\n\n    loss_pos = x_f\n    loss_neg = tl.maximum(0.0, margin_f - x_f)\n\n    loss = tl.where(is_pos, loss_pos, tl.where(is_neg, loss_neg, 0.0))\n\n    # Cast back to input dtype for storage\n    loss_out = tl.cast(loss, x.dtype)\n    tl.store(output_ptr + offsets, loss_out, mask=mask)\n\n\n# Preserve kernel reference before defining the Python wrapper with the same name\n_hinge_embedding_loss_kernel = hinge_embedding_loss\n\n\ndef hinge_embedding_loss(input: torch.Tensor,\n                         target: torch.Tensor,\n                         margin: float = 1.0,\n                         reduction: str | int = \"mean\") -> torch.Tensor:\n    # Fallback for non-CUDA tensors\n    if input.device.type != \"cuda\" or target.device.type != \"cuda\":\n        return torch.ops.aten.hinge_embedding_loss(input, target, margin, reduction)\n\n    # Ensure shapes and dtypes\n    if input.numel() != target.numel():\n        raise ValueError(\"input and target must have the same number of elements\")\n    if not input.is_contiguous():\n        input = input.contiguous()\n    if not target.is_contiguous():\n        target = target.contiguous()\n\n    supported_dtypes = {torch.float16, torch.bfloat16, torch.float32}\n    if input.dtype not in supported_dtypes:\n        # Fallback if dtype unsupported by Triton\n        return torch.ops.aten.hinge_embedding_loss(input, target, margin, reduction)\n\n    # Allocate output buffer for per-element losses\n    out = torch.empty_like(input)\n\n    n_elements = input.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    _hinge_embedding_loss_kernel[grid](\n        input, target, out, n_elements, float(margin),\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Handle reduction\n    red_map = {0: \"none\", 1: \"mean\", 2: \"sum\"}\n    if isinstance(reduction, int):\n        reduction = red_map.get(reduction, \"mean\")\n    reduction = reduction.lower()\n\n    if reduction == \"none\":\n        return out\n    elif reduction == \"mean\":\n        return out.mean()\n    elif reduction == \"sum\":\n        return out.sum()\n    else:\n        raise ValueError(f\"Invalid reduction: {reduction}. Supported: 'none', 'mean', 'sum'.\")",
        "has_code": true
      },
      "aten::soft_margin_loss": {
        "gpt_speedup_vs_cuda": 3.3688466385478795,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _soft_margin_loss_elementwise_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    xf = x.to(tl.float32)\n    yf = y.to(tl.float32)\n    z = -xf * yf\n    absz = tl.abs(z)\n    vals = tl.maximum(z, 0.0) + tl.log(1.0 + tl.exp(-absz))\n\n    tl.store(out_ptr + offsets, vals, mask=mask)\n\n\n@triton.jit\ndef _soft_margin_loss_sum_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    xf = x.to(tl.float32)\n    yf = y.to(tl.float32)\n    z = -xf * yf\n    absz = tl.abs(z)\n    vals = tl.maximum(z, 0.0) + tl.log(1.0 + tl.exp(-absz))\n    vals = tl.where(mask, vals, 0.0)\n\n    acc = tl.sum(vals, axis=0)\n    tl.atomic_add(out_ptr, acc)\n\n\ndef _normalize_reduction(reduction):\n    # Accept both string and enum/int forms: 0=none,1=mean,2=sum\n    if isinstance(reduction, str):\n        r = reduction.lower()\n        if r == \"none\":\n            return 0\n        if r == \"mean\":\n            return 1\n        if r == \"sum\":\n            return 2\n        raise ValueError(f\"Invalid reduction: {reduction}\")\n    if isinstance(reduction, int):\n        if reduction in (0, 1, 2):\n            return reduction\n        raise ValueError(f\"Invalid reduction int: {reduction}\")\n    raise ValueError(f\"Unsupported reduction type: {type(reduction)}\")\n\n\ndef _check_tensors(input: torch.Tensor, target: torch.Tensor):\n    if not (input.is_cuda and target.is_cuda):\n        raise AssertionError(\"soft_margin_loss: input and target must be CUDA tensors for Triton kernel.\")\n    if input.device != target.device:\n        raise AssertionError(\"soft_margin_loss: input and target must be on the same device.\")\n    if input.numel() != target.numel():\n        raise AssertionError(\"soft_margin_loss: input and target must have the same number of elements.\")\n    if not input.is_contiguous():\n        input = input.contiguous()\n    if not target.is_contiguous():\n        target = target.contiguous()\n    return input, target\n\n\ndef soft_margin_loss(input: torch.Tensor, target: torch.Tensor, reduction=\"mean\"):\n    input, target = _check_tensors(input, target)\n    red = _normalize_reduction(reduction)\n    n_elements = input.numel()\n\n    if red == 0:\n        # reduction = 'none'\n        out = torch.empty_like(input)\n        if n_elements == 0:\n            return out\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        _soft_margin_loss_elementwise_kernel[grid](\n            input, target, out, n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n        return out\n    else:\n        # reduction = 'sum' or 'mean' (1=mean, 2=sum)\n        if n_elements == 0:\n            # Follow PyTorch behavior: sum -> 0, mean -> NaN\n            if red == 2:\n                return torch.zeros((), device=input.device, dtype=input.dtype)\n            else:\n                return torch.full((), float(\"nan\"), device=input.device, dtype=input.dtype)\n        tmp_sum = torch.zeros((), device=input.device, dtype=torch.float32)\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        _soft_margin_loss_sum_kernel[grid](\n            input, target, tmp_sum, n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n        if red == 2:\n            # sum\n            return tmp_sum.to(dtype=input.dtype)\n        else:\n            # mean\n            mean_val = (tmp_sum / float(n_elements)).to(dtype=input.dtype)\n            return mean_val\n\n\ndef soft_margin_loss_out(input: torch.Tensor, target: torch.Tensor, reduction=\"mean\", out: torch.Tensor = None):\n    input, target = _check_tensors(input, target)\n    red = _normalize_reduction(reduction)\n    n_elements = input.numel()\n\n    if out is None:\n        # Allocate output based on reduction\n        if red == 0:\n            out = torch.empty_like(input)\n        else:\n            out = torch.empty((), device=input.device, dtype=input.dtype)\n    else:\n        if not out.is_cuda:\n            raise AssertionError(\"soft_margin_loss_out: out must be a CUDA tensor.\")\n        if red == 0:\n            if out.numel() != n_elements:\n                raise AssertionError(\"soft_margin_loss_out: for reduction='none', out must match input shape.\")\n        else:\n            if out.numel() != 1:\n                raise AssertionError(\"soft_margin_loss_out: for reduction='sum' or 'mean', out must be a scalar tensor.\")\n        if out.device != input.device:\n            raise AssertionError(\"soft_margin_loss_out: out must be on the same device as input.\")\n\n    if red == 0:\n        if n_elements > 0:\n            BLOCK_SIZE = 1024\n            grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n            _soft_margin_loss_elementwise_kernel[grid](\n                input, target, out, n_elements, BLOCK_SIZE=BLOCK_SIZE\n            )\n        return out\n    else:\n        if n_elements == 0:\n            if red == 2:\n                out.fill_(0)\n            else:\n                out.fill_(float(\"nan\"))\n            return out\n        tmp_sum = torch.zeros((), device=input.device, dtype=torch.float32)\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        _soft_margin_loss_sum_kernel[grid](\n            input, target, tmp_sum, n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n        if red == 2:\n            out.fill_(tmp_sum.to(dtype=input.dtype))\n        else:\n            mean_val = (tmp_sum / float(n_elements)).to(dtype=input.dtype)\n            out.fill_(mean_val)\n        return out",
        "has_code": true
      },
      "aten::margin_ranking_loss": {
        "gpt_speedup_vs_cuda": 2.2465122412701652,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef margin_ranking_loss(x1_ptr, x2_ptr, target_ptr, out_ptr, n_elements, margin, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x1 = tl.load(x1_ptr + offsets, mask=mask, other=0)\n    x2 = tl.load(x2_ptr + offsets, mask=mask, other=0)\n    y = tl.load(target_ptr + offsets, mask=mask, other=0)\n\n    diff = x1 - x2\n    m = tl.full([BLOCK_SIZE], margin, x1.dtype)\n    val = -y * diff + m\n    zero = tl.zeros([BLOCK_SIZE], dtype=val.dtype)\n    loss = tl.maximum(val, zero)\n\n    tl.store(out_ptr + offsets, loss, mask=mask)\n\n\n# Preserve a handle to the Triton kernel before defining the Python wrapper with the same name.\n_margin_ranking_loss_kernel = margin_ranking_loss\n\n\ndef margin_ranking_loss(*args, **kwargs):\n    # Parse inputs: (input1, input2, target, margin=0.0, reduction='mean')\n    if len(args) < 3 and not all(k in kwargs for k in (\"self\", \"other\", \"target\")):\n        raise TypeError(\"margin_ranking_loss requires at least three positional arguments: input1, input2, target\")\n\n    # Positional extraction\n    if len(args) >= 3:\n        x1, x2, target = args[0], args[1], args[2]\n    else:\n        # Fallback to keyword names similar to ATen signature\n        x1 = kwargs[\"self\"]\n        x2 = kwargs[\"other\"]\n        target = kwargs[\"target\"]\n\n    # margin and reduction extraction\n    margin = 0.0\n    reduction = \"mean\"\n    if len(args) >= 4:\n        margin = args[3]\n    if len(args) >= 5:\n        reduction = args[4]\n    if \"margin\" in kwargs:\n        margin = kwargs[\"margin\"]\n    if \"reduction\" in kwargs:\n        reduction = kwargs[\"reduction\"]\n\n    # Normalize reduction\n    if isinstance(reduction, int):\n        reduction = {0: \"none\", 1: \"mean\", 2: \"sum\"}.get(reduction, \"mean\")\n    if reduction not in (\"none\", \"mean\", \"sum\"):\n        raise ValueError(\"reduction must be one of 'none', 'mean', or 'sum'\")\n\n    # Device check and fallback\n    device = x1.device\n    if not (isinstance(device, torch.device) and device.type == \"cuda\"):\n        # Fallback to PyTorch implementation for non-CUDA tensors\n        return torch.ops.aten.margin_ranking_loss(x1, x2, target, float(margin), {\"none\": 0, \"mean\": 1, \"sum\": 2}[reduction])\n\n    # Broadcast tensors\n    x1_b, x2_b, tgt_b = torch.broadcast_tensors(x1, x2, target)\n\n    # Choose dtype (prefer input dtype; fall back to float32 if non-floating)\n    common_dtype = x1_b.dtype if x1_b.is_floating_point() else torch.float32\n    x1_b = x1_b.to(dtype=common_dtype)\n    x2_b = x2_b.to(dtype=common_dtype)\n    tgt_b = tgt_b.to(dtype=common_dtype)\n\n    # Flatten contiguous buffers\n    x1_c = x1_b.contiguous().view(-1)\n    x2_c = x2_b.contiguous().view(-1)\n    tgt_c = tgt_b.contiguous().view(-1)\n\n    # Output buffer\n    out = torch.empty_like(x1_c)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        # Handle empty tensors\n        if reduction == \"none\":\n            return out.view(x1_b.shape)\n        elif reduction == \"sum\":\n            return out.sum()\n        else:\n            return out.mean()\n\n    # Launch Triton kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _margin_ranking_loss_kernel[grid](x1_c, x2_c, tgt_c, out, n_elements, float(margin), BLOCK_SIZE=BLOCK_SIZE)\n\n    # Apply reduction\n    if reduction == \"none\":\n        return out.view(x1_b.shape)\n    elif reduction == \"sum\":\n        return out.sum()\n    else:\n        return out.mean()",
        "has_code": true
      },
      "aten::masked_scatter": {
        "gpt_speedup_vs_cuda": 2.141666824109292,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _masked_scatter_count_kernel(\n    mask_ptr,            # *Pointer* to mask tensor (bool)\n    counts_ptr,          # *Pointer* to per-block counts (int32)\n    n_elements,          # Number of elements in the flattened input\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    in_bounds = offsets < n_elements\n\n    m = tl.load(mask_ptr + offsets, mask=in_bounds, other=0)\n    m_i32 = m.to(tl.int32)\n    local_count = tl.sum(m_i32, axis=0)\n    tl.store(counts_ptr + pid, local_count)\n\n\n@triton.jit\ndef _masked_scatter_apply_kernel(\n    in_ptr,              # *Pointer* to input tensor\n    mask_ptr,            # *Pointer* to mask tensor (bool)\n    src_ptr,             # *Pointer* to source tensor (1D)\n    out_ptr,             # *Pointer* to output tensor\n    n_elements,          # Number of elements in the flattened input\n    prefix_ptr,          # *Pointer* to per-block exclusive prefix sums (int32)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    in_bounds = offsets < n_elements\n\n    x = tl.load(in_ptr + offsets, mask=in_bounds)\n    m = tl.load(mask_ptr + offsets, mask=in_bounds, other=0)\n    m_i32 = m.to(tl.int32)\n\n    # Compute per-block exclusive ranks for True mask elements\n    inclusive = tl.cumsum(m_i32, axis=0)\n    rank = inclusive - m_i32  # exclusive rank within the block\n\n    block_offset = tl.load(prefix_ptr + pid, mask=True, other=0).to(rank.dtype)\n    global_rank = block_offset + rank\n\n    take = m_i32 != 0\n    gathered = tl.load(src_ptr + global_rank, mask=(in_bounds & take), other=0)\n\n    out_vals = tl.where(take, gathered, x)\n    tl.store(out_ptr + offsets, out_vals, mask=in_bounds)\n\n\ndef _launch_masked_scatter(input_tensor: torch.Tensor,\n                           mask: torch.Tensor,\n                           source: torch.Tensor,\n                           out_tensor: torch.Tensor = None):\n    # Validate inputs\n    if input_tensor is None or mask is None or source is None:\n        raise ValueError(\"masked_scatter requires input, mask, and source tensors\")\n\n    if mask.dtype != torch.bool:\n        mask = mask.to(torch.bool)\n\n    if input_tensor.numel() != mask.numel():\n        raise ValueError(\"input and mask must have the same number of elements\")\n\n    if out_tensor is None:\n        out = torch.empty_like(input_tensor)\n    else:\n        out = out_tensor\n        if out.shape != input_tensor.shape:\n            raise ValueError(\"out tensor must have the same shape as input\")\n        if out.dtype != input_tensor.dtype:\n            raise ValueError(\"out tensor must have the same dtype as input\")\n        if out.device != input_tensor.device:\n            raise ValueError(\"out tensor must be on the same device as input\")\n\n    device = input_tensor.device\n    if not device.type == \"cuda\":\n        raise ValueError(\"Triton kernels require CUDA tensors\")\n\n    # Flatten to 1D contiguous views\n    x_flat = input_tensor.contiguous().view(-1)\n    m_flat = mask.contiguous().view(-1)\n    s_flat = source.contiguous().view(-1)\n    out_flat = out.contiguous().view(-1)\n\n    n_elements = x_flat.numel()\n    if n_elements == 0:\n        # Nothing to do\n        out.copy_(input_tensor)\n        return out\n\n    BLOCK_SIZE = 1024\n    n_blocks = triton.cdiv(n_elements, BLOCK_SIZE)\n\n    # 1) Count number of True mask elements per block\n    counts = torch.empty(n_blocks, dtype=torch.int32, device=device)\n    grid = (n_blocks,)\n    _masked_scatter_count_kernel[grid](\n        m_flat, counts, n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    # 2) Compute exclusive prefix sums of per-block counts\n    counts_prefix = torch.cumsum(counts, dim=0)\n    total_true = int(counts_prefix[-1].item()) if n_blocks > 0 else 0\n    if s_flat.numel() < total_true:\n        raise ValueError(\n            f\"source has fewer elements ({s_flat.numel()}) than required by mask ({total_true})\"\n        )\n    prefix_exclusive = counts_prefix - counts  # int32, same device\n\n    # 3) Apply masked_scatter using per-block prefix offsets\n    _masked_scatter_apply_kernel[grid](\n        x_flat,\n        m_flat,\n        s_flat,\n        out_flat,\n        n_elements,\n        prefix_exclusive,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Reshape already matches; ensure out has the result\n    if out.data_ptr() != out_flat.data_ptr():\n        out.view(-1).copy_(out_flat)\n    return out\n\n\ndef masked_scatter(input: torch.Tensor, mask: torch.Tensor, source: torch.Tensor):\n    return _launch_masked_scatter(input, mask, source, out_tensor=None)\n\n\ndef masked_scatter_out(input: torch.Tensor, mask: torch.Tensor, source: torch.Tensor, out: torch.Tensor):\n    return _launch_masked_scatter(input, mask, source, out_tensor=out)",
        "has_code": true
      },
      "aten::special_i0e": {
        "gpt_speedup_vs_cuda": 1.4625178684640705,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _special_i0e_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute in fp32 for accuracy/stability\n    xf = x.to(tl.float32)\n    ax = tl.abs(xf)\n\n    # Small region: x <= 3.75\n    t_small = ax / 3.75\n    t2 = t_small * t_small\n    # Polynomial approximation for I0 in small region (Numerical Recipes)\n    p = 1.0 + t2 * (\n        3.5156229 + t2 * (\n            3.0899424 + t2 * (\n                1.2067492 + t2 * (\n                    0.2659732 + t2 * (\n                        0.0360768 + t2 * 0.0045813\n                    )\n                )\n            )\n        )\n    )\n    small = p * tl.exp(-ax)\n\n    # Large region: x > 3.75, use asymptotic expansion to avoid exp overflow\n    # i0e(x) = I0(x)*exp(-|x|) â‰ˆ (1/sqrt(|x|)) * poly(3.75/|x|)\n    t = 3.75 / ax\n    q = 0.39894228 + t * (\n        0.01328592 + t * (\n            0.00225319 + t * (\n                -0.00157565 + t * (\n                    0.00916281 + t * (\n                        -0.02057706 + t * (\n                            0.02635537 + t * (\n                                -0.01647633 + t * 0.00392377\n                            )\n                        )\n                    )\n                )\n            )\n        )\n    )\n    large = q / tl.sqrt(ax)\n\n    is_large = ax > 3.75\n    y = tl.where(is_large, large, small)\n\n    # Cast back to input dtype for storage\n    y = y.to(x.dtype)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _run_special_i0e_kernel(x: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and out.is_cuda, \"Tensors must be CUDA tensors\"\n    assert x.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64), \"Unsupported dtype\"\n    assert out.dtype == x.dtype, \"Output dtype must match input dtype\"\n\n    x_c = x.contiguous()\n    out_c = out.contiguous()\n\n    n_elements = out_c.numel()\n    if n_elements == 0:\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _special_i0e_kernel[grid](x_c, out_c, n_elements, BLOCK_SIZE=1024)\n\n    if out_c.data_ptr() != out.data_ptr():\n        out.copy_(out_c)\n    return out\n\n\ndef special_i0e(x: torch.Tensor):\n    \"\"\"\n    ATen wrapper: special_i0e(Tensor self) -> Tensor\n    \"\"\"\n    out = torch.empty_like(x)\n    return _run_special_i0e_kernel(x, out)\n\n\ndef special_i0e_out(x: torch.Tensor, out: torch.Tensor):\n    \"\"\"\n    ATen wrapper: special_i0e.out(Tensor self, Tensor out) -> Tensor\n    \"\"\"\n    # Broadcast input to out's shape if needed\n    if x.shape != out.shape:\n        x = x.expand(out.shape)\n    _run_special_i0e_kernel(x, out)\n    return out",
        "has_code": true
      },
      "aten::prelu": {
        "gpt_speedup_vs_cuda": 1.3229121726591495,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef prelu(x_ptr,  # *Pointer* to input tensor.\n          w_ptr,  # *Pointer* to weight tensor (scalar or per-channel vector).\n          out_ptr,  # *Pointer* to output tensor.\n          n_elements,  # Total number of elements in input.\n          S,  # Spatial size = product of dims after channel dim (or 1 if none).\n          C,  # Number of channels (or 1).\n          w_is_scalar: tl.constexpr,  # Whether weight is a single scalar.\n          BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if w_is_scalar:\n        alpha = tl.load(w_ptr)  # scalar\n        y = tl.where(x >= 0, x, alpha * x)\n    else:\n        c = (offsets // S) % C\n        alpha = tl.load(w_ptr + c, mask=mask)\n        y = tl.where(x >= 0, x, alpha * x)\n\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\n# Keep a reference to the Triton kernel before defining the Python wrapper with the same name.\nprelu_kernel = prelu\n\n\ndef prelu(*args, **kwargs):\n    # Extract inputs\n    if len(args) >= 2:\n        x, weight = args[0], args[1]\n    else:\n        x = kwargs.get('input', kwargs.get('self'))\n        weight = kwargs.get('weight')\n    if x is None or weight is None:\n        raise ValueError(\"prelu expects (input, weight) as arguments.\")\n\n    if not (x.is_cuda and weight.is_cuda):\n        raise AssertionError(\"Tensors must be CUDA tensors.\")\n\n    # Ensure dtype match\n    if weight.dtype != x.dtype:\n        weight = weight.to(dtype=x.dtype)\n\n    # Ensure contiguous\n    x = x.contiguous()\n    weight = weight.contiguous()\n\n    out = torch.empty_like(x)\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return out\n\n    # Determine channel count C and spatial size S\n    ndim = x.dim()\n    if weight.numel() == 1:\n        C = 1\n        S = 1\n        w_is_scalar = True\n    else:\n        if ndim == 0:\n            raise AssertionError(\"Non-scalar weight provided for a 0-dim input.\")\n        if ndim == 1:\n            C = x.shape[0]\n            S = 1\n        else:\n            C = x.shape[1]\n            S = 1\n            if ndim > 2:\n                for d in x.shape[2:]:\n                    S *= d\n        if weight.numel() != C:\n            raise AssertionError(f\"Weight numel ({weight.numel()}) must equal channel dimension size ({C}).\")\n        w_is_scalar = False\n\n    # Make sure S and C are at least 1 to avoid div/mod by zero in kernel math\n    C = max(int(C), 1)\n    S = max(int(S), 1)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    prelu_kernel[grid](\n        x, weight, out,\n        n_elements, S, C,\n        w_is_scalar=w_is_scalar,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return out",
        "has_code": true
      },
      "aten::arcsinh": {
        "gpt_speedup_vs_cuda": 1.1848005874746537,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef arcsinh_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n\n    # Compute asinh using: asinh(x) = log(x + sqrt(x*x + 1))\n    x_f32 = x.to(tl.float32)\n    tmp = x_f32 * x_f32 + 1.0\n    sqrt_term = tl.sqrt(tmp)\n    y_f32 = tl.log(x_f32 + sqrt_term)\n\n    # Store result; will cast to out dtype as needed\n    tl.store(out_ptr + offsets, y_f32, mask=mask)\n\n\ndef _ensure_cuda_tensor(t):\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"Expected a torch.Tensor\")\n    if not t.is_cuda:\n        raise ValueError(\"Input tensors must be on CUDA device\")\n    if t.is_complex():\n        raise NotImplementedError(\"Complex dtypes are not supported by this Triton kernel\")\n\n\ndef _arcsinh_impl(input_tensor: torch.Tensor, out_tensor: torch.Tensor = None):\n    _ensure_cuda_tensor(input_tensor)\n\n    # Determine result dtype following basic promotion: float -> same, otherwise float32\n    if input_tensor.is_floating_point():\n        result_dtype = input_tensor.dtype\n    else:\n        result_dtype = torch.float32\n\n    x = input_tensor\n    n_elements = x.numel()\n\n    if out_tensor is None:\n        out = torch.empty_like(x, dtype=result_dtype, device=x.device)\n    else:\n        _ensure_cuda_tensor(out_tensor)\n        if out_tensor.numel() != n_elements:\n            raise ValueError(\"Output tensor must have the same number of elements as input\")\n        # Enforce dtype consistent with promotion\n        if out_tensor.dtype != (result_dtype):\n            raise TypeError(f\"Output tensor has dtype {out_tensor.dtype}, expected {result_dtype}\")\n        out = out_tensor\n\n    # Work with contiguous buffers for the kernel\n    x_contig = x.contiguous()\n    out_contig = out if out.is_contiguous() else out.contiguous()\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    arcsinh_kernel[grid](\n        x_contig, out_contig, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    # If out was non-contiguous, copy back\n    if out_contig.data_ptr() != out.data_ptr():\n        out.copy_(out_contig)\n\n    return out\n\n\ndef arcsinh(input_tensor: torch.Tensor):\n    return _arcsinh_impl(input_tensor)\n\n\ndef arcsinh_out(input_tensor: torch.Tensor, out: torch.Tensor):\n    return _arcsinh_impl(input_tensor, out)",
        "has_code": true
      },
      "aten::logical_xor_": {
        "gpt_speedup_vs_cuda": 1.0769799119450454,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logical_xor_(a_ptr, b_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    a = tl.load(a_ptr + offsets, mask=mask)\n    b = tl.load(b_ptr + offsets, mask=mask)\n\n    a_bool = a != 0\n    b_bool = b != 0\n    out = a_bool ^ b_bool\n\n    tl.store(a_ptr + offsets, out, mask=mask)\n\n\n# Preserve reference to the Triton kernel before defining the Python wrapper with the same name.\nlogical_xor___triton_kernel = logical_xor_\n\n\ndef logical_xor_(*args, **kwargs):\n    # Parse inputs: expect (self, other)\n    if len(args) >= 2:\n        self, other = args[0], args[1]\n    else:\n        self = kwargs.get(\"input\", kwargs.get(\"self\", None))\n        other = kwargs.get(\"other\", None)\n\n    if not isinstance(self, torch.Tensor):\n        raise TypeError(\"logical_xor_: first argument must be a torch.Tensor\")\n    if self.dtype is not torch.bool:\n        raise RuntimeError(\"logical_xor_: in-place logical operations require self to have dtype torch.bool\")\n\n    if not self.is_cuda:\n        raise RuntimeError(\"logical_xor_: tensor must be on CUDA device\")\n\n    # Prepare 'other' as tensor on same device\n    if isinstance(other, torch.Tensor):\n        other_t = other.to(device=self.device)\n    else:\n        # Create scalar tensor with dtype matching self (bool)\n        other_t = torch.tensor(other, device=self.device, dtype=self.dtype)\n\n    # Broadcast 'other' to self's shape and make it contiguous for simple indexing\n    try:\n        other_bc = torch.broadcast_to(other_t, self.shape).contiguous()\n    except Exception as e:\n        raise RuntimeError(f\"logical_xor_: cannot broadcast 'other' to shape {tuple(self.shape)}: {e}\")\n\n    # Work on a contiguous copy if self is not contiguous, then copy back\n    work_self = self if self.is_contiguous() else self.contiguous()\n\n    n_elements = work_self.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    logical_xor___triton_kernel[grid](work_self, other_bc, n_elements, BLOCK_SIZE=1024)\n\n    if work_self.data_ptr() != self.data_ptr():\n        self.copy_(work_self)\n\n    return self",
        "has_code": true
      },
      "aten::logit": {
        "gpt_speedup_vs_cuda": 1.0694918044241668,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logit_kernel(x_ptr, y_ptr, n_elements, eps, HAS_EPS: tl.constexpr, BLOCK_SIZE: tl.constexpr, OUT_DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x_f32 = x.to(tl.float32)\n\n    if HAS_EPS:\n        lo = eps\n        hi = 1.0 - eps\n        x_f32 = tl.minimum(tl.maximum(x_f32, lo), hi)\n\n    y = tl.log(x_f32 / (1.0 - x_f32))\n    tl.store(y_ptr + offsets, y.to(OUT_DTYPE), mask=mask)\n\n\ndef _to_triton_dtype(dtype):\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    return None\n\n\ndef _logit_impl(input: torch.Tensor, eps=None, out: torch.Tensor = None):\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(\"input must be a torch.Tensor\")\n    if not input.is_cuda:\n        raise AssertionError(\"Input tensor must be on CUDA device for Triton kernel.\")\n    if not input.is_floating_point():\n        raise TypeError(\"logit expected a floating point tensor as input\")\n    if eps is not None:\n        eps = float(eps)\n        if not (0.0 <= eps <= 0.5):\n            raise ValueError(\"eps must be in the range [0.0, 0.5].\")\n\n    in_contig = input.contiguous()\n    in_supported = _to_triton_dtype(in_contig.dtype) is not None\n    in_kernel = in_contig if in_supported else in_contig.to(torch.float32)\n\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if not out.is_cuda:\n            raise AssertionError(\"Out tensor must be on CUDA device for Triton kernel.\")\n        if out.shape != input.shape:\n            raise ValueError(\"out tensor must have the same shape as input\")\n        if out.dtype != input.dtype:\n            raise TypeError(\"For logit.out, out.dtype must match input.dtype\")\n        # Decide working output (contiguous and with supported dtype)\n        out_supported = _to_triton_dtype(out.dtype) is not None\n        need_copy_back = (not out.is_contiguous()) or (not out_supported)\n\n        if need_copy_back:\n            work_dtype = out.dtype if out_supported else torch.float32\n            work_out = torch.empty_like(out, dtype=work_dtype)\n        else:\n            work_out = out\n\n        n_elements = in_kernel.numel()\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n        triton_dtype = _to_triton_dtype(work_out.dtype)\n        logit_kernel[grid](\n            in_kernel,\n            work_out,\n            n_elements,\n            eps if eps is not None else 0.0,\n            HAS_EPS=(eps is not None),\n            BLOCK_SIZE=BLOCK_SIZE,\n            OUT_DTYPE=triton_dtype,\n        )\n\n        if need_copy_back:\n            out.copy_(work_out.to(out.dtype))\n        return out\n\n    # out is None -> produce and return a new tensor\n    desired_dtype = input.dtype\n    desired_supported = _to_triton_dtype(desired_dtype) is not None\n    if desired_supported:\n        result = torch.empty_like(input, dtype=desired_dtype)\n        work_out = result\n    else:\n        # compute in fp32, cast back to desired\n        work_out = torch.empty_like(input, dtype=torch.float32)\n\n    n_elements = in_kernel.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    triton_dtype = _to_triton_dtype(work_out.dtype)\n    logit_kernel[grid](\n        in_kernel,\n        work_out,\n        n_elements,\n        eps if eps is not None else 0.0,\n        HAS_EPS=(eps is not None),\n        BLOCK_SIZE=BLOCK_SIZE,\n        OUT_DTYPE=triton_dtype,\n    )\n\n    if desired_supported:\n        return work_out\n    else:\n        return work_out.to(desired_dtype)\n\n\ndef logit(input, eps=None):\n    return _logit_impl(input, eps=eps, out=None)\n\n\ndef logit_out(input, eps=None, out=None):\n    if out is None:\n        raise TypeError(\"logit_out requires an 'out' tensor.\")\n    return _logit_impl(input, eps=eps, out=out)",
        "has_code": true
      },
      "aten::copy_": {
        "gpt_speedup_vs_cuda": 1.0492255176197947,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\ndef _tl_dtype_from_torch(dtype: torch.dtype):\n    # Map common torch dtypes to Triton dtypes\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.float64:\n        return tl.float64\n    if dtype == torch.int8:\n        return tl.int8\n    if dtype == torch.int16:\n        return tl.int16\n    if dtype == torch.int32:\n        return tl.int32\n    if dtype == torch.int64:\n        return tl.int64\n    if dtype == torch.uint8:\n        return tl.uint8\n    raise NotImplementedError(f\"Unsupported dtype for Triton copy_: {dtype}\")\n\n\n@triton.jit\ndef _copy_kernel(dst_ptr, src_ptr, n_elements, BLOCK_SIZE: tl.constexpr, DST_DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    vals = tl.load(src_ptr + offsets, mask=mask)\n    vals = tl.cast(vals, DST_DTYPE)\n    tl.store(dst_ptr + offsets, vals, mask=mask)\n\n\n@triton.jit\ndef _fill_kernel(dst_ptr, scalar_value, n_elements, BLOCK_SIZE: tl.constexpr, DST_DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    vals = tl.full((BLOCK_SIZE,), tl.cast(scalar_value, DST_DTYPE), DST_DTYPE)\n    tl.store(dst_ptr + offsets, vals, mask=mask)\n\n\ndef _launch_copy_tensor(dst: torch.Tensor, src: torch.Tensor):\n    assert dst.is_cuda and src.is_cuda, \"Triton copy_ supports CUDA tensors only.\"\n    assert dst.is_contiguous() and src.is_contiguous(), \"Only contiguous tensors are supported.\"\n    n_elements = dst.numel()\n    assert src.numel() == n_elements, \"Source and destination must have the same number of elements.\"\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    DST_DTYPE = _tl_dtype_from_torch(dst.dtype)\n    _copy_kernel[grid](\n        dst, src, n_elements,\n        BLOCK_SIZE=1024,\n        DST_DTYPE=DST_DTYPE,\n    )\n    return dst\n\n\ndef _launch_fill_scalar(dst: torch.Tensor, scalar):\n    assert dst.is_cuda, \"Triton copy_ (scalar) supports CUDA tensors only.\"\n    assert dst.is_contiguous(), \"Only contiguous tensors are supported.\"\n    n_elements = dst.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    DST_DTYPE = _tl_dtype_from_torch(dst.dtype)\n    # Convert scalar to a Python number for kernel argument\n    if dst.dtype.is_floating_point:\n        scalar_val = float(scalar)\n    else:\n        scalar_val = int(scalar)\n    _fill_kernel[grid](\n        dst, scalar_val, n_elements,\n        BLOCK_SIZE=1024,\n        DST_DTYPE=DST_DTYPE,\n    )\n    return dst\n\n\ndef copy_(self: torch.Tensor, src, non_blocking: bool = False):\n    if isinstance(src, torch.Tensor):\n        return _launch_copy_tensor(self, src)\n    elif isinstance(src, (int, bool)):\n        return _launch_fill_scalar(self, int(src))\n    elif isinstance(src, float):\n        return _launch_fill_scalar(self, float(src))\n    else:\n        raise TypeError(f\"Unsupported src type for copy_: {type(src)}\")\n\n\ndef copy__Tensor(self: torch.Tensor, src: torch.Tensor, non_blocking: bool = False):\n    return _launch_copy_tensor(self, src)\n\n\ndef copy__int(self: torch.Tensor, src: int, non_blocking: bool = False):\n    return _launch_fill_scalar(self, int(src))\n\n\ndef copy__float(self: torch.Tensor, src: float, non_blocking: bool = False):\n    return _launch_fill_scalar(self, float(src))",
        "has_code": true
      },
      "aten::clamp_max_": {
        "gpt_speedup_vs_cuda": 1.0471137179871808,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef clamp_max_tensor_kernel(\n    x_ptr,\n    other_ptr,\n    n_elements,\n    in_size0, in_size1, in_size2, in_size3, in_size4, in_size5, in_size6, in_size7,\n    other_size0, other_size1, other_size2, other_size3, other_size4, other_size5, other_size6, other_size7,\n    in_stride0, in_stride1, in_stride2, in_stride3, in_stride4, in_stride5, in_stride6, in_stride7,\n    other_stride0, other_stride1, other_stride2, other_stride3, other_stride4, other_stride5, other_stride6, other_stride7,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    rem = offsets\n\n    # Initialize offsets in element units\n    offset_in = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    offset_other = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Dimension 7\n    c7 = rem % in_size7\n    rem = rem // in_size7\n    offset_in += c7 * in_stride7\n    use_other7 = other_size7 != 1\n    o7 = tl.where(use_other7, c7, 0)\n    offset_other += o7 * other_stride7\n\n    # Dimension 6\n    c6 = rem % in_size6\n    rem = rem // in_size6\n    offset_in += c6 * in_stride6\n    use_other6 = other_size6 != 1\n    o6 = tl.where(use_other6, c6, 0)\n    offset_other += o6 * other_stride6\n\n    # Dimension 5\n    c5 = rem % in_size5\n    rem = rem // in_size5\n    offset_in += c5 * in_stride5\n    use_other5 = other_size5 != 1\n    o5 = tl.where(use_other5, c5, 0)\n    offset_other += o5 * other_stride5\n\n    # Dimension 4\n    c4 = rem % in_size4\n    rem = rem // in_size4\n    offset_in += c4 * in_stride4\n    use_other4 = other_size4 != 1\n    o4 = tl.where(use_other4, c4, 0)\n    offset_other += o4 * other_stride4\n\n    # Dimension 3\n    c3 = rem % in_size3\n    rem = rem // in_size3\n    offset_in += c3 * in_stride3\n    use_other3 = other_size3 != 1\n    o3 = tl.where(use_other3, c3, 0)\n    offset_other += o3 * other_stride3\n\n    # Dimension 2\n    c2 = rem % in_size2\n    rem = rem // in_size2\n    offset_in += c2 * in_stride2\n    use_other2 = other_size2 != 1\n    o2 = tl.where(use_other2, c2, 0)\n    offset_other += o2 * other_stride2\n\n    # Dimension 1\n    c1 = rem % in_size1\n    rem = rem // in_size1\n    offset_in += c1 * in_stride1\n    use_other1 = other_size1 != 1\n    o1 = tl.where(use_other1, c1, 0)\n    offset_other += o1 * other_stride1\n\n    # Dimension 0\n    c0 = rem % in_size0\n    rem = rem // in_size0\n    offset_in += c0 * in_stride0\n    use_other0 = other_size0 != 1\n    o0 = tl.where(use_other0, c0, 0)\n    offset_other += o0 * other_stride0\n\n    x = tl.load(x_ptr + offset_in, mask=mask)\n    y = tl.load(other_ptr + offset_other, mask=mask)\n    out = tl.where(x > y, y, x)\n    tl.store(x_ptr + offset_in, out, mask=mask)\n\n\ndef _pad_sizes_strides(shape, strides, target_ndim=8):\n    pad = target_ndim - len(shape)\n    sizes = ([1] * pad) + list(shape)\n    s = ([0] * pad) + list(strides)\n    return sizes, s\n\n\ndef _check_broadcastable(in_sizes, other_sizes):\n    assert len(in_sizes) == len(other_sizes)\n    for a, b in zip(in_sizes, other_sizes):\n        if not (b == 1 or b == a):\n            raise RuntimeError(\"Shapes are not broadcastable for clamp_max_.Tensor\")\n\n\ndef _launch_clamp_max_tensor_kernel(x: torch.Tensor, other: torch.Tensor):\n    assert x.is_cuda and other.is_cuda, \"Tensors must be on CUDA device\"\n    assert x.dtype == other.dtype, \"Dtypes must match for in-place clamp_max_\"\n    ndim = max(x.ndim, other.ndim)\n    assert ndim <= 8, \"Supported up to 8 dimensions\"\n\n    in_sizes, in_strides = _pad_sizes_strides(x.shape, x.stride(), 8)\n    other_sizes, other_strides = _pad_sizes_strides(other.shape, other.stride(), 8)\n    _check_broadcastable(in_sizes, other_sizes)\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    clamp_max_tensor_kernel[grid](\n        x, other,\n        n_elements,\n        in_sizes[0], in_sizes[1], in_sizes[2], in_sizes[3], in_sizes[4], in_sizes[5], in_sizes[6], in_sizes[7],\n        other_sizes[0], other_sizes[1], other_sizes[2], other_sizes[3], other_sizes[4], other_sizes[5], other_sizes[6], other_sizes[7],\n        in_strides[0], in_strides[1], in_strides[2], in_strides[3], in_strides[4], in_strides[5], in_strides[6], in_strides[7],\n        other_strides[0], other_strides[1], other_strides[2], other_strides[3], other_strides[4], other_strides[5], other_strides[6], other_strides[7],\n        BLOCK_SIZE=1024,\n    )\n\n\ndef clamp_max_(*args, **kwargs):\n    if len(args) >= 2:\n        x, max_val = args[0], args[1]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\"))\n        max_val = kwargs.get(\"max\")\n    if x is None or max_val is None:\n        raise ValueError(\"Expected arguments (Tensor self, Scalar max)\")\n    assert isinstance(x, torch.Tensor), \"self must be a torch.Tensor\"\n    assert x.is_cuda, \"Tensor must be on CUDA device\"\n\n    # Create a single-element tensor for max and broadcast via strides\n    max_tensor = torch.tensor(max_val, dtype=x.dtype, device=x.device)\n    _launch_clamp_max_tensor_kernel(x, max_tensor)\n    return x\n\n\ndef clamp_max__Tensor(*args, **kwargs):\n    if len(args) >= 2:\n        x, other = args[0], args[1]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\"))\n        other = kwargs.get(\"max\")\n    if x is None or other is None:\n        raise ValueError(\"Expected arguments (Tensor self, Tensor max)\")\n    assert isinstance(x, torch.Tensor) and isinstance(other, torch.Tensor), \"Arguments must be torch.Tensors\"\n    assert x.is_cuda and other.is_cuda, \"Tensors must be on CUDA device\"\n    if other.dtype != x.dtype:\n        other = other.to(dtype=x.dtype)\n    _launch_clamp_max_tensor_kernel(x, other)\n    return x",
        "has_code": true
      },
      "aten::_functional_sym_constrain_range_for_size": {
        "gpt_speedup_vs_cuda": 1.0352812775723073,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _functional_sym_constrain_range_for_size_kernel(x_ptr,  # Pointer to input tensor\n                                                    y_ptr,  # Pointer to output tensor (can be same as input for in-place)\n                                                    n_elements,  # Number of elements\n                                                    BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    tl.store(y_ptr + offsets, x, mask=mask)\n\n\ndef _functional_sym_constrain_range_for_size(*args, **kwargs):\n    # Emulate the behavior of torch.ops.aten._functional_sym_constrain_range_for_size:\n    # return the primary Tensor input unchanged, while optionally launching a no-op kernel.\n    # Identify the first tensor among args or kwargs.\n    tensor_arg = None\n    for a in args:\n        if isinstance(a, torch.Tensor):\n            tensor_arg = a\n            break\n    if tensor_arg is None:\n        for v in kwargs.values():\n            if isinstance(v, torch.Tensor):\n                tensor_arg = v\n                break\n\n    # If we found a tensor, return it unchanged and, if possible, launch an in-place no-op kernel.\n    if tensor_arg is not None:\n        # Only launch the Triton kernel for CUDA, contiguous tensors with numel > 0.\n        if tensor_arg.is_cuda and tensor_arg.is_contiguous():\n            n_elements = tensor_arg.numel()\n            if n_elements > 0:\n                grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n                _functional_sym_constrain_range_for_size_kernel[grid](\n                    tensor_arg, tensor_arg, n_elements, BLOCK_SIZE=1024\n                )\n        return tensor_arg\n\n    # If no tensor was found among inputs, simply return the first positional argument if present, else None.\n    return args[0] if len(args) > 0 else None",
        "has_code": true
      },
      "aten::log1p_": {
        "gpt_speedup_vs_cuda": 1.0292927177873379,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef log1p_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_fp32 = x.to(tl.float32)\n    y = tl.log(x_fp32 + 1.0)\n    y_cast = y.to(x.dtype)\n\n    tl.store(x_ptr + offsets, y_cast, mask=mask)\n\n\n_log1p_kernel = log1p_\n\n\ndef log1p_(*args, **kwargs):\n    # Accept tensor from positional or keyword args\n    x = None\n    if len(args) > 0:\n        x = args[0]\n    else:\n        x = kwargs.get(\"input\", None)\n\n    if x is None:\n        raise ValueError(\"log1p_ expects a tensor as the first argument or keyword 'input'.\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"log1p_ expects a torch.Tensor as input.\")\n\n    # Fallback for unsupported device/dtype/layout\n    if not x.is_cuda:\n        return torch.ops.aten.log1p_(x)\n    if not x.is_contiguous():\n        return torch.ops.aten.log1p_(x)\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        return torch.ops.aten.log1p_(x)\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _log1p_kernel[grid](x, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::logaddexp": {
        "gpt_speedup_vs_cuda": 1.026881513837637,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logaddexp_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    xf32 = x.to(tl.float32)\n    yf32 = y.to(tl.float32)\n\n    delta = xf32 - yf32\n    adelta = tl.abs(delta)\n    m = tl.maximum(xf32, yf32)\n    res = m + tl.log(1.0 + tl.exp(-adelta))\n\n    out_ty = out_ptr.dtype.element_ty\n    tl.store(out_ptr + offsets, res.to(out_ty), mask=mask)\n\n\ndef _ensure_cuda_tensor(obj, device, dtype):\n    if torch.is_tensor(obj):\n        return obj.to(device=device, dtype=dtype)\n    else:\n        return torch.tensor(obj, device=device, dtype=dtype)\n\n\ndef _common_float_dtype(x: torch.Tensor, y: torch.Tensor):\n    dt = torch.result_type(x, y)\n    if dt not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        dt = torch.get_default_dtype()\n    return dt\n\n\ndef _launch_logaddexp_kernel(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and y.is_cuda and out.is_cuda, \"All tensors must be on CUDA device\"\n    assert x.numel() == y.numel() == out.numel(), \"Input and output must have the same number of elements\"\n\n    x_flat = x.contiguous().view(-1)\n    y_flat = y.contiguous().view(-1)\n    out_flat = out.contiguous().view(-1)\n\n    n_elements = out_flat.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    logaddexp_kernel[grid](x_flat, y_flat, out_flat, n_elements, BLOCK_SIZE=1024)\n\n    # If out was non-contiguous, copy results back into original layout\n    if not out.is_contiguous():\n        out.copy_(out_flat.view_as(out))\n\n\ndef logaddexp(x, y):\n    # Determine device\n    device = None\n    if torch.is_tensor(x) and x.is_cuda:\n        device = x.device\n    if device is None and torch.is_tensor(y) and y.is_cuda:\n        device = y.device\n    if device is None:\n        raise ValueError(\"At least one input must be a CUDA tensor\")\n\n    # Determine dtype\n    x_t = x if torch.is_tensor(x) else torch.tensor(x)\n    y_t = y if torch.is_tensor(y) else torch.tensor(y)\n    dtype = _common_float_dtype(x_t, y_t)\n\n    # Convert to device and dtype\n    x_t = _ensure_cuda_tensor(x, device, dtype)\n    y_t = _ensure_cuda_tensor(y, device, dtype)\n\n    # Broadcast\n    xb, yb = torch.broadcast_tensors(x_t, y_t)\n\n    # Allocate output\n    out = torch.empty_like(xb, dtype=dtype, device=device)\n\n    _launch_logaddexp_kernel(xb, yb, out)\n    return out\n\n\ndef logaddexp_out(x, y, out):\n    if not torch.is_tensor(out) or not out.is_cuda:\n        raise ValueError(\"out must be a CUDA tensor\")\n\n    # Determine computation device and dtype from out\n    device = out.device\n    out_dtype = out.dtype\n    if out_dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise ValueError(\"out dtype must be a floating point type\")\n\n    # Prepare inputs\n    x_t = _ensure_cuda_tensor(x, device, out_dtype)\n    y_t = _ensure_cuda_tensor(y, device, out_dtype)\n\n    # Broadcast inputs\n    xb, yb = torch.broadcast_tensors(x_t, y_t)\n\n    # Ensure out shape matches\n    if tuple(out.shape) != tuple(xb.shape):\n        raise ValueError(f\"out shape {tuple(out.shape)} does not match broadcasted shape {tuple(xb.shape)}\")\n\n    _launch_logaddexp_kernel(xb, yb, out)\n    return out",
        "has_code": true
      },
      "aten::fmin": {
        "gpt_speedup_vs_cuda": 1.0253627754476335,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fmin_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    out = tl.minimum(x, y)\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\ndef _to_tensor(x, device=None, dtype=None):\n    if isinstance(x, torch.Tensor):\n        t = x\n        if device is not None and t.device != device:\n            t = t.to(device)\n        if dtype is not None and t.dtype != dtype:\n            t = t.to(dtype)\n        return t\n    return torch.tensor(x, device=device, dtype=dtype)\n\n\ndef _prepare_inputs(a, b, out=None):\n    # Determine target device\n    dev = None\n    if isinstance(out, torch.Tensor):\n        dev = out.device\n    else:\n        if isinstance(a, torch.Tensor):\n            dev = a.device\n        if isinstance(b, torch.Tensor):\n            dev = b.device if dev is None else dev\n    if dev is None:\n        dev = torch.device('cuda')\n    # Convert to tensors on the target device\n    a = _to_tensor(a, device=dev)\n    b = _to_tensor(b, device=dev)\n    if a.device.type != 'cuda' or b.device.type != 'cuda':\n        raise ValueError(\"Inputs must be CUDA tensors or convertible to CUDA tensors for Triton kernels.\")\n    # Broadcast\n    a_b, b_b = torch.broadcast_tensors(a, b)\n    # Determine output dtype\n    out_dtype = torch.result_type(a_b, b_b)\n    if out_dtype.is_complex:\n        raise TypeError(\"fmin does not support complex dtypes.\")\n    # Compute dtype for kernel (avoid bool in kernel by using int8)\n    compute_dtype = torch.int8 if out_dtype == torch.bool else out_dtype\n    a_c = a_b.to(compute_dtype).contiguous()\n    b_c = b_b.to(compute_dtype).contiguous()\n    return a_c, b_c, out_dtype, compute_dtype\n\n\ndef fmin(a, b):\n    a_c, b_c, out_dtype, compute_dtype = _prepare_inputs(a, b, out=None)\n    out_shape = a_c.shape  # same as b_c.shape after broadcast\n    # Allocate outputs\n    if compute_dtype == out_dtype:\n        out = torch.empty(out_shape, dtype=out_dtype, device=a_c.device)\n        out_c = out\n    else:\n        out = torch.empty(out_shape, dtype=out_dtype, device=a_c.device)\n        out_c = torch.empty(out_shape, dtype=compute_dtype, device=a_c.device)\n    n_elements = out_c.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    fmin_kernel[grid](a_c, b_c, out_c, n_elements, BLOCK_SIZE=1024)\n    if out_c.dtype != out.dtype:\n        out.copy_(out_c.to(out_dtype))\n    return out\n\n\ndef fmin_out(a, b, out):\n    if not isinstance(out, torch.Tensor):\n        raise TypeError(\"out must be a Tensor\")\n    a_c, b_c, out_dtype, compute_dtype = _prepare_inputs(a, b, out=out)\n    # Validate out tensor shape/dtype/device\n    expected_shape = a_c.shape\n    if out.device != a_c.device:\n        raise ValueError(\"out tensor must be on the same device as inputs.\")\n    if out.dtype != out_dtype:\n        raise TypeError(f\"out tensor has dtype {out.dtype}, expected {out_dtype}.\")\n    if tuple(out.shape) != tuple(expected_shape):\n        raise ValueError(f\"out tensor has shape {tuple(out.shape)}, expected {tuple(expected_shape)} after broadcasting.\")\n    # Prepare a contiguous buffer to write into\n    if compute_dtype == out_dtype and out.is_contiguous():\n        out_c = out\n    else:\n        # If dtype conversion is needed or out is non-contiguous, use a temporary buffer\n        out_c = torch.empty(expected_shape, dtype=compute_dtype, device=out.device)\n    n_elements = out_c.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    fmin_kernel[grid](a_c, b_c, out_c, n_elements, BLOCK_SIZE=1024)\n    # Move result into out if we used a temporary buffer or dtype differs\n    if out_c is not out:\n        if out_c.dtype != out.dtype:\n            out.copy_(out_c.to(out.dtype))\n        else:\n            if out.is_contiguous():\n                out.copy_(out_c)\n            else:\n                out.view_as(out.contiguous()).copy_(out_c)\n    return out",
        "has_code": true
      },
      "aten::hardswish_backward": {
        "gpt_speedup_vs_cuda": 1.024932631352829,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef hardswish_bwd_kernel(grad_out_ptr, x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    go = tl.load(grad_out_ptr + offsets, mask=mask)\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    mid = x / 3.0 + 0.5\n    deriv = tl.where(x < -3.0, 0.0, tl.where(x <= 3.0, mid, 1.0))\n    gout = go * deriv\n\n    tl.store(out_ptr + offsets, gout, mask=mask)\n\n\ndef _check_inputs(grad_output: torch.Tensor, self: torch.Tensor, out: torch.Tensor = None):\n    if grad_output.device.type != 'cuda' or self.device.type != 'cuda' or (out is not None and out.device.type != 'cuda'):\n        raise RuntimeError(\"All tensors must be on CUDA device for Triton kernel execution.\")\n    if grad_output.numel() != self.numel():\n        raise RuntimeError(\"grad_output and self must have the same number of elements.\")\n    if out is not None and out.numel() != self.numel():\n        raise RuntimeError(\"out must have the same number of elements as inputs.\")\n    if grad_output.shape != self.shape:\n        raise RuntimeError(\"grad_output and self must have the same shape.\")\n    if out is not None and out.shape != self.shape:\n        raise RuntimeError(\"out must have the same shape as inputs.\")\n    allowed_dtypes = {torch.float16, torch.bfloat16, torch.float32}\n    if grad_output.dtype not in allowed_dtypes or self.dtype not in allowed_dtypes:\n        raise RuntimeError(\"Supported dtypes are float16, bfloat16, and float32.\")\n\n\ndef _launch_hardswish_backward(grad_output: torch.Tensor, self: torch.Tensor, out: torch.Tensor):\n    n_elements = out.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    hardswish_bwd_kernel[grid](\n        grad_output, self, out,\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n\ndef hardswish_backward(grad_output: torch.Tensor, self: torch.Tensor):\n    _check_inputs(grad_output, self)\n    go = grad_output.contiguous()\n    x = self.contiguous()\n    out = torch.empty_like(x)\n    _launch_hardswish_backward(go, x, out)\n    return out\n\n\ndef hardswish_backward_out(grad_output: torch.Tensor, self: torch.Tensor, out: torch.Tensor):\n    _check_inputs(grad_output, self, out)\n    go = grad_output.contiguous()\n    x = self.contiguous()\n    use_out = out if out.is_contiguous() else torch.empty_like(out)\n    _launch_hardswish_backward(go, x, use_out)\n    if use_out.data_ptr() != out.data_ptr():\n        out.copy_(use_out)\n    return out",
        "has_code": true
      },
      "aten::softshrink": {
        "gpt_speedup_vs_cuda": 1.0242632929195243,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef softshrink_kernel(x_ptr, out_ptr, n_elements, lambd, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x32 = x.to(tl.float32)\n\n    l = lambd  # scalar float32\n\n    gt = x32 > l\n    lt = x32 < -l\n    res32 = tl.where(gt, x32 - l, tl.where(lt, x32 + l, 0.0))\n\n    # Propagate NaN: if x is NaN, keep it\n    res32 = tl.where(x32 != x32, x32, res32)\n\n    res = res32.to(x.dtype)\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\ndef _check_supported_dtype(t: torch.Tensor):\n    if t.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        raise TypeError(f\"Unsupported dtype {t.dtype}. Supported dtypes are float16, bfloat16, and float32.\")\n\n\ndef _launch_softshrink_kernel(x: torch.Tensor, out: torch.Tensor, lambd: float):\n    n_elements = x.numel()\n    if n_elements == 0:\n        return\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    softshrink_kernel[grid](\n        x, out, n_elements, float(lambd),\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=4,\n    )\n\n\ndef softshrink(input: torch.Tensor, lambd: float = 0.5):\n    if not input.is_cuda:\n        raise ValueError(\"Input tensor must be on CUDA device.\")\n    _check_supported_dtype(input)\n    x = input.contiguous()\n    out = torch.empty_like(x)\n    _launch_softshrink_kernel(x, out, lambd)\n    return out.reshape_as(input)\n\n\ndef softshrink_out(input: torch.Tensor, lambd: float = 0.5, out: torch.Tensor = None):\n    if out is None:\n        raise ValueError(\"Argument 'out' must be provided for softshrink_out.\")\n    if not input.is_cuda or not out.is_cuda:\n        raise ValueError(\"Input and out tensors must be on CUDA device.\")\n    if input.shape != out.shape:\n        raise ValueError(f\"Shape mismatch: input.shape={input.shape}, out.shape={out.shape}\")\n    if input.dtype != out.dtype:\n        raise TypeError(f\"Dtype mismatch: input.dtype={input.dtype}, out.dtype={out.dtype}\")\n    _check_supported_dtype(input)\n\n    x = input.contiguous()\n    if out.is_contiguous():\n        out_buf = out\n    else:\n        out_buf = torch.empty_like(out, memory_format=torch.contiguous_format)\n\n    _launch_softshrink_kernel(x, out_buf, lambd)\n\n    if out_buf.data_ptr() != out.data_ptr():\n        out.copy_(out_buf)\n    return out",
        "has_code": true
      },
      "aten::hypot": {
        "gpt_speedup_vs_cuda": 1.0179640268124395,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\ndef _torch_dtype_to_triton(dtype: torch.dtype):\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.float64:\n        return tl.float64\n    raise ValueError(f\"Unsupported dtype for Triton conversion: {dtype}\")\n\n\n@triton.jit\ndef _hypot_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr, OUT_DTYPE: tl.constexpr, COMPUTE_DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0)\n\n    xf = x.to(COMPUTE_DTYPE)\n    yf = y.to(COMPUTE_DTYPE)\n\n    ax = tl.abs(xf)\n    ay = tl.abs(yf)\n    t = tl.maximum(ax, ay)\n    m = tl.minimum(ax, ay)\n    t_nz = tl.where(t > 0, t, 1).to(COMPUTE_DTYPE)\n    r = m / t_nz\n    res = tl.where(t > 0, t * tl.sqrt(1 + r * r), m)\n\n    out_val = res.to(OUT_DTYPE)\n    tl.store(out_ptr + offsets, out_val, mask=mask)\n\n\ndef _infer_hypot_out_dtype(a: torch.Tensor, b: torch.Tensor) -> torch.dtype:\n    if a.is_complex() or b.is_complex():\n        raise NotImplementedError(\"Complex dtypes are not supported for hypot in this implementation.\")\n    if a.is_floating_point() or b.is_floating_point():\n        return torch.result_type(a, b)\n    # For integral/bool inputs, follow floating promotion behavior\n    return torch.get_default_dtype()\n\n\ndef _launch_hypot_kernel(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor):\n    assert x.device == y.device == out.device, \"All tensors must be on the same device\"\n    assert out.is_cuda, \"Triton kernels require CUDA tensors\"\n    n_elements = out.numel()\n    if n_elements == 0:\n        return\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    out_dtype = out.dtype\n    if out_dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise ValueError(f\"Unsupported output dtype for hypot: {out_dtype}\")\n\n    OUT_DTYPE = _torch_dtype_to_triton(out_dtype)\n    COMPUTE_DTYPE = tl.float64 if out_dtype == torch.float64 else tl.float32\n\n    _hypot_kernel[grid](\n        x, y, out, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        OUT_DTYPE=OUT_DTYPE,\n        COMPUTE_DTYPE=COMPUTE_DTYPE,\n    )\n\n\ndef hypot(a: torch.Tensor, b: torch.Tensor):\n    # Determine output dtype and broadcasted shape\n    out_dtype = _infer_hypot_out_dtype(a, b)\n    device = a.device\n    if b.device != device:\n        raise ValueError(\"Input tensors must be on the same device\")\n    if device.type != \"cuda\":\n        raise ValueError(\"This implementation requires CUDA tensors\")\n\n    out_shape = torch.broadcast_shapes(a.shape, b.shape)\n    out = torch.empty(out_shape, dtype=out_dtype, device=device)\n\n    # Prepare expanded, contiguous inputs\n    x = a.expand(out_shape).contiguous()\n    y = b.expand(out_shape).contiguous()\n\n    _launch_hypot_kernel(x, y, out)\n    return out\n\n\ndef hypot_out(a: torch.Tensor, b: torch.Tensor, out: torch.Tensor):\n    # Validate device and shape\n    device = out.device\n    if (not out.is_cuda) or a.device != device or b.device != device:\n        raise ValueError(\"All tensors (a, b, out) must be CUDA tensors on the same device\")\n\n    # Validate dtype\n    if out.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise ValueError(f\"Unsupported out dtype for hypot_out: {out.dtype}\")\n\n    # Validate/broadcast inputs to out shape\n    target_shape = out.shape\n    x = a.expand(target_shape).contiguous()\n    y = b.expand(target_shape).contiguous()\n\n    _launch_hypot_kernel(x, y, out)\n    return out",
        "has_code": true
      },
      "aten::huber_loss": {
        "gpt_speedup_vs_cuda": 1.0166166403247952,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef huber_loss_element_kernel(\n    x_ptr,    # pointer to input tensor (broadcasted, contiguous, flattened)\n    y_ptr,    # pointer to target tensor (broadcasted, contiguous, flattened)\n    out_ptr,  # pointer to output tensor (contiguous, flattened)\n    n_elements,  # number of elements\n    delta,       # huber delta (scalar)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    diff = x - y\n    absdiff = tl.abs(diff)\n\n    # loss = 0.5 * diff^2 if absdiff <= delta else delta * (absdiff - 0.5 * delta)\n    loss_quad = 0.5 * diff * diff\n    loss_linear = delta * (absdiff - 0.5 * delta)\n    loss = tl.where(absdiff <= delta, loss_quad, loss_linear)\n\n    tl.store(out_ptr + offsets, loss, mask=mask)\n\n\n@triton.jit\ndef reduce_sum_kernel(\n    x_ptr,     # pointer to input tensor (contiguous, flattened)\n    out_ptr,   # pointer to single scalar (float32) to accumulate sum into\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    vals = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    vals_f32 = vals.to(tl.float32)\n    partial_sum = tl.sum(vals_f32, axis=0)\n    tl.atomic_add(out_ptr, partial_sum)\n\n\ndef _normalize_reduction(reduction):\n    if isinstance(reduction, str):\n        r = reduction.lower()\n        if r == 'none':\n            return 0\n        elif r == 'mean':\n            return 1\n        elif r == 'sum':\n            return 2\n        else:\n            raise ValueError(f\"Unsupported reduction: {reduction}\")\n    elif isinstance(reduction, int):\n        if reduction in (0, 1, 2):\n            return reduction\n        else:\n            raise ValueError(f\"Unsupported reduction: {reduction}\")\n    else:\n        raise ValueError(f\"Unsupported reduction type: {type(reduction)}\")\n\n\ndef huber_loss(input, target, reduction=1, delta=1.0):\n    reduction = _normalize_reduction(reduction)\n    if not (input.is_cuda and target.is_cuda):\n        raise AssertionError(\"Triton kernels require CUDA tensors\")\n    device = input.device\n    # Promote dtype similar to PyTorch type promotion rules\n    result_dtype = torch.result_type(input, target)\n\n    # Broadcast tensors to a common shape\n    x_b, y_b = torch.broadcast_tensors(input.to(result_dtype), target.to(result_dtype))\n    x_b = x_b.contiguous()\n    y_b = y_b.contiguous()\n    numel = x_b.numel()\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(numel, meta['BLOCK_SIZE']),)\n\n    if reduction == 0:  # 'none'\n        out = torch.empty_like(x_b, dtype=result_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, out, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        return out\n    else:\n        # Compute elementwise loss into a temporary buffer\n        tmp = torch.empty_like(x_b, dtype=result_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        # Reduce to scalar using float32 accumulator\n        acc = torch.zeros((), dtype=torch.float32, device=device)\n        reduce_sum_kernel[grid](tmp, acc, numel, BLOCK_SIZE=BLOCK_SIZE)\n        if reduction == 1:  # mean\n            val = (acc / numel).to(result_dtype)\n        else:  # sum\n            val = acc.to(result_dtype)\n        return val\n\n\ndef huber_loss_out(input, target, reduction=1, delta=1.0, out=None):\n    if out is None:\n        raise ValueError(\"huber_loss_out requires an 'out' tensor\")\n    reduction = _normalize_reduction(reduction)\n    if not (input.is_cuda and target.is_cuda and out.is_cuda):\n        raise AssertionError(\"Triton kernels require CUDA tensors\")\n\n    device = input.device\n    # Determine result dtype; use out.dtype if provided to match .out behavior\n    # but ensure it's compatible with promoted dtype\n    promoted_dtype = torch.result_type(input, target)\n    result_dtype = out.dtype\n\n    # Broadcast tensors\n    x_b, y_b = torch.broadcast_tensors(input.to(promoted_dtype), target.to(promoted_dtype))\n    x_b = x_b.contiguous()\n    y_b = y_b.contiguous()\n    numel = x_b.numel()\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(numel, meta['BLOCK_SIZE']),)\n\n    if reduction == 0:  # 'none'\n        # Ensure out has correct shape\n        if out.numel() != numel or out.shape != x_b.shape:\n            raise ValueError(f\"'out' tensor must have shape {tuple(x_b.shape)} for reduction='none'\")\n        # Compute into a temporary if out is not contiguous or dtype mismatches\n        needs_tmp = (not out.is_contiguous()) or (out.dtype != result_dtype)\n        if needs_tmp:\n            tmp = torch.empty_like(x_b, dtype=result_dtype, device=device)\n            huber_loss_element_kernel[grid](x_b.to(result_dtype), y_b.to(result_dtype), tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n            out.copy_(tmp)\n        else:\n            huber_loss_element_kernel[grid](x_b.to(result_dtype), y_b.to(result_dtype), out, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        return out\n    else:\n        # Compute elementwise loss into temporary (in promoted dtype), then reduce to scalar\n        tmp = torch.empty_like(x_b, dtype=promoted_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        acc = torch.zeros((), dtype=torch.float32, device=device)\n        reduce_sum_kernel[grid](tmp, acc, numel, BLOCK_SIZE=BLOCK_SIZE)\n        if reduction == 1:  # mean\n            val = (acc / numel).to(result_dtype)\n        else:  # sum\n            val = acc.to(result_dtype)\n        # Ensure out is scalar/0-d\n        if out.numel() != 1 or out.dim() > 1:\n            raise ValueError(\"For reduction='mean' or 'sum', 'out' must be a scalar (0-d or 1-element) tensor\")\n        # Copy the scalar value into out\n        out.copy_(val)\n        return out",
        "has_code": true
      },
      "aten::greater_equal_": {
        "gpt_speedup_vs_cuda": 1.012778286492548,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ge_inplace_kernel(x_ptr,  # *Pointer* to input/output tensor (in-place).\n                      y_ptr,  # *Pointer* to comparison tensor (same shape as x).\n                      n_elements,  # Number of elements.\n                      BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    out = x >= y\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef greater_equal__Scalar(*args, **kwargs):\n    # Parse inputs\n    if len(args) >= 2:\n        x = args[0]\n        other = args[1]\n    else:\n        x = kwargs.get('self', kwargs.get('input'))\n        other = kwargs.get('other')\n    assert isinstance(x, torch.Tensor), \"self must be a torch.Tensor\"\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    # Create a tensor filled with the scalar 'other' matching x's shape and dtype\n    y = torch.full_like(x, other)\n    # Ensure contiguity\n    if not x.is_contiguous():\n        raise RuntimeError(\"greater_equal__Scalar: input tensor must be contiguous\")\n    if not y.is_contiguous():\n        y = y.contiguous()\n    # Launch kernel\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ge_inplace_kernel[grid](x, y, n_elements, BLOCK_SIZE=1024)\n    return x\n\n\ndef greater_equal__Tensor(*args, **kwargs):\n    # Parse inputs\n    if len(args) >= 2:\n        x = args[0]\n        other = args[1]\n    else:\n        x = kwargs.get('self', kwargs.get('input'))\n        other = kwargs.get('other')\n    assert isinstance(x, torch.Tensor) and isinstance(other, torch.Tensor), \"Inputs must be torch.Tensors\"\n    assert x.is_cuda and other.is_cuda, \"Both tensors must be on CUDA device\"\n    # Cast other to x's dtype to ensure the comparison works in kernel\n    if other.dtype != x.dtype:\n        other = other.to(dtype=x.dtype)\n    # Broadcast other to x's shape and make contiguous for simple indexing\n    if other.shape != x.shape:\n        other = other.expand_as(x).contiguous()\n    else:\n        if not other.is_contiguous():\n            other = other.contiguous()\n    # Ensure x (in-place) is contiguous\n    if not x.is_contiguous():\n        raise RuntimeError(\"greater_equal__Tensor: input tensor must be contiguous\")\n    # Launch kernel\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ge_inplace_kernel[grid](x, other, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::smooth_l1_loss": {
        "gpt_speedup_vs_cuda": 1.0127506378597784,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef smooth_l1_elementwise_kernel(\n    x_ptr,\n    y_ptr,\n    out_ptr,\n    n_elements,\n    beta,  # scalar\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0)\n\n    diff = x - y\n    ad = tl.abs(diff)\n\n    # Broadcast beta to vector shape\n    beta_vec = tl.full(ad.shape, beta, x.dtype)\n\n    # Smooth L1 piecewise (for beta > 0):\n    # 0.5 * x^2 / beta             if |x| < beta\n    # |x| - 0.5 * beta             otherwise\n    loss_beta = 0.5 * diff * diff / beta_vec\n    loss_piecewise = tl.where(ad < beta_vec, loss_beta, ad - 0.5 * beta_vec)\n\n    # If beta <= 0, fall back to L1: |x|\n    # Use vectorized condition to avoid divide-by-zero\n    use_piecewise = beta_vec > 0\n    loss = tl.where(use_piecewise, loss_piecewise, ad)\n\n    tl.store(out_ptr + offsets, loss, mask=mask)\n\n\ndef _normalize_reduction(reduction):\n    if reduction is None:\n        return 'mean'\n    if isinstance(reduction, str):\n        reduction = reduction.lower()\n        if reduction in ('none', 'mean', 'sum'):\n            return reduction\n        raise ValueError(f\"Invalid reduction: {reduction}\")\n    if isinstance(reduction, int):\n        mapping = {0: 'none', 1: 'mean', 2: 'sum'}\n        if reduction in mapping:\n            return mapping[reduction]\n        raise ValueError(f\"Invalid reduction code: {reduction}\")\n    raise ValueError(f\"Unsupported reduction type: {type(reduction)}\")\n\n\ndef _parse_smooth_l1_args(args, kwargs, out_variant=False):\n    if len(args) < 2:\n        raise TypeError(\"smooth_l1_loss requires at least input and target tensors\")\n\n    x = args[0]\n    y = args[1]\n\n    beta = kwargs.pop('beta', None)\n    reduction = kwargs.pop('reduction', None)\n    out = kwargs.pop('out', None) if out_variant else None\n\n    # Parse remaining positional arguments flexibly\n    rest = list(args[2:])\n\n    # Try to infer reduction and beta from positional args\n    def maybe_set_reduction(val):\n        nonlocal reduction\n        if reduction is not None:\n            return False\n        if isinstance(val, str):\n            reduction = val\n            return True\n        if isinstance(val, int) and val in (0, 1, 2):\n            reduction = val\n            return True\n        return False\n\n    def maybe_set_beta(val):\n        nonlocal beta\n        if beta is not None:\n            return False\n        if isinstance(val, (float, int)):\n            beta = float(val)\n            return True\n        return False\n\n    # Accept either order for the two optional parameters\n    for val in rest:\n        if not maybe_set_reduction(val):\n            maybe_set_beta(val)\n\n    if beta is None:\n        beta = 1.0\n    reduction = _normalize_reduction(reduction)\n\n    return x, y, reduction, float(beta), out, kwargs\n\n\ndef _launch_smooth_l1_elementwise(x, y, out_buf, beta):\n    n_elements = out_buf.numel()\n    if n_elements == 0:\n        return  # nothing to do\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    smooth_l1_elementwise_kernel[grid](\n        x, y, out_buf, n_elements, beta, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n\ndef _prepare_tensors_for_elementwise(x, y, dtype=None):\n    if dtype is None:\n        dtype = torch.result_type(x, y)\n        if not (dtype.is_floating_point or dtype.is_complex):\n            dtype = torch.get_default_dtype()\n    device = x.device\n    if x.device != y.device:\n        raise ValueError(\"input and target must be on the same device\")\n    if not device.type == 'cuda':\n        return None, None, None, None  # signal fallback\n\n    # Broadcast to a common shape\n    bshape = torch.broadcast_shapes(tuple(x.shape), tuple(y.shape))\n    xb = x.to(dtype).expand(bshape).contiguous()\n    yb = y.to(dtype).expand(bshape).contiguous()\n    out_buf = torch.empty(bshape, device=device, dtype=dtype)\n    return xb, yb, out_buf, bshape\n\n\ndef smooth_l1_loss(*args, **kwargs):\n    x, y, reduction, beta, _, leftover = _parse_smooth_l1_args(args, kwargs, out_variant=False)\n    if leftover:\n        raise TypeError(f\"Unexpected keyword arguments: {list(leftover.keys())}\")\n\n    prep = _prepare_tensors_for_elementwise(x, y)\n    if prep[0] is None:\n        # Fallback to PyTorch if not CUDA\n        return torch.ops.aten.smooth_l1_loss(x, y, reduction=reduction, beta=beta)\n\n    xb, yb, tmp, _ = prep\n    _launch_smooth_l1_elementwise(xb, yb, tmp, beta)\n\n    if reduction == 'none':\n        return tmp\n    elif reduction == 'mean':\n        return tmp.mean()\n    elif reduction == 'sum':\n        return tmp.sum()\n    else:\n        raise ValueError(f\"Invalid reduction: {reduction}\")\n\n\ndef smooth_l1_loss_out(*args, **kwargs):\n    x, y, reduction, beta, out, leftover = _parse_smooth_l1_args(args, kwargs, out_variant=True)\n    if leftover:\n        raise TypeError(f\"Unexpected keyword arguments: {list(leftover.keys())}\")\n\n    # Fallback if not CUDA\n    if x.device.type != 'cuda' or y.device.type != 'cuda':\n        res = torch.ops.aten.smooth_l1_loss(x, y, reduction=reduction, beta=beta)\n        if out is None:\n            return res\n        else:\n            out.copy_(res)\n            return out\n\n    xb, yb, tmp, bshape = _prepare_tensors_for_elementwise(x, y)\n    if xb is None:\n        # Should not happen due to device check above\n        res = torch.ops.aten.smooth_l1_loss(x, y, reduction=reduction, beta=beta)\n        if out is None:\n            return res\n        else:\n            out.copy_(res)\n            return out\n\n    _launch_smooth_l1_elementwise(xb, yb, tmp, beta)\n\n    if reduction == 'none':\n        if out is None:\n            return tmp\n        # Validate 'out' shape/device/dtype for 'none'\n        if out.device != tmp.device:\n            raise ValueError(\"out tensor device mismatch\")\n        if out.dtype != tmp.dtype:\n            raise ValueError(\"out tensor dtype mismatch\")\n        if tuple(out.shape) != tuple(bshape):\n            raise ValueError(\"out tensor shape mismatch for reduction='none'\")\n        if out.is_contiguous():\n            out.copy_(tmp)\n        else:\n            out.reshape(-1).copy_(tmp.reshape(-1))\n        return out\n    else:\n        if reduction == 'mean':\n            res = tmp.mean()\n        elif reduction == 'sum':\n            res = tmp.sum()\n        else:\n            raise ValueError(f\"Invalid reduction: {reduction}\")\n        if out is None:\n            return res\n        # For reduced results, expect out to be a scalar tensor (numel == 1)\n        if out.device != res.device:\n            raise ValueError(\"out tensor device mismatch\")\n        if out.dtype != res.dtype:\n            raise ValueError(\"out tensor dtype mismatch\")\n        if out.numel() != 1:\n            raise ValueError(\"out tensor must have one element for reduced output\")\n        out.copy_(res)\n        return out",
        "has_code": true
      },
      "aten::atanh_": {
        "gpt_speedup_vs_cuda": 1.0120109641876314,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef atanh_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x_fp32 = x.to(tl.float32)\n    numerator = 1.0 + x_fp32\n    denominator = 1.0 - x_fp32\n    out_fp32 = 0.5 * tl.log(numerator / denominator)\n    out = out_fp32.to(x.dtype)\n\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n# Preserve a handle to the Triton kernel before defining the Python wrapper of the same name\natanh__triton_kernel = atanh_\n\n\ndef atanh_(*args, **kwargs):\n    if len(args) < 1:\n        raise TypeError(\"atanh_ expects at least one argument: a torch.Tensor\")\n    x = args[0]\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"atanh_ expects a torch.Tensor as the first argument\")\n    if not x.is_cuda:\n        raise ValueError(\"atanh_ expects the tensor to be on a CUDA device\")\n    if not x.is_floating_point():\n        raise TypeError(\"atanh_ expects a floating-point tensor\")\n\n    # Work on a contiguous buffer, then copy results back to x to preserve in-place semantics.\n    xc = x.contiguous()\n    n_elements = xc.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    atanh__triton_kernel[grid](xc, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    x.copy_(xc)\n    return x",
        "has_code": true
      },
      "aten::ge_": {
        "gpt_speedup_vs_cuda": 1.0115167805863512,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ge_inplace_scalar_kernel(x_ptr, n_elements, other, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    cmp = x >= other\n    one = tl.full([BLOCK_SIZE], 1, x.dtype)\n    zero = tl.full([BLOCK_SIZE], 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n@triton.jit\ndef ge_inplace_tensor_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    cmp = x >= y\n    one = tl.full([BLOCK_SIZE], 1, x.dtype)\n    zero = tl.full([BLOCK_SIZE], 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef ge__Scalar(*args, **kwargs):\n    x = args[0]\n    other = args[1] if len(args) > 1 else kwargs.get('other', None)\n\n    # If other is a tensor with a single element, treat it as a scalar\n    if isinstance(other, torch.Tensor):\n        if other.numel() == 1:\n            other_val = other.to(dtype=x.dtype, device=x.device).item()\n        else:\n            return ge__Tensor(x, other)\n    else:\n        other_val = other\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ge_inplace_scalar_kernel[grid](x, n_elements, other_val, BLOCK_SIZE=1024)\n    return x\n\n\ndef ge__Tensor(*args, **kwargs):\n    x = args[0]\n    y = args[1]\n\n    # Handle scalar-like tensor by delegating to scalar implementation\n    if isinstance(y, torch.Tensor) and y.numel() == 1:\n        return ge__Scalar(x, y)\n\n    # Match device and dtype, then expand and ensure contiguity for y\n    y_prepped = y.to(device=x.device, dtype=x.dtype)\n    if y_prepped.shape != x.shape:\n        y_prepped = y_prepped.expand_as(x)\n    y_prepped = y_prepped.contiguous()\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ge_inplace_tensor_kernel[grid](x, y_prepped, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::eq_": {
        "gpt_speedup_vs_cuda": 1.0113462492697556,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef eq_inplace_scalar_kernel(self_ptr, scalar_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(self_ptr + offsets, mask=mask)\n    s = tl.load(scalar_ptr)  # scalar\n    eq = x == s\n    result = eq.to(x.dtype)\n    tl.store(self_ptr + offsets, result, mask=mask)\n\n\n@triton.jit\ndef eq_inplace_tensor_kernel(self_ptr, other_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(self_ptr + offsets, mask=mask)\n    y = tl.load(other_ptr + offsets, mask=mask)\n    eq = x == y\n    result = eq.to(x.dtype)\n    tl.store(self_ptr + offsets, result, mask=mask)\n\n\ndef eq__Scalar(*args, **kwargs):\n    # Parse inputs: expect (self, other)\n    if len(args) >= 2:\n        self, other = args[0], args[1]\n    else:\n        # Support keyword styles\n        self = kwargs.get(\"self\", kwargs.get(\"input\"))\n        other = kwargs.get(\"other\", None)\n    if self is None:\n        raise ValueError(\"eq__Scalar expects a 'self' tensor as the first argument\")\n    if other is None:\n        raise ValueError(\"eq__Scalar expects 'other' scalar as the second argument\")\n    if not isinstance(self, torch.Tensor):\n        raise TypeError(\"eq__Scalar: 'self' must be a torch.Tensor\")\n    if not self.is_cuda:\n        raise ValueError(\"eq__Scalar requires CUDA tensors\")\n\n    # Convert other to a scalar tensor of the same dtype/device as self\n    if isinstance(other, torch.Tensor):\n        if other.numel() != 1:\n            # If other is not scalar, redirect to tensor variant\n            return eq__Tensor(self, other)\n        other_val = other.item()\n    else:\n        other_val = other\n\n    scalar_tensor = torch.tensor(other_val, dtype=self.dtype, device=self.device)\n\n    # For simplicity, require contiguous self (common for in-place ops in kernels)\n    if not self.is_contiguous():\n        raise ValueError(\"eq__Scalar currently supports only contiguous 'self' tensors\")\n\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    grid = lambda META: (triton.cdiv(n_elements, META[\"BLOCK_SIZE\"]),)\n    eq_inplace_scalar_kernel[grid](self, scalar_tensor, n_elements, BLOCK_SIZE=1024)\n    return self\n\n\ndef eq__Tensor(*args, **kwargs):\n    # Parse inputs: expect (self, other)\n    if len(args) >= 2:\n        self, other = args[0], args[1]\n    else:\n        # Support keyword styles\n        self = kwargs.get(\"self\", kwargs.get(\"input\"))\n        other = kwargs.get(\"other\", None)\n    if self is None or other is None:\n        raise ValueError(\"eq__Tensor expects 'self' and 'other' tensors\")\n    if not isinstance(self, torch.Tensor) or not isinstance(other, torch.Tensor):\n        raise TypeError(\"eq__Tensor: both 'self' and 'other' must be torch.Tensors\")\n    if not self.is_cuda:\n        raise ValueError(\"eq__Tensor requires CUDA tensors\")\n\n    # Match device and dtype to self for comparison in kernel\n    other = other.to(device=self.device, dtype=self.dtype)\n\n    # Broadcast other to self's shape, then make contiguous for simple 1D indexing\n    try:\n        other_b = other.expand_as(self)\n    except Exception as e:\n        raise ValueError(f\"eq__Tensor: tensors are not broadcastable: {e}\")\n    other_c = other_b.contiguous()\n\n    # Require contiguous self (in-place on original storage)\n    if not self.is_contiguous():\n        raise ValueError(\"eq__Tensor currently supports only contiguous 'self' tensors\")\n\n    n_elements = self.numel()\n    if other_c.numel() != n_elements:\n        raise ValueError(\"eq__Tensor: broadcasted 'other' does not match number of elements in 'self'\")\n    if n_elements == 0:\n        return self\n\n    grid = lambda META: (triton.cdiv(n_elements, META[\"BLOCK_SIZE\"]),)\n    eq_inplace_tensor_kernel[grid](self, other_c, n_elements, BLOCK_SIZE=1024)\n    return self",
        "has_code": true
      },
      "aten::lt_": {
        "gpt_speedup_vs_cuda": 1.0112114128924738,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef lt_inplace_kernel(x_ptr, y_ptr, n_elements, IS_SCALAR: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if IS_SCALAR:\n        s = tl.load(y_ptr)  # y_ptr points to a single element tensor\n        cmp = x < s\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask)\n        cmp = x < y\n\n    # Cast boolean comparison result to the dtype of x for in-place store\n    one = tl.full(x.shape, 1, x.dtype)\n    zero = tl.full(x.shape, 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef lt__Tensor(self: torch.Tensor, other: torch.Tensor):\n    assert self.is_cuda, \"Input tensor must be on CUDA device\"\n    assert other.is_cuda, \"Other tensor must be on CUDA device\"\n    assert not self.is_complex(), \"Complex dtypes are not supported\"\n    assert self.is_contiguous(), \"Only contiguous tensors are supported for in-place lt_\"\n    # Support either same numel or scalar-like other (numel == 1)\n    if other.numel() == 1:\n        # Ensure dtype matches self for comparison\n        scalar_buf = other.to(self.dtype).reshape(1).contiguous()\n        n_elements = self.numel()\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        lt_inplace_kernel[grid](\n            self, scalar_buf, n_elements, IS_SCALAR=True, BLOCK_SIZE=BLOCK_SIZE\n        )\n        return self\n    else:\n        assert self.numel() == other.numel(), \"Shapes must match or other must be scalar\"\n        assert other.is_contiguous(), \"Only contiguous tensors are supported for other\"\n        # Cast other to self dtype for comparison\n        other_buf = other.to(self.dtype).contiguous()\n        n_elements = self.numel()\n        BLOCK_SIZE = 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        lt_inplace_kernel[grid](\n            self, other_buf, n_elements, IS_SCALAR=False, BLOCK_SIZE=BLOCK_SIZE\n        )\n        return self\n\n\ndef lt__Scalar(self: torch.Tensor, other):\n    assert self.is_cuda, \"Input tensor must be on CUDA device\"\n    assert not self.is_complex(), \"Complex dtypes are not supported\"\n    assert self.is_contiguous(), \"Only contiguous tensors are supported for in-place lt_\"\n    # Create a 1-element tensor on device with same dtype as self for scalar compare\n    scalar_buf = torch.tensor([other], device=self.device, dtype=self.dtype).contiguous()\n    n_elements = self.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    lt_inplace_kernel[grid](\n        self, scalar_buf, n_elements, IS_SCALAR=True, BLOCK_SIZE=BLOCK_SIZE\n    )\n    return self",
        "has_code": true
      },
      "aten::logaddexp2": {
        "gpt_speedup_vs_cuda": 1.0104822100215445,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logaddexp2_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n\n    # Load inputs and upcast to fp32 for numerics\n    x = tl.load(x_ptr + offs, mask=mask, other=0).to(tl.float32)\n    y = tl.load(y_ptr + offs, mask=mask, other=0).to(tl.float32)\n\n    # Numerically-stable logaddexp2:\n    # logaddexp2(x, y) = m + log2(1 + 2^(-|x - y|)), where m = max(x, y)\n    ln2 = 0.6931471805599453\n    inv_ln2 = 1.4426950408889634\n\n    d = tl.abs(x - y)\n    m = tl.maximum(x, y)\n    t = tl.exp(-d * ln2)              # 2^(-|x-y|) = exp(-(abs(x-y)) * ln(2))\n    res = m + tl.log(1.0 + t) * inv_ln2  # log2(1 + t) = ln(1+t) / ln(2)\n\n    # Store; Triton will cast to the dtype of out_ptr as needed\n    tl.store(out_ptr + offs, res, mask=mask)\n\n\ndef _broadcast_and_check(x, y):\n    # Convert scalars to tensors\n    if not isinstance(x, torch.Tensor):\n        x = torch.as_tensor(x)\n    if not isinstance(y, torch.Tensor):\n        y = torch.as_tensor(y)\n    # Broadcast\n    bx, by = torch.broadcast_tensors(x, y)\n    return bx, by\n\n\ndef _choose_out_dtype(x, y, out=None):\n    if out is not None:\n        return out.dtype\n    # Prefer highest precision floating dtype present; else default dtype\n    float_priority = [torch.float64, torch.float32, torch.bfloat16, torch.float16]\n    for dt in float_priority:\n        if x.dtype == dt or y.dtype == dt:\n            return dt\n    # If none are floating, use default dtype\n    return torch.get_default_dtype()\n\n\ndef _launch_kernel(xc, yc, outc):\n    n_elements = outc.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    logaddexp2_kernel[grid](xc, yc, outc, n_elements, BLOCK_SIZE=1024)\n\n\ndef logaddexp2(x, y):\n    bx, by = _broadcast_and_check(x, y)\n\n    # Fallback for unsupported devices or complex dtype\n    if bx.device.type != 'cuda' or by.device.type != 'cuda' or bx.device != by.device or bx.is_complex() or by.is_complex():\n        return torch.ops.aten.logaddexp2(bx, by)\n\n    out_dtype = _choose_out_dtype(bx, by, out=None)\n    out = torch.empty(bx.shape, device=bx.device, dtype=out_dtype)\n\n    # Ensure contiguous 1D buffers for the kernel\n    xc = bx.contiguous().view(-1)\n    yc = by.contiguous().view(-1)\n    outc = out.contiguous().view(-1)\n\n    _launch_kernel(xc, yc, outc)\n    return out\n\n\ndef logaddexp2_out(x, y, out):\n    if out is None:\n        raise ValueError(\"out tensor must be provided for logaddexp2_out\")\n\n    bx, by = _broadcast_and_check(x, y)\n\n    # Fallback for unsupported devices or complex dtype\n    if (out.device.type != 'cuda' or bx.device.type != 'cuda' or by.device.type != 'cuda' or\n        not (bx.device == by.device == out.device) or bx.is_complex() or by.is_complex() or out.is_complex()):\n        # Use PyTorch implementation for unsupported cases\n        return torch.ops.aten.logaddexp2.out(bx, by, out=out)\n\n    # Shape and dtype checks\n    if out.shape != bx.shape:\n        raise ValueError(f\"out tensor has shape {out.shape}, expected {bx.shape} from broadcast\")\n    # We allow dtype differences; computation will write to out's dtype\n\n    # Prepare contiguous buffers\n    xc = bx.contiguous().view(-1)\n    yc = by.contiguous().view(-1)\n\n    if out.is_contiguous():\n        outc = out.view(-1)\n        _launch_kernel(xc, yc, outc)\n        return out\n    else:\n        # Compute into a temporary contiguous buffer then copy back\n        tmp = torch.empty_like(out, memory_format=torch.contiguous_format)\n        outc = tmp.view(-1)\n        _launch_kernel(xc, yc, outc)\n        out.copy_(tmp)\n        return out",
        "has_code": true
      },
      "aten::hypot_": {
        "gpt_speedup_vs_cuda": 1.010429673381476,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef hypot_(x_ptr,  # Pointer to first input (will be output if in-place).\n           y_ptr,  # Pointer to second input (broadcasted/contiguous).\n           out_ptr,  # Pointer to output buffer.\n           n_elements,  # Number of elements to process.\n           BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n\n    x32 = x.to(tl.float32)\n    y32 = y.to(tl.float32)\n    out32 = tl.sqrt(x32 * x32 + y32 * y32)\n\n    out_cast = out32.to(x.dtype)\n    tl.store(out_ptr + offsets, out_cast, mask=mask)\n\n\n_hypot_kernel = hypot_\n\n\ndef hypot_(*args, **kwargs):\n    # Extract arguments similar to torch.ops.aten.hypot_(self, other)\n    x = None\n    other = None\n    if len(args) >= 1:\n        x = args[0]\n    if len(args) >= 2:\n        other = args[1]\n    if x is None:\n        x = kwargs.get('input', kwargs.get('self', None))\n    if other is None:\n        other = kwargs.get('other', None)\n\n    if x is None or other is None:\n        raise TypeError(\"hypot_ expects two arguments: self and other\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"self must be a torch.Tensor\")\n    if not x.is_cuda:\n        raise ValueError(\"hypot_ Triton kernel only supports CUDA tensors\")\n\n    device = x.device\n\n    # Prepare 'other' on the same device and dtype as x (in-place ops keep dtype)\n    if isinstance(other, torch.Tensor):\n        other_t = other.to(device)\n    else:\n        other_t = torch.tensor(other, device=device)\n\n    # In-place must keep dtype of x; cast other to x.dtype\n    if other_t.dtype != x.dtype:\n        other_t = other_t.to(x.dtype)\n\n    # Broadcast other to x's shape\n    try:\n        other_b = torch.broadcast_to(other_t, x.shape)\n    except Exception:\n        other_b = torch.broadcast_tensors(other_t, x)[0]\n\n    # Ensure contiguous buffers for kernel\n    x_c = x if x.is_contiguous() else x.contiguous()\n    other_c = other_b if other_b.is_contiguous() else other_b.contiguous()\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n\n    # If x is contiguous, write directly in-place into x; otherwise write to temp and copy back.\n    out_buf = x_c if x.is_contiguous() else torch.empty_like(x_c)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _hypot_kernel[grid](x_c, other_c, out_buf, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    if not x.is_contiguous():\n        x.copy_(out_buf)\n\n    return x",
        "has_code": true
      },
      "aten::leaky_relu_": {
        "gpt_speedup_vs_cuda": 1.0100589633211612,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef leaky_relu_(x_ptr,  # *Pointer* to input tensor data (modified in-place).\n                n_elements,  # Number of elements to process.\n                negative_slope,  # Scalar negative slope.\n                BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.where(x >= 0, x, x * negative_slope)\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n\n_leaky_relu_kernel = leaky_relu_\n\n\ndef leaky_relu_(*args, **kwargs):\n    # Parse arguments: expect (input, negative_slope=0.01)\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('self', kwargs.get('input', None))\n    if x is None:\n        raise TypeError(\"leaky_relu_ expected a tensor as the first argument\")\n\n    negative_slope = 0.01\n    if len(args) >= 2:\n        negative_slope = args[1]\n    else:\n        negative_slope = kwargs.get('negative_slope', negative_slope)\n\n    if isinstance(negative_slope, torch.Tensor):\n        negative_slope = negative_slope.item()\n    negative_slope = float(negative_slope)\n\n    # Fallbacks for unsupported environments/dtypes\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"leaky_relu_ expected a torch.Tensor\")\n    if not x.is_cuda or x.numel() == 0:\n        return torch.ops.aten.leaky_relu_(x, negative_slope)\n\n    # For dtypes not well supported by Triton math, fallback to PyTorch\n    supported_dtypes = (torch.float16, torch.bfloat16, torch.float32)\n    if x.dtype not in supported_dtypes:\n        return torch.ops.aten.leaky_relu_(x, negative_slope)\n\n    # Ensure contiguous memory for in-place kernel; otherwise operate on a temp and copy back.\n    if not x.is_contiguous():\n        tmp = x.contiguous()\n        n_elements = tmp.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _leaky_relu_kernel[grid](tmp, n_elements, negative_slope, BLOCK_SIZE=1024)\n        x.copy_(tmp)\n        return x\n\n    # Launch Triton kernel in-place\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _leaky_relu_kernel[grid](x, n_elements, negative_slope, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::not_equal_": {
        "gpt_speedup_vs_cuda": 1.0098543575924865,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\ndef _torch_dtype_to_triton(dtype: torch.dtype):\n    mapping = {\n        torch.float16: tl.float16,\n        torch.bfloat16: tl.bfloat16,\n        torch.float32: tl.float32,\n        torch.float64: tl.float64,\n        torch.int8: tl.int8,\n        torch.uint8: tl.uint8,\n        torch.int16: tl.int16,\n        torch.int32: tl.int32,\n        torch.int64: tl.int64,\n        torch.bool: tl.int1,\n    }\n    if dtype not in mapping:\n        raise NotImplementedError(f\"Unsupported dtype: {dtype}\")\n    return mapping[dtype]\n\n\n@triton.jit\ndef _not_equal_scalar_kernel(x_ptr,  # in-out: pointer to self tensor\n                             scalar,  # scalar to compare against\n                             n_elements,\n                             BLOCK_SIZE: tl.constexpr,\n                             DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=tl.zeros([BLOCK_SIZE], dtype=DTYPE))\n    other = tl.full([BLOCK_SIZE], scalar, dtype=DTYPE)\n    cmp = x != other\n    out = tl.cast(cmp, DTYPE)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n@triton.jit\ndef _not_equal_tensor_kernel(x_ptr,  # in-out: pointer to self tensor\n                             y_ptr,  # pointer to other tensor\n                             n_elements,\n                             BLOCK_SIZE: tl.constexpr,\n                             DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=tl.zeros([BLOCK_SIZE], dtype=DTYPE))\n    y = tl.load(y_ptr + offsets, mask=mask, other=tl.zeros([BLOCK_SIZE], dtype=DTYPE))\n    cmp = x != y\n    out = tl.cast(cmp, DTYPE)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef not_equal__Scalar(self: torch.Tensor, other):\n    # In-place: self <- (self != other) cast into self.dtype as 0/1\n    if isinstance(other, torch.Tensor):\n        if other.numel() != 1:\n            raise TypeError(\"Scalar variant expects a Python number or 0-dim tensor.\")\n        other = other.item()\n\n    assert self.is_cuda, \"Input tensor must be on CUDA device\"\n    assert self.is_contiguous(), \"Only contiguous tensors are supported\"\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    DTYPE = _torch_dtype_to_triton(self.dtype)\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _not_equal_scalar_kernel[grid](\n        self, other, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        DTYPE=DTYPE\n    )\n    return self\n\n\ndef not_equal__Tensor(self: torch.Tensor, other: torch.Tensor):\n    # In-place: self <- (self != other) cast into self.dtype as 0/1\n    assert self.is_cuda and other.is_cuda, \"Input tensors must be on CUDA device\"\n    assert self.is_contiguous() and other.is_contiguous(), \"Only contiguous tensors are supported\"\n    assert self.dtype == other.dtype, \"Dtype mismatch is not supported\"\n    assert self.numel() == other.numel(), \"Only tensors with the same number of elements are supported\"\n\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    DTYPE = _torch_dtype_to_triton(self.dtype)\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _not_equal_tensor_kernel[grid](\n        self, other, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        DTYPE=DTYPE\n    )\n    return self",
        "has_code": true
      },
      "aten::le_": {
        "gpt_speedup_vs_cuda": 1.0098517158640397,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef le_inplace_kernel(\n    x_ptr,             # *Pointer* to input/output tensor (in-place)\n    y_ptr,             # *Pointer* to second tensor (ignored if is_scalar=True)\n    n_elements,        # Number of elements\n    scalar,            # Scalar value for comparison when is_scalar=True\n    IS_SCALAR: tl.constexpr,   # Whether we are comparing against a scalar\n    X_IS_BOOL: tl.constexpr,   # Whether x dtype is bool\n    BLOCK_SIZE: tl.constexpr,  # Elements per program\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if IS_SCALAR:\n        y = tl.full((BLOCK_SIZE,), scalar, x.dtype)\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask).to(x.dtype)\n\n    cmp_res = x <= y\n\n    if X_IS_BOOL:\n        out = cmp_res\n    else:\n        one = tl.full((BLOCK_SIZE,), 1, x.dtype)\n        zero = tl.full((BLOCK_SIZE,), 0, x.dtype)\n        out = tl.where(cmp_res, one, zero)\n\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _launch_le_inplace(x: torch.Tensor, y: torch.Tensor = None, scalar=None):\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    assert x.is_contiguous(), \"Only contiguous tensors are supported\"\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    x_is_bool = x.dtype == torch.bool\n\n    if scalar is not None:\n        # Use scalar path; pass x as y_ptr placeholder (unused)\n        le_inplace_kernel[grid](\n            x, x, n_elements, scalar,\n            IS_SCALAR=True, X_IS_BOOL=x_is_bool, BLOCK_SIZE=BLOCK_SIZE\n        )\n    else:\n        assert y is not None, \"Either a tensor 'y' or a scalar must be provided\"\n        assert y.is_cuda and y.device == x.device, \"Both tensors must be on the same CUDA device\"\n        if y.numel() == 1:\n            # Treat 0-dim or single element tensor as scalar\n            scalar_val = y.item()\n            le_inplace_kernel[grid](\n                x, x, n_elements, scalar_val,\n                IS_SCALAR=True, X_IS_BOOL=x_is_bool, BLOCK_SIZE=BLOCK_SIZE\n            )\n            return x\n        assert y.is_contiguous(), \"Only contiguous tensors are supported\"\n        assert y.numel() == x.numel(), \"Tensors must have the same number of elements\"\n        if y.dtype != x.dtype:\n            y = y.to(dtype=x.dtype)\n        le_inplace_kernel[grid](\n            x, y, n_elements, 0,  # scalar is unused in tensor path\n            IS_SCALAR=False, X_IS_BOOL=x_is_bool, BLOCK_SIZE=BLOCK_SIZE\n        )\n    return x\n\n\ndef le__Scalar(self: torch.Tensor, other):\n    \"\"\"\n    In-place self <= other (scalar). Returns self.\n    \"\"\"\n    return _launch_le_inplace(self, scalar=other)\n\n\ndef le__Tensor(self: torch.Tensor, other: torch.Tensor):\n    \"\"\"\n    In-place self <= other (tensor). Returns self.\n    \"\"\"\n    return _launch_le_inplace(self, y=other)",
        "has_code": true
      },
      "aten::ne_": {
        "gpt_speedup_vs_cuda": 1.0098328324931234,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ne_inplace_kernel(x_ptr, y_ptr, n_elements, y_numel,\n                      BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # broadcast y if needed\n    y_offsets = offsets % y_numel\n    y = tl.load(y_ptr + y_offsets, mask=mask)\n\n    cmp = x != y\n    one = tl.full(x.shape, 1, x.dtype)\n    zero = tl.full(x.shape, 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _launch_ne_inplace(x: torch.Tensor, y_buf: torch.Tensor, y_numel: int):\n    assert x.is_cuda and y_buf.is_cuda, \"Tensors must be on CUDA device\"\n    assert x.dtype == y_buf.dtype, \"x and y must have the same dtype\"\n    assert x.is_contiguous(), \"Input tensor must be contiguous\"\n    assert y_buf.is_contiguous(), \"y buffer must be contiguous\"\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n    BLOCK_SIZE = 1024\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    ne_inplace_kernel[grid](x, y_buf, n_elements, y_numel, BLOCK_SIZE=BLOCK_SIZE)\n    return x\n\n\ndef ne__Scalar(self: torch.Tensor, other):\n    # other is a Python scalar or 0-d tensor\n    if isinstance(other, torch.Tensor):\n        assert other.numel() == 1, \"Scalar overload expects a single value\"\n        other = other.item()\n    y_buf = torch.tensor(other, dtype=self.dtype, device=self.device).reshape(1).contiguous()\n    return _launch_ne_inplace(self, y_buf, 1)\n\n\ndef ne__Tensor(self: torch.Tensor, other: torch.Tensor):\n    assert other.device == self.device, \"Tensors must be on the same device\"\n    # Handle scalar-like tensor\n    if other.numel() == 1:\n        y_buf = other.to(dtype=self.dtype).reshape(1).contiguous()\n        return _launch_ne_inplace(self, y_buf, 1)\n    # Broadcast to self's shape\n    try:\n        other_exp = other.to(dtype=self.dtype).expand_as(self)\n    except RuntimeError as e:\n        raise RuntimeError(f\"Incompatible shapes for broadcasting: {self.shape} and {other.shape}\") from e\n    assert self.numel() == other_exp.numel(), \"Broadcasted tensor must match the number of elements\"\n    y_buf = other_exp.contiguous().view(-1)\n    return _launch_ne_inplace(self, y_buf, y_buf.numel())",
        "has_code": true
      },
      "aten::xlogy_": {
        "gpt_speedup_vs_cuda": 1.009714773481114,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef xlogy_inplace_tensor_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n\n    x_f32 = x.to(tl.float32)\n    y_f32 = y.to(tl.float32)\n\n    logy = tl.log(y_f32)\n    res = x_f32 * logy\n    res = tl.where(x_f32 == 0.0, 0.0, res)\n\n    tl.store(x_ptr + offsets, res.to(x.dtype), mask=mask)\n\n\n@triton.jit\ndef xlogy_inplace_scalar_kernel(x_ptr, y_scalar, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_f32 = x.to(tl.float32)\n\n    y_vec = tl.full((BLOCK_SIZE,), y_scalar, tl.float32)\n    logy = tl.log(y_vec)\n\n    res = x_f32 * logy\n    res = tl.where(x_f32 == 0.0, 0.0, res)\n\n    tl.store(x_ptr + offsets, res.to(x.dtype), mask=mask)\n\n\ndef _ensure_supported_dtype(t: torch.Tensor):\n    if t.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        raise TypeError(f\"Unsupported dtype {t.dtype}. Supported: float16, bfloat16, float32.\")\n\n\ndef _ensure_cuda_contiguous(t: torch.Tensor, name: str):\n    if not t.is_cuda:\n        raise RuntimeError(f\"{name} must be a CUDA tensor.\")\n    if not t.is_contiguous():\n        raise RuntimeError(f\"{name} must be contiguous.\")\n\n\ndef xlogy__Tensor(*args, **kwargs):\n    # Expecting signature: (self, other)\n    if len(args) >= 2:\n        x, other = args[0], args[1]\n    else:\n        x = kwargs.get('self', kwargs.get('input', None))\n        other = kwargs.get('other', None)\n    if x is None or other is None:\n        raise ValueError(\"xlogy__Tensor expects (self, other) where both are tensors.\")\n\n    if not isinstance(other, torch.Tensor):\n        raise TypeError(\"xlogy__Tensor expects 'other' to be a Tensor. Use xlogy__Scalar_Other for scalar 'other'.\")\n\n    _ensure_cuda_contiguous(x, \"self\")\n    _ensure_supported_dtype(x)\n    _ensure_cuda_contiguous(other, \"other\")\n    _ensure_supported_dtype(other)\n\n    n_elements = x.numel()\n    if other.numel() == 1:\n        # Treat as scalar\n        y_scalar = other.to(torch.float32).item()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        xlogy_inplace_scalar_kernel[grid](x, y_scalar, n_elements, BLOCK_SIZE=1024)\n    else:\n        if x.numel() != other.numel() or x.shape != other.shape:\n            raise RuntimeError(\"For xlogy__Tensor, 'other' must have the same shape as 'self' or be a scalar tensor.\")\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        xlogy_inplace_tensor_kernel[grid](x, other, n_elements, BLOCK_SIZE=1024)\n\n    return x\n\n\ndef xlogy__Scalar_Other(*args, **kwargs):\n    # Expecting signature: (self, other_scalar)\n    if len(args) >= 2:\n        x, other = args[0], args[1]\n    else:\n        x = kwargs.get('self', kwargs.get('input', None))\n        other = kwargs.get('other', None)\n    if x is None:\n        raise ValueError(\"xlogy__Scalar_Other expects 'self' tensor.\")\n    if other is None or isinstance(other, torch.Tensor):\n        raise TypeError(\"xlogy__Scalar_Other expects 'other' to be a Python scalar (not a Tensor).\")\n\n    _ensure_cuda_contiguous(x, \"self\")\n    _ensure_supported_dtype(x)\n\n    # Convert scalar to float for kernel\n    y_scalar = float(other)\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    xlogy_inplace_scalar_kernel[grid](x, y_scalar, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::greater_": {
        "gpt_speedup_vs_cuda": 1.0086269505650582,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef greater_inplace_kernel(\n    x_ptr,           # *Pointer* to self tensor (modified in-place)\n    y_ptr,           # *Pointer* to other tensor (if tensor path)\n    scalar_ptr,      # *Pointer* to 1-element scalar tensor (if scalar path)\n    n_elements,      # Number of elements\n    IS_SCALAR: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n\n    if IS_SCALAR:\n        s = tl.load(scalar_ptr)  # scalar value\n        y = s\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask, other=0)\n\n    cmp = x > y\n    one = tl.full([BLOCK_SIZE], 1, x.dtype)\n    zero = tl.full([BLOCK_SIZE], 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _launch_greater_inplace_tensor(self: torch.Tensor, other: torch.Tensor):\n    assert self.is_cuda and other.is_cuda, \"Tensors must be on CUDA device\"\n    assert self.is_contiguous(), \"Only contiguous tensors are supported for in-place operation\"\n    # Prepare other: broadcast to self and ensure same dtype and contiguity\n    other_exp = other.to(dtype=self.dtype, device=self.device).expand_as(self).contiguous()\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    greater_inplace_kernel[grid](self, other_exp, self, n_elements, IS_SCALAR=False, BLOCK_SIZE=1024)\n    return self\n\n\ndef _launch_greater_inplace_scalar(self: torch.Tensor, other):\n    assert self.is_cuda, \"Tensor must be on CUDA device\"\n    assert self.is_contiguous(), \"Only contiguous tensors are supported for in-place operation\"\n    # Make a 1-element tensor holding the scalar, converted to self's dtype/device\n    if isinstance(other, torch.Tensor):\n        assert other.numel() == 1, \"Scalar variant expects a single value\"\n        scalar_t = other.to(dtype=self.dtype, device=self.device).reshape(())\n    else:\n        scalar_t = torch.tensor(other, dtype=self.dtype, device=self.device)\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    greater_inplace_kernel[grid](self, self, scalar_t, n_elements, IS_SCALAR=True, BLOCK_SIZE=1024)\n    return self\n\n\ndef greater__Scalar(*args, **kwargs):\n    # Expected: (self, other)\n    self = args[0]\n    other = args[1]\n    return _launch_greater_inplace_scalar(self, other)\n\n\ndef greater__Tensor(*args, **kwargs):\n    # Expected: (self, other)\n    self = args[0]\n    other = args[1]\n    return _launch_greater_inplace_tensor(self, other)",
        "has_code": true
      },
      "aten::addcmul_": {
        "gpt_speedup_vs_cuda": 1.0085974572995544,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef addcmul_(self_ptr, t1_ptr, t2_ptr, n_elements, value, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(self_ptr + offsets, mask=mask)\n    a = tl.load(t1_ptr + offsets, mask=mask)\n    b = tl.load(t2_ptr + offsets, mask=mask)\n\n    xf = x.to(tl.float32)\n    af = a.to(tl.float32)\n    bf = b.to(tl.float32)\n\n    out_f = xf + af * bf * value\n    out = out_f.to(x.dtype)\n\n    tl.store(self_ptr + offsets, out, mask=mask)\n\n\n_addcmul_kernel = addcmul_\n\n\ndef addcmul_(*args, **kwargs):\n    # Parse arguments: self, tensor1, tensor2, value (defaults to 1)\n    if len(args) == 0:\n        raise TypeError(\"addcmul_ expected at least 1 argument (self tensor)\")\n    self = args[0]\n\n    # Extract tensor1 and tensor2\n    if len(args) >= 3:\n        tensor1 = args[1]\n        tensor2 = args[2]\n        if len(args) >= 4:\n            value = args[3]\n        else:\n            value = kwargs.get(\"value\", kwargs.get(\"alpha\", 1.0))\n    else:\n        tensor1 = kwargs.get(\"tensor1\", None)\n        tensor2 = kwargs.get(\"tensor2\", None)\n        value = kwargs.get(\"value\", kwargs.get(\"alpha\", 1.0))\n\n    if tensor1 is None or tensor2 is None:\n        raise TypeError(\"addcmul_ requires tensor1 and tensor2\")\n\n    # Convert value to float\n    value = float(value)\n\n    # Broadcast tensor1 and tensor2 to match self's shape\n    try:\n        t1 = tensor1.expand_as(self)\n        t2 = tensor2.expand_as(self)\n    except Exception:\n        t1 = torch.broadcast_to(tensor1, self.shape)\n        t2 = torch.broadcast_to(tensor2, self.shape)\n\n    # Fallback conditions\n    # - non-CUDA tensors\n    # - non-contiguous self (in-place update with non-contiguous memory)\n    # - unsupported dtype\n    if not (self.is_cuda and t1.is_cuda and t2.is_cuda):\n        return torch.ops.aten.addcmul_(self, tensor1, tensor2, value=value)\n\n    if not self.is_contiguous():\n        return torch.ops.aten.addcmul_(self, tensor1, tensor2, value=value)\n\n    if self.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        return torch.ops.aten.addcmul_(self, tensor1, tensor2, value=value)\n\n    # Make inputs contiguous for efficient loads\n    t1 = t1.contiguous()\n    t2 = t2.contiguous()\n\n    # Cast inputs to self dtype if needed\n    if t1.dtype != self.dtype:\n        t1 = t1.to(self.dtype)\n    if t2.dtype != self.dtype:\n        t2 = t2.to(self.dtype)\n\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    _addcmul_kernel[grid](\n        self, t1, t2,\n        n_elements,\n        value,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return self",
        "has_code": true
      },
      "aten::less_": {
        "gpt_speedup_vs_cuda": 1.0084309824083875,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef less_inplace_kernel(x_ptr, y_ptr, n_elements, SCALAR: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    if SCALAR:\n        y = tl.load(y_ptr)\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask)\n\n    cmp = x < y\n    one = tl.full([BLOCK_SIZE], 1, x.dtype)\n    zero = tl.full([BLOCK_SIZE], 0, x.dtype)\n    out = tl.where(cmp, one, zero)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _launch_less_inplace(x: torch.Tensor, y_tensor: torch.Tensor, scalar: bool):\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    assert y_tensor.is_cuda, \"Other/scalar tensor must be on CUDA device\"\n    assert x.is_contiguous(), \"Input tensor must be contiguous\"\n    assert y_tensor.is_contiguous(), \"Other/scalar tensor must be contiguous\"\n    n_elements = x.numel()\n    assert scalar or (y_tensor.numel() == n_elements), \"Shape mismatch for elementwise comparison\"\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    less_inplace_kernel[grid](x, y_tensor, n_elements, SCALAR=scalar, BLOCK_SIZE=1024)\n    return x\n\n\ndef less__Scalar(self: torch.Tensor, other):\n    x = self\n    # Ensure contiguity; if not, operate on a contiguous copy and copy back.\n    needs_copy_back = not x.is_contiguous()\n    if needs_copy_back:\n        x_work = x.contiguous()\n    else:\n        x_work = x\n\n    # Make scalar tensor on device, cast to x dtype\n    y_tensor = torch.tensor([other], dtype=x_work.dtype, device=x_work.device).contiguous()\n\n    _launch_less_inplace(x_work, y_tensor, scalar=True)\n\n    if needs_copy_back:\n        self.copy_(x_work)\n    return self\n\n\ndef less__Tensor(self: torch.Tensor, other: torch.Tensor):\n    x = self\n    # Ensure contiguity; if not, operate on a contiguous copy and copy back.\n    needs_copy_back = not x.is_contiguous()\n    if needs_copy_back:\n        x_work = x.contiguous()\n    else:\n        x_work = x\n\n    # Prepare 'other' on same device/dtype and choose scalar or tensor path\n    if other.numel() == 1:\n        y_tensor = other.to(dtype=x_work.dtype, device=x_work.device).reshape(1).contiguous()\n        scalar = True\n    else:\n        assert other.numel() == x_work.numel(), \"Shape mismatch for elementwise comparison\"\n        y_tensor = other.to(dtype=x_work.dtype, device=x_work.device).contiguous()\n        scalar = False\n\n    _launch_less_inplace(x_work, y_tensor, scalar=scalar)\n\n    if needs_copy_back:\n        self.copy_(x_work)\n    return self",
        "has_code": true
      },
      "aten::multiply": {
        "gpt_speedup_vs_cuda": 1.007042884193492,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\nfrom numbers import Number\n\n\n@triton.jit\ndef _multiply_tt_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    tl.store(out_ptr + offsets, x * y, mask=mask)\n\n\n@triton.jit\ndef _multiply_ts_kernel(x_ptr, scalar, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # scalar will be implicitly cast to x's dtype by Triton during multiplication\n    tl.store(out_ptr + offsets, x * scalar, mask=mask)\n\n\ndef _broadcast_shape(a_shape, b_shape):\n    return torch.broadcast_shapes(a_shape, b_shape)\n\n\ndef _result_dtype_for(a, b):\n    if isinstance(b, torch.Tensor):\n        return torch.result_type(a, b)\n    else:\n        # b is a Python scalar/Number\n        return torch.result_type(a, torch.tensor(b))\n\n\ndef _ensure_cuda_device(t):\n    if not (isinstance(t, torch.Tensor) and t.is_cuda):\n        raise ValueError(\"Input tensors must be CUDA tensors for Triton kernels.\")\n\n\ndef _launch_tt(a_ctg, b_ctg, out_t):\n    n_elements = out_t.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _multiply_tt_kernel[grid](a_ctg, b_ctg, out_t, n_elements, BLOCK_SIZE=1024)\n\n\ndef _launch_ts(a_ctg, scalar, out_t):\n    n_elements = out_t.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _multiply_ts_kernel[grid](a_ctg, scalar, out_t, n_elements, BLOCK_SIZE=1024)\n\n\ndef _multiply_impl(a, b, out=None):\n    if not isinstance(a, torch.Tensor):\n        raise TypeError(\"First argument must be a torch.Tensor\")\n    _ensure_cuda_device(a)\n    device = a.device\n\n    # Determine result dtype and broadcasted shape\n    res_dtype = _result_dtype_for(a, b)\n\n    if isinstance(b, torch.Tensor):\n        _ensure_cuda_device(b)\n        if b.device != device:\n            raise ValueError(\"Both tensors must be on the same CUDA device.\")\n        out_shape = _broadcast_shape(a.shape, b.shape)\n        a_ctg = a.to(res_dtype).expand(out_shape).contiguous()\n        b_ctg = b.to(res_dtype).expand(out_shape).contiguous()\n        if out is None:\n            out_t = torch.empty(out_shape, device=device, dtype=res_dtype)\n        else:\n            if not isinstance(out, torch.Tensor) or not out.is_cuda:\n                raise TypeError(\"out must be a CUDA torch.Tensor\")\n            if out.shape != out_shape:\n                raise ValueError(f\"out shape {out.shape} does not match broadcasted shape {out_shape}\")\n            if out.dtype != res_dtype:\n                raise TypeError(f\"out dtype {out.dtype} does not match result dtype {res_dtype}\")\n            if out.device != device:\n                raise ValueError(\"out must be on the same CUDA device as inputs\")\n            out_t = out\n        _launch_tt(a_ctg, b_ctg, out_t)\n        return out_t\n    elif isinstance(b, Number):\n        # Scalar path\n        out_shape = a.shape\n        a_ctg = a.to(res_dtype).contiguous()\n        if out is None:\n            out_t = torch.empty(out_shape, device=device, dtype=res_dtype)\n        else:\n            if not isinstance(out, torch.Tensor) or not out.is_cuda:\n                raise TypeError(\"out must be a CUDA torch.Tensor\")\n            if out.shape != out_shape:\n                raise ValueError(f\"out shape {out.shape} does not match input tensor shape {out_shape}\")\n            if out.dtype != res_dtype:\n                raise TypeError(f\"out dtype {out.dtype} does not match result dtype {res_dtype}\")\n            if out.device != device:\n                raise ValueError(\"out must be on the same CUDA device as inputs\")\n            out_t = out\n        _launch_ts(a_ctg, b, out_t)\n        return out_t\n    else:\n        raise TypeError(\"Second argument must be a torch.Tensor or a Python scalar.\")\n\n\ndef multiply_Tensor(self, other):\n    return _multiply_impl(self, other, out=None)\n\n\ndef multiply_Scalar(self, other):\n    return _multiply_impl(self, other, out=None)\n\n\ndef multiply_out(self, other, out):\n    return _multiply_impl(self, other, out=out)",
        "has_code": true
      },
      "aten::xlogy": {
        "gpt_speedup_vs_cuda": 1.0062889145023028,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef xlogy_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=1)\n\n    x_f32 = x.to(tl.float32)\n    y_f32 = y.to(tl.float32)\n\n    # result = where(x == 0, 0, x * log(y))\n    res = tl.where(x_f32 == 0.0, 0.0, x_f32 * tl.log(y_f32))\n\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\ndef _ensure_tensor_on_device(obj, device, dtype):\n    if isinstance(obj, torch.Tensor):\n        return obj.to(device=device, dtype=dtype)\n    else:\n        return torch.as_tensor(obj, device=device, dtype=dtype)\n\n\ndef _prepare_tensors(self, other, out=None):\n    # Determine device\n    if isinstance(self, torch.Tensor):\n        device = self.device\n    elif isinstance(other, torch.Tensor):\n        device = other.device\n    else:\n        raise ValueError(\"At least one of the inputs must be a Tensor.\")\n\n    if device.type != \"cuda\":\n        raise ValueError(\"Triton kernels require CUDA tensors.\")\n\n    # Type promotion following PyTorch semantics\n    if isinstance(self, torch.Tensor) and isinstance(other, torch.Tensor):\n        result_dtype = torch.result_type(self, other)\n    elif isinstance(self, torch.Tensor):\n        other_tmp = torch.as_tensor(other)\n        result_dtype = torch.result_type(self, other_tmp)\n    else:\n        self_tmp = torch.as_tensor(self)\n        result_dtype = torch.result_type(self_tmp, other)\n\n    t_self = _ensure_tensor_on_device(self, device, result_dtype)\n    t_other = _ensure_tensor_on_device(other, device, result_dtype)\n\n    # Broadcast to a common shape\n    b_self, b_other = torch.broadcast_tensors(t_self, t_other)\n\n    # Prepare output\n    if out is None:\n        out_tensor = torch.empty(b_self.shape, device=device, dtype=result_dtype)\n        return b_self.contiguous(), b_other.contiguous(), out_tensor, out_tensor\n    else:\n        if out.device != device:\n            raise ValueError(\"Output tensor must be on the same device as inputs.\")\n        # Out dtype/shape should be able to hold result\n        expected_shape = b_self.shape\n        if out.shape != expected_shape:\n            raise ValueError(f\"Output tensor has shape {out.shape}, expected {expected_shape}.\")\n        if out.dtype != result_dtype:\n            raise ValueError(f\"Output tensor has dtype {out.dtype}, expected {result_dtype}.\")\n        # If out is contiguous, write directly; otherwise use a temporary\n        if out.is_contiguous():\n            return b_self.contiguous(), b_other.contiguous(), out, out\n        else:\n            tmp = torch.empty(expected_shape, device=device, dtype=result_dtype)\n            return b_self.contiguous(), b_other.contiguous(), tmp, out\n\n\ndef _launch_xlogy(self, other, out=None):\n    x, y, dst, final_out = _prepare_tensors(self, other, out)\n    n_elements = dst.numel()\n    if n_elements == 0:\n        if final_out is not dst:\n            final_out.copy_(dst)\n        return final_out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    xlogy_kernel[grid](x, y, dst, n_elements, BLOCK_SIZE=1024)\n\n    if final_out is not dst:\n        final_out.copy_(dst)\n    return final_out\n\n\n# Wrappers corresponding to ATen operator interfaces\n\ndef xlogy_Tensor(self: torch.Tensor, other: torch.Tensor):\n    return _launch_xlogy(self, other, out=None)\n\n\ndef xlogy_Scalar_Other(self: torch.Tensor, other):\n    return _launch_xlogy(self, other, out=None)\n\n\ndef xlogy_Scalar_Self(self, other: torch.Tensor):\n    return _launch_xlogy(self, other, out=None)\n\n\ndef xlogy_OutTensor(self: torch.Tensor, other: torch.Tensor, out: torch.Tensor):\n    return _launch_xlogy(self, other, out=out)\n\n\ndef xlogy_OutScalar_Self(self, other: torch.Tensor, out: torch.Tensor):\n    return _launch_xlogy(self, other, out=out)\n\n\ndef xlogy_OutScalar_Other(self: torch.Tensor, other, out: torch.Tensor):\n    return _launch_xlogy(self, other, out=out)",
        "has_code": true
      },
      "aten::heaviside_": {
        "gpt_speedup_vs_cuda": 1.0062154720717496,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef heaviside_(x_ptr, v_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    v = tl.load(v_ptr + offsets, mask=mask)\n\n    is_zero = x == 0\n    is_neg = x < 0\n    is_pos = x > 0\n\n    # For NaN handling on floating types: if none of the comparisons are true, use x + x to propagate NaN.\n    res = tl.where(is_zero, v, tl.where(is_neg, 0, tl.where(is_pos, 1, x + x)))\n    tl.store(x_ptr + offsets, res, mask=mask)\n\n\n# Keep a handle to the kernel (its __name__ is \"heaviside_\")\nheaviside__kernel = heaviside_\n\n\ndef heaviside_(*args, **kwargs):\n    # Parse arguments similar to torch.ops.aten.heaviside_(self, values)\n    if len(args) >= 2:\n        x, values = args[0], args[1]\n    else:\n        # Fallback to kwargs if provided\n        x = kwargs.get('input', kwargs.get('self', None))\n        values = kwargs.get('values', None)\n    assert x is not None and values is not None, \"heaviside_ requires two arguments: input tensor and values.\"\n\n    # Ensure CUDA tensors\n    assert x.is_cuda, \"Input tensor must be on CUDA device.\"\n    assert x.is_contiguous(), \"Input tensor must be contiguous.\"\n\n    # Prepare values tensor (support scalar or tensor), broadcast to input shape and ensure same dtype/device\n    if not torch.is_tensor(values):\n        v_tensor = torch.as_tensor(values, device=x.device, dtype=x.dtype)\n    else:\n        v_tensor = values.to(device=x.device, dtype=x.dtype)\n\n    v_tensor = v_tensor.expand_as(x).contiguous()\n    assert v_tensor.is_cuda and v_tensor.is_contiguous(), \"Values tensor must be CUDA and contiguous after expansion.\"\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    heaviside__kernel[grid](x, v_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    return x",
        "has_code": true
      },
      "aten::leaky_relu": {
        "gpt_speedup_vs_cuda": 1.0058231746086614,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _leaky_relu_kernel(x_ptr, y_ptr, n_elements, negative_slope,\n                       BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    zero = tl.zeros([BLOCK_SIZE], dtype=x.dtype)\n    slope = tl.full([BLOCK_SIZE], negative_slope, dtype=x.dtype)\n    # y = x if x >= 0 else slope * x\n    # Equivalent, branchless:\n    y = tl.maximum(x, zero) + slope * tl.minimum(x, zero)\n\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n\ndef _launch_leaky_relu_kernel(x: torch.Tensor, out: torch.Tensor, negative_slope: float):\n    if not x.is_cuda or not out.is_cuda:\n        raise ValueError(\"Input and output tensors must be on CUDA device.\")\n    if x.numel() != out.numel():\n        raise ValueError(\"Input and output must have the same number of elements.\")\n    if x.dtype != out.dtype:\n        raise ValueError(\"Input and output tensors must have the same dtype.\")\n    if not x.is_contiguous():\n        x = x.contiguous()\n    if not out.is_contiguous():\n        raise ValueError(\"Output tensor must be contiguous.\")\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _leaky_relu_kernel[grid](x, out, n_elements, float(negative_slope), BLOCK_SIZE=1024)\n    return out\n\n\ndef leaky_relu(input: torch.Tensor, negative_slope: float = 0.01):\n    \"\"\"\n    ATen: ('leaky_relu', <Autograd.disable: False>)\n    \"\"\"\n    out = torch.empty_like(input)\n    return _launch_leaky_relu_kernel(input, out, negative_slope)\n\n\ndef leaky_relu_out(input: torch.Tensor, negative_slope: float = 0.01, out: torch.Tensor = None):\n    \"\"\"\n    ATen: ('leaky_relu.out', <Autograd.disable: False>)\n    \"\"\"\n    if out is None:\n        raise ValueError(\"Argument 'out' must be provided for leaky_relu_out.\")\n    return _launch_leaky_relu_kernel(input, out, negative_slope)",
        "has_code": true
      },
      "aten::heaviside": {
        "gpt_speedup_vs_cuda": 1.004409566828267,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _heaviside_kernel(x_ptr, v_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    v = tl.load(v_ptr + offsets, mask=mask)\n\n    zeros = x - x\n    ones = zeros + 1\n    is_pos = x > zeros\n    is_zero = x == zeros\n    out = tl.where(is_zero, v, tl.where(is_pos, ones, zeros))\n\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\ndef heaviside(input, values):\n    # Prepare tensors\n    if not isinstance(values, torch.Tensor):\n        values = torch.as_tensor(values, device=input.device)\n    # Broadcast\n    x_b, v_b = torch.broadcast_tensors(input, values)\n    # Dtype promotion\n    out_dtype = torch.result_type(x_b, v_b)\n    x = x_b.to(dtype=out_dtype).contiguous()\n    v = v_b.to(dtype=out_dtype).contiguous()\n\n    # Allocate output\n    out = torch.empty_like(x)\n\n    # Launch kernel\n    n_elements = out.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _heaviside_kernel[grid](x, v, out, n_elements, BLOCK_SIZE=1024)\n    return out\n\n\ndef heaviside_out(input, values, out):\n    # Prepare tensors\n    if not isinstance(values, torch.Tensor):\n        values = torch.as_tensor(values, device=input.device)\n    # Broadcast\n    x_b, v_b = torch.broadcast_tensors(input, values)\n    # Dtype promotion\n    expected_dtype = torch.result_type(x_b, v_b)\n    expected_shape = x_b.shape\n    device = x_b.device\n    # Check output tensor\n    if out.device != device:\n        raise ValueError(\"out tensor device must match input device\")\n    if out.dtype != expected_dtype:\n        raise ValueError(\"out tensor dtype must be the result type of input and values\")\n    if out.shape != expected_shape:\n        raise ValueError(\"out tensor shape must be the broadcasted shape of input and values\")\n\n    x = x_b.to(dtype=expected_dtype).contiguous()\n    v = v_b.to(dtype=expected_dtype).contiguous()\n\n    # If out is contiguous, write directly; otherwise use a temp and copy\n    if out.is_contiguous():\n        target = out\n    else:\n        target = torch.empty_like(out, memory_format=torch.contiguous_format)\n\n    n_elements = target.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _heaviside_kernel[grid](x, v, target, n_elements, BLOCK_SIZE=1024)\n\n    if target is not out:\n        out.copy_(target)\n    return out",
        "has_code": true
      },
      "aten::scalar_tensor": {
        "gpt_speedup_vs_cuda": 1.003317097640352,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _copy_scalar_kernel(in_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask, other=0)\n    tl.store(out_ptr + offsets, x, mask=mask)\n\n\ndef scalar_tensor(*args, **kwargs):\n    # Expected usage: scalar_tensor(value, *, dtype=None, device=None)\n    if len(args) == 0 and \"value\" not in kwargs:\n        raise TypeError(\"scalar_tensor expected a scalar 'value' as the first positional argument or 'value' kwarg\")\n    value = args[0] if len(args) > 0 else kwargs[\"value\"]\n    dtype = kwargs.get(\"dtype\", None)\n    device = kwargs.get(\"device\", None)\n\n    # Prepare output 0-d tensor\n    if isinstance(value, torch.Tensor) and dtype is None:\n        inferred_dtype = value.dtype\n    else:\n        inferred_dtype = dtype\n    out = torch.empty((), dtype=inferred_dtype if inferred_dtype is not None else None, device=device)\n\n    # Prepare a 1-element input tensor on the same device/dtype as out\n    if isinstance(value, torch.Tensor):\n        if value.numel() != 1:\n            raise ValueError(\"scalar_tensor expects a scalar or 0-d/1-element tensor as input.\")\n        in_buf = value.to(device=out.device, dtype=out.dtype).reshape(1)\n    else:\n        in_buf = torch.tensor(value, device=out.device, dtype=out.dtype).reshape(1)\n\n    # Launch kernel to copy the single element into the 0-d output\n    n_elements = 1\n    grid = lambda meta: (1,)\n    _copy_scalar_kernel[grid](in_buf, out.view(1), n_elements, BLOCK_SIZE=1)\n    return out\n\n\ndef scalar_tensor_out(*args, **kwargs):\n    # Expected usage: scalar_tensor.out(value, *, dtype=None, device=None, out=...)\n    if len(args) == 0 and \"value\" not in kwargs:\n        raise TypeError(\"scalar_tensor_out expected a scalar 'value' as the first positional argument or 'value' kwarg\")\n    value = args[0] if len(args) > 0 else kwargs[\"value\"]\n\n    # 'out' can be provided as kwarg; attempt to also accept as last positional if provided (best-effort)\n    out = kwargs.get(\"out\", None)\n    if out is None and len(args) > 1:\n        out = args[1]\n    if out is None:\n        raise TypeError(\"scalar_tensor_out requires an 'out' tensor argument\")\n\n    if not isinstance(out, torch.Tensor):\n        raise TypeError(\"'out' must be a torch.Tensor\")\n    if out.numel() != 1:\n        raise ValueError(\"'out' must be a 0-d (numel==1) tensor\")\n    # dtype/device in kwargs are ignored for allocation since out is provided; we only cast the input accordingly\n\n    # Prepare a 1-element input tensor on the same device/dtype as out\n    if isinstance(value, torch.Tensor):\n        if value.numel() != 1:\n            raise ValueError(\"scalar_tensor_out expects a scalar or 0-d/1-element tensor as input.\")\n        in_buf = value.to(device=out.device, dtype=out.dtype).reshape(1)\n    else:\n        in_buf = torch.tensor(value, device=out.device, dtype=out.dtype).reshape(1)\n\n    # Launch kernel to copy the single element into the provided 'out'\n    n_elements = 1\n    grid = lambda meta: (1,)\n    _copy_scalar_kernel[grid](in_buf, out.view(1), n_elements, BLOCK_SIZE=1)\n    return out",
        "has_code": true
      },
      "aten::log2_": {
        "gpt_speedup_vs_cuda": 1.0006913050210484,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef log2_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    val = tl.load(x_ptr + offsets, mask=mask)\n    x = val.to(tl.float32)\n\n    inv_ln2 = tl.full((), 1.4426950408889634, tl.float32)  # 1 / ln(2)\n    y = tl.log(x) * inv_ln2\n\n    out = y.to(val.dtype)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n# Keep a reference to the Triton kernel before redefining the name for the Python wrapper.\n_log2__kernel = log2_\n\n\ndef log2_(*args, **kwargs):\n    x = args[0] if len(args) > 0 else kwargs.get('input', None)\n    if x is None:\n        raise ValueError(\"log2_ expects a tensor as the first argument.\")\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"log2_ expects a torch.Tensor as input.\")\n\n    # Handle empty tensors directly\n    if x.numel() == 0:\n        return x\n\n    # Fallback for non-CUDA tensors or unsupported dtypes\n    if (not x.is_cuda) or (x.dtype not in (torch.float16, torch.bfloat16, torch.float32)):\n        # Use PyTorch's implementation as a fallback\n        x.log2_()\n        return x\n\n    # Work on a contiguous buffer; copy back if needed\n    x_contig = x if x.is_contiguous() else x.contiguous()\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        return x\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _log2__kernel[grid](x_contig, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    if x_contig is not x:\n        x.copy_(x_contig)\n\n    return x",
        "has_code": true
      },
      "aten::deg2rad": {
        "gpt_speedup_vs_cuda": 1.000294411725973,
        "meets_threshold": true,
        "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef deg2rad_kernel(x_ptr, y_ptr, n_elements, scale, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = x * scale\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n\ndef _launch_deg2rad_kernel(x_contig: torch.Tensor, out_contig: torch.Tensor):\n    assert x_contig.is_cuda and out_contig.is_cuda, \"Tensors must be on CUDA device\"\n    n_elements = out_contig.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    scale = math.pi / 180.0\n    deg2rad_kernel[grid](\n        x_contig, out_contig, n_elements, scale, BLOCK_SIZE=1024\n    )\n\n\ndef deg2rad(*args, **kwargs):\n    # Expecting a single input tensor\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"input\", None)\n        if x is None:\n            x = kwargs.get(\"self\", None)\n    if x is None:\n        raise TypeError(\"deg2rad expected a single input tensor\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"deg2rad input must be a torch.Tensor\")\n\n    if not x.is_cuda:\n        raise AssertionError(\"deg2rad expects CUDA tensors\")\n\n    if x.is_complex():\n        raise NotImplementedError(\"Complex tensors are not supported in this Triton implementation\")\n\n    # Determine result dtype: keep floating dtype; promote integer/bool to float32\n    if x.is_floating_point():\n        result_dtype = x.dtype\n    else:\n        result_dtype = torch.float32\n\n    x_cast = x.to(result_dtype)\n    x_contig = x_cast.contiguous()\n    out = torch.empty_like(x_contig, dtype=result_dtype, device=x.device)\n\n    _launch_deg2rad_kernel(x_contig.view(-1), out.view(-1))\n\n    return out.view_as(x)\n\n\ndef deg2rad_out(*args, **kwargs):\n    # Expecting input tensor and out tensor, either as positional or keyword args\n    x = None\n    out = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"input\", None) or kwargs.get(\"self\", None)\n    if len(args) >= 2:\n        out = args[1]\n    else:\n        out = kwargs.get(\"out\", None)\n\n    if x is None or out is None:\n        raise TypeError(\"deg2rad_out expected (input, out) tensors\")\n\n    if not isinstance(x, torch.Tensor) or not isinstance(out, torch.Tensor):\n        raise TypeError(\"deg2rad_out arguments must be torch.Tensor\")\n\n    if not x.is_cuda or not out.is_cuda:\n        raise AssertionError(\"deg2rad_out expects CUDA tensors\")\n\n    if x.is_complex() or out.is_complex():\n        raise NotImplementedError(\"Complex tensors are not supported in this Triton implementation\")\n\n    if out.numel() != x.numel():\n        raise RuntimeError(\"deg2rad_out: 'out' must have the same number of elements as 'input'\")\n\n    if out.device != x.device:\n        raise RuntimeError(\"deg2rad_out: 'out' must be on the same device as 'input'\")\n\n    # Compute in the dtype of 'out' to match out-variant semantics\n    x_cast = x.to(out.dtype)\n    x_contig = x_cast.contiguous()\n\n    if out.is_contiguous():\n        out_contig = out\n        need_copy_back = False\n    else:\n        out_contig = torch.empty_like(out, memory_format=torch.contiguous_format)\n        need_copy_back = True\n\n    _launch_deg2rad_kernel(x_contig.view(-1), out_contig.view(-1))\n\n    if need_copy_back:\n        out.copy_(out_contig)\n\n    return out",
        "has_code": true
      },
      "aten::empty_permuted": {
        "gpt_speedup_vs_cuda": 1.0002436647951074,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef empty_permuted_kernel(\n    out_ptr,  # Pointer to output tensor data\n    n_elements,  # Number of elements in tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size for grid launch\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    # Intentionally do nothing to preserve \"empty\" (uninitialized) semantics.\n\n\ndef _compute_permuted_strides(size, permutation):\n    # Compute strides such that the tensor is contiguous when iterating in `permutation` order.\n    dim = len(size)\n    assert len(permutation) == dim, \"permutation must have the same number of dims as size\"\n    stride = [0] * dim\n    curr = 1\n    for idx in range(dim - 1, -1, -1):\n        d = permutation[idx]\n        s = int(size[d])\n        if s == 0:\n            s = 1\n        stride[d] = curr\n        curr *= s\n    return tuple(stride)\n\n\ndef _parse_args_empty_permuted(args, kwargs):\n    # Flexible parsing for factory-like signature: (size, permutation, *, dtype=None, device=None, layout=None, pin_memory=None)\n    size = kwargs.get(\"size\", None)\n    permutation = kwargs.get(\"permutation\", None)\n    dtype = kwargs.get(\"dtype\", None)\n    device = kwargs.get(\"device\", None)\n    layout = kwargs.get(\"layout\", None)\n    pin_memory = kwargs.get(\"pin_memory\", False)\n\n    # Positional fallback\n    if size is None and len(args) > 0:\n        size = args[0]\n    if permutation is None and len(args) > 1:\n        permutation = args[1]\n    if dtype is None and len(args) > 2 and isinstance(args[2], torch.dtype):\n        dtype = args[2]\n    if device is None and len(args) > 3 and isinstance(args[3], (torch.device, str, int)):\n        device = args[3]\n    if layout is None and len(args) > 4:\n        layout = args[4]\n    if len(args) > 5:\n        pin_memory = args[5]\n\n    if dtype is None:\n        dtype = torch.float32\n\n    if device is None:\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    return size, permutation, dtype, device, layout, pin_memory\n\n\ndef empty_permuted(*args, **kwargs):\n    size, permutation, dtype, device, layout, pin_memory = _parse_args_empty_permuted(args, kwargs)\n    assert size is not None and permutation is not None, \"size and permutation must be provided\"\n\n    strides = _compute_permuted_strides(list(size), list(permutation))\n    out = torch.empty_strided(size=list(size), stride=strides, dtype=dtype, device=device)\n\n    if out.is_cuda and out.numel() > 0:\n        n_elements = out.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        empty_permuted_kernel[grid](out, n_elements, BLOCK_SIZE=1024)\n\n    return out\n\n\ndef empty_permuted_out(*args, **kwargs):\n    # Expected signature similar to: (out, size, permutation, *, dtype=None, device=None, layout=None, pin_memory=None)\n    out = kwargs.get(\"out\", None)\n    positional = list(args)\n\n    if out is None and len(positional) > 0 and isinstance(positional[0], torch.Tensor):\n        out = positional.pop(0)\n\n    size, permutation, dtype, device, layout, pin_memory = _parse_args_empty_permuted(positional, kwargs)\n    assert size is not None and permutation is not None, \"size and permutation must be provided\"\n\n    if out is None:\n        # Fallback: create a new tensor if 'out' is not provided\n        strides = _compute_permuted_strides(list(size), list(permutation))\n        out = torch.empty_strided(size=list(size), stride=strides, dtype=dtype, device=device)\n    else:\n        # Resize the provided 'out' to requested size; cannot change strides in-place.\n        if list(out.size()) != list(size):\n            out.resize_(list(size))\n        # Ensure dtype/device, move if needed\n        if dtype is not None and out.dtype != dtype:\n            out = out.to(dtype)\n        if device is not None and out.device != torch.device(device):\n            out = out.to(device)\n\n    if out.is_cuda and out.numel() > 0:\n        n_elements = out.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        empty_permuted_kernel[grid](out, n_elements, BLOCK_SIZE=1024)\n\n    return out",
        "has_code": true
      },
      "aten::square": {
        "gpt_speedup_vs_cuda": 0.9960885509312877,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n\n@triton.jit\ndef square_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = x * x\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _launch_square(x: torch.Tensor, out: torch.Tensor):\n    n_elements = out.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    square_kernel[grid](x, out, n_elements, BLOCK_SIZE=1024)\n\n\ndef square(input: torch.Tensor, *, out: Optional[torch.Tensor] = None):\n    assert isinstance(input, torch.Tensor), \"input must be a torch.Tensor\"\n    assert input.is_cuda, \"input must be on a CUDA device\"\n    if out is None:\n        out = torch.empty_like(input)\n    else:\n        assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n        assert out.is_cuda, \"out must be on a CUDA device\"\n        assert out.dtype == input.dtype, \"out dtype must match input dtype\"\n        assert out.numel() == input.numel(), \"out must have the same number of elements as input\"\n        assert out.shape == input.shape, \"out must have the same shape as input\"\n\n    x_c = input.contiguous()\n    out_c = out.contiguous()\n\n    _launch_square(x_c, out_c)\n\n    if out_c.data_ptr() != out.data_ptr():\n        out.copy_(out_c)\n    return out\n\n\ndef square_out(input: torch.Tensor, out: torch.Tensor):\n    assert isinstance(input, torch.Tensor), \"input must be a torch.Tensor\"\n    assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n    assert input.is_cuda and out.is_cuda, \"input and out must be on a CUDA device\"\n    assert out.dtype == input.dtype, \"out dtype must match input dtype\"\n    assert out.numel() == input.numel(), \"out must have the same number of elements as input\"\n    assert out.shape == input.shape, \"out must have the same shape as input\"\n\n    x_c = input.contiguous()\n    out_c = out.contiguous()\n\n    _launch_square(x_c, out_c)\n\n    if out_c.data_ptr() != out.data_ptr():\n        out.copy_(out_c)\n    return out",
        "has_code": true
      },
      "aten::negative_": {
        "gpt_speedup_vs_cuda": 0.9940189425876468,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef negative_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x = -x\n    tl.store(x_ptr + offsets, x, mask=mask)\n\n\n_negative__kernel = negative_\n\n\ndef negative_(*args, **kwargs):\n    x = args[0] if len(args) > 0 else kwargs.get('input', kwargs.get('self', None))\n    if x is None:\n        raise ValueError(\"negative_ expects a tensor as the first argument\")\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    assert x.is_contiguous(), \"Input tensor must be contiguous\"\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _negative__kernel[grid](x, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::new_ones": {
        "gpt_speedup_vs_cuda": 0.9897666212236619,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fill_ones_kernel(\n    out_ptr,  # *Pointer* to output tensor\n    n_elements,  # Total number of elements to write\n    BLOCK_SIZE: tl.constexpr,\n    DTYPE: tl.constexpr,  # Triton dtype for output\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    ones = tl.full((BLOCK_SIZE,), 1, dtype=DTYPE)\n    tl.store(out_ptr + offsets, ones, mask=mask)\n\n\ndef _torch_dtype_to_triton_dtype(dtype: torch.dtype):\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    if dtype == torch.float64:\n        return tl.float64\n    if dtype == torch.int8:\n        return tl.int8\n    if dtype == torch.uint8:\n        return tl.uint8\n    if dtype == torch.int16:\n        return tl.int16\n    if dtype == torch.int32:\n        return tl.int32\n    if dtype == torch.int64:\n        return tl.int64\n    # Best-effort for bool\n    if dtype == torch.bool:\n        # Triton represents boolean as int1\n        return tl.int1\n    raise TypeError(f\"Unsupported dtype for Triton kernel: {dtype}\")\n\n\ndef _normalize_size_arg(size):\n    if isinstance(size, torch.Size):\n        return tuple(int(s) for s in size)\n    if isinstance(size, (list, tuple)):\n        return tuple(int(s) for s in size)\n    if isinstance(size, int):\n        return (int(size),)\n    raise TypeError(f\"Invalid size specification: {size}\")\n\n\ndef _extract_size_from_args(after_self_pos_args, kwargs):\n    # Try positional\n    if len(after_self_pos_args) == 1 and isinstance(after_self_pos_args[0], (list, tuple, torch.Size, int)):\n        return _normalize_size_arg(after_self_pos_args[0])\n    if len(after_self_pos_args) >= 1 and all(isinstance(x, int) for x in after_self_pos_args):\n        return tuple(int(x) for x in after_self_pos_args)\n    # Try kwargs\n    if \"size\" in kwargs:\n        return _normalize_size_arg(kwargs[\"size\"])\n    if \"sizes\" in kwargs:\n        return _normalize_size_arg(kwargs[\"sizes\"])\n    raise ValueError(\"Size must be provided as a list/tuple/torch.Size or as multiple integer positional arguments.\")\n\n\ndef _launch_fill_ones(out: torch.Tensor):\n    if out.numel() == 0:\n        return\n    if not out.is_cuda or not out.is_contiguous():\n        out.fill_(1)\n        return\n    triton_dtype = _torch_dtype_to_triton_dtype(out.dtype)\n    n_elements = out.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _fill_ones_kernel[grid](out, n_elements, BLOCK_SIZE=BLOCK_SIZE, DTYPE=triton_dtype)\n\n\ndef new_ones(*args, **kwargs):\n    # Expected schema (aten): new_ones(Tensor self, int[] size, *, dtype?, layout?, device?, pin_memory?)\n    if len(args) == 0:\n        raise TypeError(\"new_ones expects at least a 'self' tensor as the first argument.\")\n    self = args[0]\n    after_self = list(args[1:])\n    size = _extract_size_from_args(after_self, kwargs)\n\n    # Resolve dtype/device defaults\n    dtype = kwargs.get(\"dtype\", None)\n    if dtype is None:\n        if isinstance(self, torch.Tensor) and self.dtype is not None:\n            dtype = self.dtype\n        else:\n            dtype = torch.get_default_dtype()\n\n    device = kwargs.get(\"device\", None)\n    if device is None:\n        if isinstance(self, torch.Tensor):\n            device = self.device\n        else:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    out = torch.empty(size, dtype=dtype, device=device)\n    _launch_fill_ones(out)\n    return out\n\n\ndef new_ones_out(*args, **kwargs):\n    # Expected schema (aten): new_ones.out(Tensor self, int[] size, *, dtype?, layout?, device?, pin_memory?, Tensor(a!) out) -> Tensor(a!)\n    if len(args) == 0:\n        raise TypeError(\"new_ones_out expects at least a 'self' tensor as the first argument.\")\n    self = args[0]\n    # Determine 'out' tensor (either in kwargs or as the last positional argument)\n    out = kwargs.get(\"out\", None)\n    pos_after_self = list(args[1:])\n\n    if out is None and len(pos_after_self) >= 1 and isinstance(pos_after_self[-1], torch.Tensor):\n        out = pos_after_self[-1]\n        pos_after_self = pos_after_self[:-1]\n\n    if out is None or not isinstance(out, torch.Tensor):\n        raise TypeError(\"new_ones_out requires an 'out' tensor (as keyword 'out' or last positional argument).\")\n\n    size = _extract_size_from_args(pos_after_self, kwargs)\n\n    # Validate/align dtype/device if provided\n    if \"dtype\" in kwargs and kwargs[\"dtype\"] is not None and out.dtype != kwargs[\"dtype\"]:\n        raise TypeError(f\"Provided dtype {kwargs['dtype']} does not match out.dtype {out.dtype}.\")\n    if \"device\" in kwargs and kwargs[\"device\"] is not None and torch.device(kwargs[\"device\"]) != out.device:\n        raise TypeError(f\"Provided device {kwargs['device']} does not match out.device {out.device}.\")\n\n    # Resize out to requested size if needed\n    if tuple(out.shape) != tuple(size):\n        out.resize_(size)\n\n    _launch_fill_ones(out)\n    return out",
        "has_code": true
      },
      "aten::hardshrink": {
        "gpt_speedup_vs_cuda": 0.9824522663642185,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef hardshrink_kernel(x_ptr, out_ptr, n_elements, lambd, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    l = lambd\n    keep = (x > l) | (x < -l)\n    y = tl.where(keep, x, 0.0)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _hardshrink_launch(x: torch.Tensor, lambd: float, out: torch.Tensor):\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    assert out.is_cuda, \"Output tensor must be on CUDA device\"\n    assert x.numel() == out.numel(), \"Input and output must have the same number of elements\"\n    assert x.dtype == out.dtype, \"Input and output must have the same dtype\"\n    assert x.is_floating_point(), \"hardshrink only supports floating point dtypes\"\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    hardshrink_kernel[grid](x, out, n_elements, float(lambd), BLOCK_SIZE=BLOCK_SIZE)\n\n\ndef hardshrink(x: torch.Tensor, lambd: float = 0.5) -> torch.Tensor:\n    x_c = x.contiguous()\n    out = torch.empty_like(x_c)\n    _hardshrink_launch(x_c, lambd, out)\n    return out\n\n\ndef hardshrink_out(x: torch.Tensor, lambd: float = 0.5, out: torch.Tensor = None) -> torch.Tensor:\n    x_c = x.contiguous()\n    if out is None:\n        out = torch.empty_like(x_c)\n        _hardshrink_launch(x_c, lambd, out)\n        return out\n    # Ensure output is allocated correctly\n    assert out.is_cuda, \"Output tensor must be on CUDA device\"\n    assert out.dtype == x_c.dtype, \"Output dtype must match input dtype\"\n    assert out.shape == x_c.shape, \"Output shape must match input shape\"\n\n    if out.is_contiguous():\n        _hardshrink_launch(x_c, lambd, out)\n    else:\n        tmp = torch.empty_like(x_c)\n        _hardshrink_launch(x_c, lambd, tmp)\n        out.copy_(tmp)\n    return out",
        "has_code": true
      },
      "aten::arctanh": {
        "gpt_speedup_vs_cuda": 0.907158760560685,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef arctanh_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x_f32 = x.to(tl.float32)\n\n    one = 1.0\n    # atanh(x) = 0.5 * (log(1 + x) - log(1 - x))\n    y_f32 = 0.5 * (tl.log(one + x_f32) - tl.log(one - x_f32))\n    y = y_f32.to(x.dtype)\n\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _launch_arctanh(x: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and out.is_cuda, \"Input and output must be CUDA tensors\"\n    assert x.shape == out.shape, \"Input and output shapes must match\"\n    assert out.dtype == x.dtype, \"Output dtype must match input dtype\"\n    assert x.dtype in (torch.float16, torch.bfloat16, torch.float32), \"Supported dtypes: float16, bfloat16, float32\"\n\n    x_contig = x.contiguous()\n    out_contig = out if out.is_contiguous() else torch.empty_like(out)\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        if out_contig is not out:\n            out.copy_(out_contig)\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    arctanh_kernel[grid](x_contig, out_contig, n_elements, BLOCK_SIZE=1024)\n\n    if out_contig is not out:\n        out.copy_(out_contig)\n    return out\n\n\ndef arctanh(x: torch.Tensor):\n    out = torch.empty_like(x)\n    _launch_arctanh(x, out)\n    return out\n\n\ndef arctanh_out(x: torch.Tensor, out: torch.Tensor):\n    _launch_arctanh(x, out)\n    return out",
        "has_code": true
      },
      "aten::erfinv": {
        "gpt_speedup_vs_cuda": 0.8534740811869269,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef erfinv_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    xf = x.to(tl.float32)\n\n    one = 1.0\n    absx = tl.abs(xf)\n    w = -tl.log((one - xf) * (one + xf))\n\n    use_low = w < 5.0\n\n    wl = w - 2.5\n    pl = 2.81022636e-08\n    pl = 3.43273939e-07 + pl * wl\n    pl = -3.5233877e-06 + pl * wl\n    pl = -4.39150654e-06 + pl * wl\n    pl = 2.1858087e-04 + pl * wl\n    pl = -1.25372503e-03 + pl * wl\n    pl = -4.17768164e-03 + pl * wl\n    pl = 2.46640727e-01 + pl * wl\n    pl = 1.50140941e+00 + pl * wl\n\n    wh = tl.sqrt(w) - 3.0\n    ph = -2.00214257e-04\n    ph = 1.00950558e-04 + ph * wh\n    ph = 1.34934322e-03 + ph * wh\n    ph = -3.67342844e-03 + ph * wh\n    ph = 5.73950773e-03 + ph * wh\n    ph = -7.62246130e-03 + ph * wh\n    ph = 9.43887047e-03 + ph * wh\n    ph = 1.00167406e+00 + ph * wh\n    ph = 2.83297682e+00 + ph * wh\n\n    p = tl.where(use_low, pl, ph)\n    res = p * xf\n\n    nan_vec = tl.full([BLOCK_SIZE], float('nan'), dtype=tl.float32)\n    inf_vec = tl.full([BLOCK_SIZE], float('inf'), dtype=tl.float32)\n\n    mask_nan = xf != xf\n    mask_oob = absx > 1.0\n    mask_pos1 = xf == 1.0\n    mask_neg1 = xf == -1.0\n\n    res = tl.where(mask_nan, nan_vec, res)\n    res = tl.where(mask_oob, nan_vec, res)\n    res = tl.where(mask_pos1, inf_vec, res)\n    res = tl.where(mask_neg1, -inf_vec, res)\n\n    y = res.to(x.dtype)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _launch_erfinv_kernel(x: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and out.is_cuda, \"Inputs must be CUDA tensors\"\n    assert x.numel() == out.numel(), \"Input and output must have the same number of elements\"\n    assert x.dtype == out.dtype, \"Input and output must have the same dtype\"\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    erfinv_kernel[grid](\n        x, out, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n\ndef erfinv(x: torch.Tensor):\n    x_in = x\n    if not x_in.is_contiguous():\n        x_in = x_in.contiguous()\n    out = torch.empty_like(x_in)\n    _launch_erfinv_kernel(x_in, out)\n    # Match original shape/strides of input if needed\n    if out.shape != x.shape or out.stride() != x.stride():\n        out = out.reshape(x.shape).as_strided(x.size(), x.stride())\n    return out\n\n\ndef erfinv_out(x: torch.Tensor, out: torch.Tensor):\n    # Resize out to match input shape if necessary\n    if out.shape != x.shape:\n        out.resize_(x.shape)\n    # Ensure dtype matches input dtype for aten out semantics\n    assert out.dtype == x.dtype, \"out tensor must have the same dtype as input\"\n    x_in = x if x.is_contiguous() else x.contiguous()\n    if out.is_contiguous():\n        _launch_erfinv_kernel(x_in, out)\n        return out\n    else:\n        tmp = torch.empty_like(out, memory_format=torch.contiguous_format)\n        _launch_erfinv_kernel(x_in, tmp)\n        out.copy_(tmp)\n        return out",
        "has_code": true
      },
      "aten::less_equal_": {
        "gpt_speedup_vs_cuda": 0.8005147731892174,
        "meets_threshold": true,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef less_equal_scalar_kernel(x_ptr, out_ptr, scalar_val, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    cond = x <= scalar_val\n\n    one = tl.full([BLOCK_SIZE], 1.0, dtype=tl.float32)\n    zero = tl.full([BLOCK_SIZE], 0.0, dtype=tl.float32)\n    out = tl.where(cond, one, zero)\n\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\n@triton.jit\ndef less_equal_tensor_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n    cond = x <= y\n\n    one = tl.full([BLOCK_SIZE], 1.0, dtype=tl.float32)\n    zero = tl.full([BLOCK_SIZE], 0.0, dtype=tl.float32)\n    out = tl.where(cond, one, zero)\n\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\ndef less_equal__Scalar(*args, **kwargs):\n    # Expected usage: less_equal__Scalar(self, other_scalar)\n    if len(args) >= 2:\n        self, other = args[0], args[1]\n    else:\n        self = kwargs.get(\"self\", None)\n        other = kwargs.get(\"other\", None)\n    assert isinstance(self, torch.Tensor), \"self must be a torch.Tensor\"\n    assert self.is_cuda, \"Tensor must be on CUDA device for Triton kernel\"\n    assert self.numel() >= 0\n\n    # Prepare working buffer in float32 contiguous\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    self_contig_f32 = self.contiguous().to(torch.float32)\n\n    # Convert scalar to float32\n    # Accept Python numbers and 0-dim tensors\n    if isinstance(other, torch.Tensor):\n        assert other.numel() == 1, \"Scalar variant expects a scalar value\"\n        scalar_f32 = other.to(device=self.device, dtype=torch.float32).item()\n    else:\n        scalar_f32 = float(other)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    less_equal_scalar_kernel[grid](\n        self_contig_f32,  # x_ptr\n        self_contig_f32,  # out_ptr (in-place on temp buffer)\n        scalar_f32,\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Copy results back to original tensor dtype/layout\n    if self.is_contiguous():\n        # Direct copy if same storage layout\n        self.copy_(self_contig_f32.to(self.dtype))\n    else:\n        self.copy_(self_contig_f32.to(self.dtype).view(self.shape))\n\n    return self\n\n\ndef less_equal__Tensor(*args, **kwargs):\n    # Expected usage: less_equal__Tensor(self, other_tensor)\n    if len(args) >= 2:\n        self, other = args[0], args[1]\n    else:\n        self = kwargs.get(\"self\", None)\n        other = kwargs.get(\"other\", None)\n    assert isinstance(self, torch.Tensor) and isinstance(other, torch.Tensor), \"Inputs must be torch.Tensors\"\n    assert self.is_cuda and other.is_cuda, \"Tensors must be on CUDA device for Triton kernel\"\n\n    # Prepare working buffers in float32 contiguous\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n\n    self_contig_f32 = self.contiguous().to(torch.float32)\n    other_expanded = other.to(device=self.device, dtype=torch.float32).expand_as(self)\n    other_contig_f32 = other_expanded.contiguous()\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    less_equal_tensor_kernel[grid](\n        self_contig_f32,  # x_ptr\n        other_contig_f32,  # y_ptr\n        self_contig_f32,   # out_ptr (in-place on temp buffer)\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Copy results back to original tensor dtype/layout\n    if self.is_contiguous():\n        self.copy_(self_contig_f32.to(self.dtype))\n    else:\n        self.copy_(self_contig_f32.to(self.dtype).view(self.shape))\n\n    return self",
        "has_code": true
      },
      "aten::zero_": {
        "gpt_speedup_vs_cuda": 0.6005817558032245,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef zero_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    # Load to obtain dtype, then compute zeros of the same dtype\n    vals = tl.load(x_ptr + offsets, mask=mask, other=0)\n    zeros = vals - vals\n    tl.store(x_ptr + offsets, zeros, mask=mask)\n\n\n_zero_kernel = zero_\n\n\ndef zero_(*args, **kwargs):\n    # Extract the tensor argument\n    x = None\n    if len(args) > 0:\n        x = args[0]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\", None))\n    if x is None:\n        raise ValueError(\"zero_ expects a Tensor as the first argument.\")\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"zero_ expects a torch.Tensor.\")\n\n    # Handle empty tensor quickly\n    if x.numel() == 0:\n        return x\n\n    # Fallback for non-CUDA or non-contiguous tensors\n    if (not x.is_cuda) or (not x.is_contiguous()):\n        x.zero_()\n        return x\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _zero_kernel[grid](x, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true
      },
      "aten::sinc": {
        "gpt_speedup_vs_cuda": 0.5448625168064117,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef sinc_kernel_fp32(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)  # fp32\n    y = x * 3.141592653589793\n    siny = tl.sin(y)\n    val = siny / y\n    out = tl.where(x == 0.0, 1.0, val)\n\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\ndef sinc(input: torch.Tensor):\n    x_fp32 = input.contiguous().to(torch.float32)\n    out_fp32 = torch.empty_like(x_fp32)\n    n_elements = x_fp32.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    sinc_kernel_fp32[grid](x_fp32, out_fp32, n_elements, BLOCK_SIZE=1024)\n\n    if input.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        return out_fp32.to(input.dtype)\n    else:\n        return out_fp32\n\n\ndef sinc_out(input: torch.Tensor, out: torch.Tensor):\n    x_fp32 = input.contiguous().to(torch.float32)\n    out_fp32 = torch.empty_like(x_fp32)\n    n_elements = x_fp32.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    sinc_kernel_fp32[grid](x_fp32, out_fp32, n_elements, BLOCK_SIZE=1024)\n\n    out.copy_(out_fp32.to(out.dtype))\n    return out",
        "has_code": true
      },
      "aten::threshold_": {
        "gpt_speedup_vs_cuda": 0.5274580788741771,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef threshold_(x_ptr,  # Pointer to input/output tensor (in-place)\n               n_elements,  # Number of elements\n               threshold_ptr,  # Pointer to scalar threshold (0-d tensor)\n               value_ptr,  # Pointer to scalar value (0-d tensor)\n               BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load data\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Load scalars (dtype matches x because we pass 0-d tensors of x.dtype)\n    thr = tl.load(threshold_ptr)\n    val = tl.load(value_ptr)\n\n    # Apply threshold in-place: if x <= thr, set to val, else keep x\n    out = tl.where(x <= thr, val, x)\n\n    # Store back\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n# Keep a handle to the Triton kernel before defining the Python wrapper of the same name\nthreshold__triton_kernel = threshold_\n\n\ndef threshold_(*args, **kwargs):\n    # Extract arguments similar to aten.threshold_ signature: (self, threshold, value=0)\n    x = kwargs.get('input', args[0] if len(args) > 0 else None)\n    threshold = kwargs.get('threshold', args[1] if len(args) > 1 else None)\n    value = kwargs.get('value', args[2] if len(args) > 2 else 0)\n\n    if x is None or threshold is None:\n        raise ValueError(\"threshold_ requires at least (input, threshold) arguments\")\n\n    if not x.is_cuda:\n        raise ValueError(\"Input tensor must be on CUDA device for Triton kernel execution\")\n    if x.is_complex():\n        raise ValueError(\"Complex dtypes are not supported by this kernel\")\n    if not x.is_contiguous():\n        raise ValueError(\"Input tensor must be contiguous for this Triton kernel\")\n\n    n_elements = x.numel()\n\n    # Prepare scalar tensors for threshold and value with matching dtype/device\n    thr_t = torch.tensor(threshold, dtype=x.dtype, device=x.device)\n    val_t = torch.tensor(value, dtype=x.dtype, device=x.device)\n\n    # Launch configuration\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    # Launch the Triton kernel (in-place)\n    threshold__triton_kernel[grid](x, n_elements, thr_t, val_t, BLOCK_SIZE=BLOCK_SIZE)\n\n    return x",
        "has_code": true
      },
      "aten::special_xlog1py": {
        "gpt_speedup_vs_cuda": 0.5126638366779087,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _xlog1py_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    out = x * tl.log(1.0 + y)\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\ndef _ensure_cuda_tensor(t):\n    if not isinstance(t, torch.Tensor):\n        raise TypeError(\"Expected a torch.Tensor\")\n    if t.device.type != \"cuda\":\n        raise ValueError(\"Tensors must be on CUDA device\")\n    return t\n\n\ndef _prepare_inputs(x, y):\n    x = _ensure_cuda_tensor(x)\n    y = _ensure_cuda_tensor(y)\n    xb, yb = torch.broadcast_tensors(x, y)\n    dtype_out = torch.result_type(xb, yb)\n    xb_fp32 = xb.to(torch.float32).contiguous()\n    yb_fp32 = yb.to(torch.float32).contiguous()\n    return xb_fp32, yb_fp32, dtype_out\n\n\ndef _launch_xlog1py(x_fp32, y_fp32, out_fp32):\n    n_elements = out_fp32.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _xlog1py_kernel[grid](x_fp32, y_fp32, out_fp32, n_elements, BLOCK_SIZE=1024)\n\n\ndef special_xlog1py(x, y):\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(x, y)\n    out_fp32 = torch.empty_like(xb_fp32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    if dtype_out == torch.float32:\n        return out_fp32\n    else:\n        return out_fp32.to(dtype_out)\n\n\ndef special_xlog1py_other_scalar(x, other):\n    x = _ensure_cuda_tensor(x)\n    other_tensor = torch.as_tensor(other, device=x.device, dtype=x.dtype)\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(x, other_tensor)\n    out_fp32 = torch.empty_like(xb_fp32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    if dtype_out == torch.float32:\n        return out_fp32\n    else:\n        return out_fp32.to(dtype_out)\n\n\ndef special_xlog1py_self_scalar(self, other):\n    other = _ensure_cuda_tensor(other)\n    self_tensor = torch.as_tensor(self, device=other.device, dtype=other.dtype)\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(self_tensor, other)\n    out_fp32 = torch.empty_like(xb_fp32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    if dtype_out == torch.float32:\n        return out_fp32\n    else:\n        return out_fp32.to(dtype_out)\n\n\ndef special_xlog1py_out(x, y, out):\n    out = _ensure_cuda_tensor(out)\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(_ensure_cuda_tensor(x), _ensure_cuda_tensor(y))\n    # Validate output shape\n    expected_shape = torch.broadcast_shapes(xb_fp32.shape, yb_fp32.shape)\n    if out.shape != expected_shape:\n        raise ValueError(f\"Out tensor has shape {out.shape}, expected {expected_shape}\")\n    out_fp32 = torch.empty(expected_shape, device=out.device, dtype=torch.float32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    out.copy_(out_fp32.to(out.dtype))\n    return out\n\n\ndef special_xlog1py_self_scalar_out(self, other, out):\n    out = _ensure_cuda_tensor(out)\n    other = _ensure_cuda_tensor(other)\n    self_tensor = torch.as_tensor(self, device=other.device, dtype=other.dtype)\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(self_tensor, other)\n    expected_shape = torch.broadcast_shapes(xb_fp32.shape, yb_fp32.shape)\n    if out.shape != expected_shape:\n        raise ValueError(f\"Out tensor has shape {out.shape}, expected {expected_shape}\")\n    out_fp32 = torch.empty(expected_shape, device=out.device, dtype=torch.float32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    out.copy_(out_fp32.to(out.dtype))\n    return out\n\n\ndef special_xlog1py_other_scalar_out(x, other, out):\n    out = _ensure_cuda_tensor(out)\n    x = _ensure_cuda_tensor(x)\n    other_tensor = torch.as_tensor(other, device=x.device, dtype=x.dtype)\n    xb_fp32, yb_fp32, dtype_out = _prepare_inputs(x, other_tensor)\n    expected_shape = torch.broadcast_shapes(xb_fp32.shape, yb_fp32.shape)\n    if out.shape != expected_shape:\n        raise ValueError(f\"Out tensor has shape {out.shape}, expected {expected_shape}\")\n    out_fp32 = torch.empty(expected_shape, device=out.device, dtype=torch.float32)\n    _launch_xlog1py(xb_fp32, yb_fp32, out_fp32)\n    out.copy_(out_fp32.to(out.dtype))\n    return out",
        "has_code": true
      },
      "aten::fft_ifftshift": {
        "gpt_speedup_vs_cuda": 0.3644779188025096,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fft_ifftshift(in_ptr_u8,  # pointer to input tensor as bytes\n                  out_ptr_u8,  # pointer to output tensor as bytes\n                  sizes_ptr,  # int64[NDIMS]\n                  in_strides_ptr,  # int64[NDIMS], in elements\n                  out_strides_ptr,  # int64[NDIMS], in elements\n                  adds_ptr,  # int64[NDIMS], per-dim add = floor(size/2) if shifted else 0\n                  n_elements,  # total number of elements\n                  ELEMENT_SIZE: tl.constexpr,  # number of bytes per element\n                  NDIMS: tl.constexpr,  # number of dimensions\n                  BLOCK_SIZE: tl.constexpr  # tile size\n                  ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offs = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n    offs64 = offs.to(tl.int64)\n\n    # Compute multi-dimensional indices from linear index (row-major)\n    tmp = offs64\n    in_off_elems = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    out_off_elems = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Iterate from last dim to first to compute indices\n    for d in range(NDIMS - 1, -1, -1):\n        size_d = tl.load(sizes_ptr + d)  # scalar int64\n        idx_d = tmp % size_d\n        tmp = tmp // size_d\n\n        add_d = tl.load(adds_ptr + d)  # scalar int64\n        idx_in_d = idx_d + add_d\n        # modulo to wrap within size_d\n        idx_in_d = idx_in_d - (idx_in_d // size_d) * size_d\n\n        in_stride_d = tl.load(in_strides_ptr + d)\n        out_stride_d = tl.load(out_strides_ptr + d)\n\n        in_off_elems += idx_in_d * in_stride_d\n        out_off_elems += idx_d * out_stride_d\n\n    in_byte_base = in_off_elems * ELEMENT_SIZE\n    out_byte_base = out_off_elems * ELEMENT_SIZE\n\n    # Copy ELEMENT_SIZE bytes per element\n    for b in range(ELEMENT_SIZE):\n        src_addr = in_ptr_u8 + in_byte_base + b\n        dst_addr = out_ptr_u8 + out_byte_base + b\n        val = tl.load(src_addr, mask=mask, other=0)\n        tl.store(dst_addr, val, mask=mask)\n\n\n# Keep a handle to the kernel before defining the wrapper with the same name\nfft_ifftshift_kernel = fft_ifftshift\n\n\ndef fft_ifftshift(*args, **kwargs):\n    x = None\n    dims = None\n\n    # Parse input tensor\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        # try kwargs\n        x = kwargs.get('input', None) or kwargs.get('self', None) or kwargs.get('tensor', None)\n    if x is None:\n        raise ValueError(\"fft_ifftshift expects at least one tensor argument as input.\")\n\n    # Parse dims (can be in args[1], or kwargs 'dim'/'dims')\n    if len(args) >= 2:\n        dims = args[1]\n    else:\n        dims = kwargs.get('dim', kwargs.get('dims', None))\n\n    # Normalize dims\n    if dims is None:\n        dims_list = list(range(x.ndim))\n    else:\n        if isinstance(dims, int):\n            dims_list = [dims]\n        else:\n            dims_list = list(dims)\n        # normalize negative dims\n        dims_list = [(d + x.ndim) % x.ndim for d in dims_list]\n        # remove duplicates while preserving order\n        seen = set()\n        tmp = []\n        for d in dims_list:\n            if d not in seen:\n                tmp.append(d)\n                seen.add(d)\n        dims_list = tmp\n\n    # Handle scalars or empty tensors quickly\n    if x.ndim == 0 or x.numel() == 0:\n        return x.clone()\n\n    device = x.device\n    dtype = x.dtype\n    out = torch.empty_like(x)\n\n    # Prepare metadata\n    sizes = torch.tensor(list(x.shape), device=device, dtype=torch.int64)\n    in_strides = torch.tensor(list(x.stride()), device=device, dtype=torch.int64)\n    out_strides = torch.tensor(list(out.stride()), device=device, dtype=torch.int64)\n\n    # Per-dimension add amount = floor(size/2) if dimension is included, else 0\n    add_list = [(sizes[d].item() // 2) if d in set(dims_list) else 0 for d in range(x.ndim)]\n    adds = torch.tensor(add_list, device=device, dtype=torch.int64)\n\n    n_elements = x.numel()\n    NDIMS = x.ndim\n    ELEMENT_SIZE = x.element_size()\n\n    # Use byte pointers by viewing as uint8 without changing storage\n    x_u8 = x.view(torch.uint8)\n    out_u8 = out.view(torch.uint8)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    BLOCK_SIZE = 1024\n\n    fft_ifftshift_kernel[grid](\n        x_u8, out_u8,\n        sizes, in_strides, out_strides, adds,\n        n_elements,\n        ELEMENT_SIZE=ELEMENT_SIZE,\n        NDIMS=NDIMS,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return out",
        "has_code": true
      },
      "aten::unsqueeze_copy": {
        "gpt_speedup_vs_cuda": 0.2942516151232812,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _unsqueeze_copy_kernel(\n    src_ptr,          # pointer to input tensor data\n    dst_ptr,          # pointer to output tensor data\n    sizes_ptr,        # pointer to int64 sizes of src tensor (NDIM)\n    src_strides_ptr,  # pointer to int64 strides of src tensor (NDIM)\n    dst_strides_ptr,  # pointer to int64 strides of dst tensor (NDIM + 1)\n    n_elements,       # total number of elements to copy (src.numel() == dst.numel())\n    NDIM: tl.constexpr,\n    INSERT_DIM: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offs = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n\n    # use int64 for index math\n    offs = offs.to(tl.int64)\n\n    # Compute source and destination element offsets using shape/strides\n    src_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    dst_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    rem = offs\n    # Decompose linear index into multi-dimensional indices (row-major order)\n    for rev_d in range(NDIM - 1, -1, -1):\n        sz_d = tl.load(sizes_ptr + rev_d)       # scalar int64\n        idx_d = rem % sz_d\n        rem = rem // sz_d\n\n        sstride_d = tl.load(src_strides_ptr + rev_d)\n        src_off += idx_d * sstride_d\n\n        # Map source dim rev_d to destination dim (account for inserted dim)\n        if rev_d < INSERT_DIM:\n            dstride_d = tl.load(dst_strides_ptr + rev_d)\n            dst_off += idx_d * dstride_d\n        else:\n            dstride_shift = tl.load(dst_strides_ptr + (rev_d + 1))\n            dst_off += idx_d * dstride_shift\n\n    vals = tl.load(src_ptr + src_off, mask=mask)\n    tl.store(dst_ptr + dst_off, vals, mask=mask)\n\n\ndef _launch_unsqueeze_copy(src: torch.Tensor, dim: int, out: torch.Tensor):\n    assert src.is_cuda and out.is_cuda, \"Tensors must be on CUDA device\"\n    assert src.dtype == out.dtype, \"Dtype mismatch between src and out\"\n\n    n_elements = src.numel()\n    if n_elements == 0:\n        return  # nothing to copy\n\n    # Build metadata arrays on device\n    sizes = torch.tensor(list(src.shape), dtype=torch.int64, device=src.device)\n    src_strides = torch.tensor(list(src.stride()), dtype=torch.int64, device=src.device)\n    dst_strides = torch.tensor(list(out.stride()), dtype=torch.int64, device=out.device)\n\n    grid = lambda META: (triton.cdiv(n_elements, META[\"BLOCK_SIZE\"]),)\n    _unsqueeze_copy_kernel[grid](\n        src,\n        out,\n        sizes,\n        src_strides,\n        dst_strides,\n        n_elements,\n        NDIM=src.dim(),\n        INSERT_DIM=dim,\n        BLOCK_SIZE=1024,\n    )\n\n\ndef unsqueeze_copy(x: torch.Tensor, dim: int):\n    # Normalize dim\n    dim_normalized = dim if dim >= 0 else dim + x.dim() + 1\n    if not (0 <= dim_normalized <= x.dim()):\n        raise IndexError(f\"dim {dim} out of range for tensor with {x.dim()} dims\")\n\n    new_shape = list(x.shape)\n    new_shape.insert(dim_normalized, 1)\n    out = torch.empty(new_shape, device=x.device, dtype=x.dtype)\n\n    _launch_unsqueeze_copy(x, dim_normalized, out)\n    return out\n\n\ndef unsqueeze_copy_out(x: torch.Tensor, dim: int, out: torch.Tensor):\n    # Normalize dim\n    dim_normalized = dim if dim >= 0 else dim + x.dim() + 1\n    if not (0 <= dim_normalized <= x.dim()):\n        raise IndexError(f\"dim {dim} out of range for tensor with {x.dim()} dims\")\n\n    if out.device != x.device:\n        raise ValueError(\"out tensor must be on the same device as input\")\n    if out.dtype != x.dtype:\n        raise ValueError(\"out tensor must have the same dtype as input\")\n\n    # Ensure out has the correct shape (resize_ follows PyTorch out semantics)\n    expected_shape = list(x.shape)\n    expected_shape.insert(dim_normalized, 1)\n    if list(out.shape) != expected_shape:\n        out.resize_(expected_shape)\n\n    _launch_unsqueeze_copy(x, dim_normalized, out)\n    return out",
        "has_code": true
      },
      "aten::permute_copy": {
        "gpt_speedup_vs_cuda": 0.2809588000574335,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef permute_copy_kernel(\n    x_ptr,                 # *Pointer* to input tensor data\n    y_ptr,                 # *Pointer* to output tensor data\n    numel,                 # total number of elements\n    out_shape_ptr,         # int64[N] sizes of output dimensions\n    in_strides_ptr,        # int64[N] input strides (in elements)\n    out_strides_ptr,       # int64[N] output strides (in elements)\n    perm_ptr,              # int64[N] mapping from output dim -> input dim\n    NDIMS: tl.constexpr,   # number of dimensions\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    off = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = off < numel\n\n    # Prepare offsets\n    tmp = off.to(tl.int64)\n    in_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    out_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Decompose linear index into multi-index over output shape\n    # and accumulate input/output offsets using strides.\n    # Iterate from last dim to first for divmod-based digit extraction.\n    for rev_i in range(NDIMS):\n        i = NDIMS - 1 - rev_i\n        size_i = tl.load(out_shape_ptr + i)  # scalar broadcasted to vector\n        # Avoid div by zero if size_i could be 0 (numel==0 covered by mask; size 0 dims produce numel 0)\n        size_i = tl.where(size_i == 0, 1, size_i)\n        idx_i = tmp % size_i\n        tmp = tmp // size_i\n\n        out_stride_i = tl.load(out_strides_ptr + i)\n        perm_i = tl.load(perm_ptr + i)\n        in_stride_axis = tl.load(in_strides_ptr + perm_i)\n\n        out_off += idx_i * out_stride_i\n        in_off += idx_i * in_stride_axis\n\n    x = tl.load(x_ptr + in_off, mask=mask, other=0)\n    tl.store(y_ptr + out_off, x, mask=mask)\n\n\ndef _normalize_dims(dims, ndim):\n    if isinstance(dims, torch.Tensor):\n        dims = dims.tolist()\n    dims = list(dims)\n    if len(dims) != ndim:\n        raise ValueError(f\"dims length {len(dims)} must equal tensor ndim {ndim}\")\n    norm = []\n    for d in dims:\n        if d < 0:\n            d += ndim\n        if not (0 <= d < ndim):\n            raise ValueError(f\"dimension out of range: {d}\")\n        norm.append(d)\n    if sorted(norm) != list(range(ndim)):\n        raise ValueError(f\"dims must be a permutation of [0..{ndim-1}], got {norm}\")\n    return norm\n\n\ndef _launch_permute_copy(x: torch.Tensor, dims, out: torch.Tensor = None):\n    assert x.is_cuda, \"Input tensor must be on CUDA device for Triton kernels.\"\n    dims = _normalize_dims(dims, x.dim())\n    out_shape = [x.size(d) for d in dims]\n    n_elements = int(torch.tensor(out_shape, dtype=torch.int64).prod().item() if len(out_shape) > 0 else 1)\n\n    if out is None:\n        out = torch.empty(out_shape, device=x.device, dtype=x.dtype)\n    else:\n        if not out.is_cuda:\n            raise ValueError(\"Output tensor must be on CUDA device.\")\n        if tuple(out.shape) != tuple(out_shape):\n            raise ValueError(f\"Output shape {tuple(out.shape)} does not match expected {tuple(out_shape)}.\")\n        if out.dtype != x.dtype:\n            raise ValueError(f\"Output dtype {out.dtype} must match input dtype {x.dtype}.\")\n        if out.device != x.device:\n            raise ValueError(\"Input and output must be on the same device.\")\n\n    # Early exit for zero elements\n    if n_elements == 0:\n        return out\n\n    # Prepare metadata tensors on device (int64)\n    NDIMS = x.dim()\n    # Handle 0-dim tensors\n    if NDIMS == 0:\n        # trivial copy\n        out.copy_(x)\n        return out\n\n    out_shape_t = torch.tensor(out_shape, device=x.device, dtype=torch.int64)\n    in_strides_t = torch.tensor(x.stride(), device=x.device, dtype=torch.int64)\n    out_strides_t = torch.tensor(out.stride(), device=x.device, dtype=torch.int64)\n    perm_t = torch.tensor(dims, device=x.device, dtype=torch.int64)\n\n    # Launch configuration\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    permute_copy_kernel[grid](\n        x, out, n_elements,\n        out_shape_t, in_strides_t, out_strides_t, perm_t,\n        NDIMS=NDIMS,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return out\n\n\ndef permute_copy(self: torch.Tensor, dims):\n    return _launch_permute_copy(self, dims, out=None)\n\n\ndef permute_copy_out(self: torch.Tensor, dims, out: torch.Tensor):\n    return _launch_permute_copy(self, dims, out=out)",
        "has_code": true
      },
      "aten::lift": {
        "gpt_speedup_vs_cuda": 0.1780237051130482,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _copy_kernel(src_ptr, dst_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(src_ptr + offsets, mask=mask)\n    tl.store(dst_ptr + offsets, x, mask=mask)\n\n\ndef lift(x: torch.Tensor):\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"lift expects a single Tensor argument\")\n    if x.device.type != \"cuda\":\n        raise RuntimeError(\"lift: input tensor must be on a CUDA device\")\n    out = torch.empty_like(x)\n    n_elements = out.numel()\n    if n_elements > 0:\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        _copy_kernel[grid](x, out, n_elements, BLOCK_SIZE=1024)\n    return out\n\n\ndef lift_out(x: torch.Tensor, out: torch.Tensor):\n    if not isinstance(x, torch.Tensor) or not isinstance(out, torch.Tensor):\n        raise TypeError(\"lift_out expects (Tensor x, Tensor out)\")\n    if x.device.type != \"cuda\" or out.device.type != \"cuda\":\n        raise RuntimeError(\"lift_out: both input and out tensors must be on a CUDA device\")\n    if out.device != x.device:\n        raise RuntimeError(\"lift_out: out tensor must be on the same device as input\")\n    if out.dtype != x.dtype:\n        raise RuntimeError(\"lift_out: out tensor must have the same dtype as input\")\n    # Resize out to match shape; this ensures a contiguous layout and correct size.\n    if tuple(out.shape) != tuple(x.shape):\n        out.resize_(x.shape)\n    n_elements = x.numel()\n    if n_elements > 0:\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        _copy_kernel[grid](x, out, n_elements, BLOCK_SIZE=1024)\n    return out",
        "has_code": true
      },
      "aten::_unsafe_view": {
        "gpt_speedup_vs_cuda": 0.16815254020964956,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _copy_1d_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    tl.store(y_ptr + offsets, x, mask=mask)\n\n\ndef _infer_view_size(input_numel, size):\n    if isinstance(size, torch.Size):\n        size = list(size)\n    elif isinstance(size, (list, tuple)):\n        size = list(size)\n    else:\n        raise TypeError(\"size must be a list/tuple/torch.Size of ints\")\n    neg_one_count = sum(1 for s in size if s == -1)\n    if neg_one_count > 1:\n        raise ValueError(\"only one dimension can be inferred\")\n    known_prod = 1\n    for s in size:\n        if s != -1:\n            if s < 0:\n                raise ValueError(\"invalid size, negative dimensions other than -1 not allowed\")\n            known_prod *= s if s != 0 else 1\n    if neg_one_count == 0:\n        prod = 1\n        for s in size:\n            prod *= s\n        if prod != input_numel:\n            raise ValueError(f\"requested view size {tuple(size)} does not match input numel {input_numel}\")\n        return tuple(size)\n    else:\n        if known_prod == 0:\n            if input_numel != 0:\n                raise ValueError(f\"cannot infer dimension with zero known product and non-zero numel {input_numel}\")\n            inferred = 0\n        else:\n            if input_numel % known_prod != 0:\n                raise ValueError(\"input numel not divisible by known product for inferred dimension\")\n            inferred = input_numel // known_prod\n        out = []\n        inferred_used = False\n        for s in size:\n            if s == -1 and not inferred_used:\n                out.append(int(inferred))\n                inferred_used = True\n            else:\n                out.append(int(s))\n        return tuple(out)\n\n\ndef _launch_copy_kernel(src_flat: torch.Tensor, dst_flat: torch.Tensor):\n    assert src_flat.is_cuda and dst_flat.is_cuda, \"tensors must be on CUDA device\"\n    assert src_flat.dtype == dst_flat.dtype, \"dtypes must match\"\n    n_elements = src_flat.numel()\n    if n_elements == 0:\n        return\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _copy_1d_kernel[grid](src_flat, dst_flat, n_elements, BLOCK_SIZE=1024)\n\n\ndef _unsafe_view(self: torch.Tensor, size):\n    new_size = _infer_view_size(self.numel(), size)\n    out = torch.empty(new_size, device=self.device, dtype=self.dtype)\n    src_flat = self.contiguous().view(-1)\n    dst_flat = out.view(-1)\n    _launch_copy_kernel(src_flat, dst_flat)\n    return out\n\n\ndef _unsafe_view_out(self: torch.Tensor, size, out: torch.Tensor = None):\n    if out is None:\n        # create out if not provided\n        out = torch.empty(0, device=self.device, dtype=self.dtype)\n    if out.device != self.device:\n        raise ValueError(\"out tensor must be on the same device as input\")\n    if out.dtype != self.dtype:\n        raise ValueError(\"out tensor must have the same dtype as input\")\n    new_size = _infer_view_size(self.numel(), size)\n    out.resize_(new_size)\n    src_flat = self.contiguous().view(-1)\n    dst_flat = out.view(-1)\n    _launch_copy_kernel(src_flat, dst_flat)\n    return out",
        "has_code": true
      },
      "aten::unsqueeze": {
        "gpt_speedup_vs_cuda": 0.014576242685975453,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef unsqueeze_kernel(\n    src_ptr,            # *Pointer* to input tensor data.\n    dst_ptr,            # *Pointer* to output tensor data.\n    out_numel,          # Total number of elements in output (same as input).\n    in_strides_ptr,     # *Pointer* to input strides (length = RANK-1).\n    out_shape_ptr,      # *Pointer* to output shape (length = RANK).\n    BLOCK_SIZE: tl.constexpr,  # Number of elements processed by each program.\n    RANK: tl.constexpr,        # Rank of the output tensor.\n    UNSQ_DIM: tl.constexpr,    # The dimension at which to unsqueeze (compile-time constant).\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    offsets = offsets.to(tl.int64)\n    mask = offsets < out_numel\n\n    tmp = offsets\n    src_offset = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    # Decompose linear index into multi-dimensional coordinates and map to input offset\n    for k in range(RANK - 1, -1, -1):\n        s_k = tl.load(out_shape_ptr + k)\n        c_k = tmp % s_k\n        tmp = tmp // s_k\n        if k != UNSQ_DIM:\n            in_k = k if k < UNSQ_DIM else k - 1\n            stride_in_k = tl.load(in_strides_ptr + in_k)\n            src_offset += c_k * stride_in_k\n\n    vals = tl.load(src_ptr + src_offset, mask=mask)\n    tl.store(dst_ptr + offsets, vals, mask=mask)\n\n\ndef unsqueeze(*args, **kwargs):\n    # Expect signature: unsqueeze(x, dim)\n    if len(args) >= 2:\n        x, dim = args[0], args[1]\n    else:\n        x = kwargs.get('self', kwargs.get('input', None))\n        dim = kwargs.get('dim', None)\n    assert isinstance(x, torch.Tensor), \"unsqueeze expects a torch.Tensor as the first argument.\"\n    assert isinstance(dim, int), \"unsqueeze expects an integer 'dim' argument.\"\n    assert x.is_cuda, \"Input tensor must be on CUDA device.\"\n\n    ndims = x.dim()\n    if dim < 0:\n        dim += ndims + 1\n    assert 0 <= dim <= ndims, f\"dim must be in range [0, {ndims}] after normalization.\"\n\n    out_shape = list(x.shape)\n    out_shape.insert(dim, 1)\n    out = torch.empty(out_shape, dtype=x.dtype, device=x.device)\n\n    numel = out.numel()\n    # Prepare strides and shape tensors on device\n    in_strides = torch.tensor(x.stride(), dtype=torch.int64, device=x.device)\n    out_shape_t = torch.tensor(out_shape, dtype=torch.int64, device=x.device)\n\n    RANK = len(out_shape)\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(numel, meta['BLOCK_SIZE']),)\n\n    unsqueeze_kernel[grid](\n        x, out, numel,\n        in_strides, out_shape_t,\n        BLOCK_SIZE=BLOCK_SIZE, RANK=RANK, UNSQ_DIM=dim\n    )\n    return out",
        "has_code": true
      },
      "aten::permute": {
        "gpt_speedup_vs_cuda": 0.007373376073781197,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef permute_kernel(\n    x_ptr,                # *Pointer* to input tensor\n    y_ptr,                # *Pointer* to output tensor\n    n_elements,           # total number of elements\n    ndim,                 # number of dimensions\n    in_strides_perm_ptr,  # int64[ndim]: input strides permuted by dims\n    out_shape_ptr,        # int64[ndim]: output shape\n    out_postfix_ptr,      # int64[ndim]: product of sizes after each axis in output\n    BLOCK_SIZE: tl.constexpr,\n    MAX_DIMS: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offs = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n\n    offs64 = offs.to(tl.int64)\n    in_index = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    for k in range(MAX_DIMS):\n        cond = k < ndim\n        step_k = tl.load(out_postfix_ptr + k, mask=cond, other=1).to(tl.int64)\n        size_k = tl.load(out_shape_ptr + k, mask=cond, other=1).to(tl.int64)\n        stride_k = tl.load(in_strides_perm_ptr + k, mask=cond, other=0).to(tl.int64)\n        coord_k = (offs64 // step_k) % size_k\n        in_index += coord_k * stride_k\n\n    vals = tl.load(x_ptr + in_index, mask=mask)\n    tl.store(y_ptr + offs64, vals, mask=mask)\n\n\ndef permute(*args, **kwargs):\n    # Parse arguments to support common PyTorch calling patterns\n    if len(args) == 0:\n        raise TypeError(\"permute() missing required argument: 'input'\")\n\n    x = args[0]\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"First argument to permute must be a torch.Tensor\")\n\n    # Determine dims from args/kwargs\n    dims = kwargs.get(\"dims\", None)\n    if dims is None:\n        # If two args and second is sequence, treat as dims\n        if len(args) == 2 and isinstance(args[1], (list, tuple)):\n            dims = args[1]\n        else:\n            # Treat remaining positional args as dims varargs\n            dims = args[1:]\n    dims = tuple(int(d) for d in dims)\n\n    ndim = x.dim()\n    if len(dims) != ndim:\n        raise ValueError(f\"permute(): dims length {len(dims)} does not match tensor ndim {ndim}\")\n\n    # Normalize negative dims and validate permutation\n    dims = tuple([d % ndim for d in dims])\n    if len(set(dims)) != ndim:\n        raise ValueError(\"permute(): dims must be a permutation of [0..ndim-1] with no repeats\")\n\n    if not x.is_cuda:\n        raise AssertionError(\"Input tensor must be on CUDA device\")\n\n    device = x.device\n    dtype = x.dtype\n\n    in_shape = tuple(x.shape)\n    out_shape = tuple(in_shape[d] for d in dims)\n\n    # Prepare strides in elements\n    in_strides = tuple(x.stride())\n    in_strides_perm = tuple(in_strides[d] for d in dims)\n\n    # Compute postfix products for output shape: prod(out_shape[k+1:])\n    out_postfix = []\n    p = 1\n    for size in reversed(out_shape):\n        out_postfix.append(p)\n        p *= int(size)\n    out_postfix = list(reversed(out_postfix))\n\n    # Create output tensor (contiguous layout)\n    out = torch.empty(out_shape, dtype=dtype, device=device)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    # Move metadata to device (int64)\n    in_strides_perm_t = torch.tensor(in_strides_perm, dtype=torch.int64, device=device)\n    out_shape_t = torch.tensor(out_shape, dtype=torch.int64, device=device)\n    out_postfix_t = torch.tensor(out_postfix, dtype=torch.int64, device=device)\n\n    BLOCK_SIZE = 1024\n    MAX_DIMS = max(1, min(16, ndim))  # cap unrolling to 16\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    permute_kernel[grid](\n        x,\n        out,\n        n_elements,\n        ndim,\n        in_strides_perm_t,\n        out_shape_t,\n        out_postfix_t,\n        BLOCK_SIZE=BLOCK_SIZE,\n        MAX_DIMS=16,\n    )\n    return out",
        "has_code": true
      },
      "aten::expand": {
        "gpt_speedup_vs_cuda": 0.006507065642202123,
        "meets_threshold": false,
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef expand(x_ptr,\n           out_ptr,\n           n_elements,\n           ndims,\n           out_shape_ptr,\n           out_cumprod_ptr,\n           in_stride_ptr,\n           BLOCK_SIZE: tl.constexpr,\n           MAX_DIMS: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Compute input offsets corresponding to each output linear index\n    in_offsets = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Accumulate contributions per dimension\n    for d in range(MAX_DIMS):\n        # Load scalars defining the output decomposition and input strides\n        s = tl.load(out_shape_ptr + d)\n        stride_right = tl.load(out_cumprod_ptr + d)\n        in_stride = tl.load(in_stride_ptr + d)\n        # idx along dimension d for each linear offset\n        idx_d = (offsets // stride_right) % s\n        # contribution to input linear offset\n        in_offsets += idx_d * in_stride\n\n    # Load from input using computed offsets and store to output\n    x = tl.load(x_ptr + in_offsets, mask=mask)\n    tl.store(out_ptr + offsets, x, mask=mask)\n\n\n_expand_kernel = expand\n\n\ndef expand(*args, **kwargs):\n    x = args[0]\n    size = args[1]\n    implicit = kwargs.get('implicit', False)  # not used but accepted for signature compatibility\n\n    if not isinstance(size, (list, tuple, torch.Size)):\n        raise TypeError(\"expand size must be a list/tuple/torch.Size of ints\")\n\n    size = list(size)\n    in_shape = list(x.shape)\n    in_strides = list(x.stride())\n\n    out_ndim = len(size)\n    in_ndim = len(in_shape)\n\n    if in_ndim > out_ndim:\n        raise RuntimeError(f\"expand: requested size has fewer dimensions ({out_ndim}) than input ({in_ndim})\")\n\n    # Pad input shape/strides on the left to match output ndim\n    if in_ndim < out_ndim:\n        pad = out_ndim - in_ndim\n        in_shape = [1] * pad + in_shape\n        # For padded (new) leading dims, stride effectively is 0 since they will be broadcast\n        in_strides = [0] * pad + in_strides\n\n    # Resolve -1 and validate broadcastability\n    out_shape = []\n    for d in range(out_ndim):\n        req = size[d]\n        src = in_shape[d]\n        if req == -1:\n            target = src\n        else:\n            target = req\n        if src != target and src != 1:\n            raise RuntimeError(\n                f\"The expanded size of the tensor ({target}) must match the existing size ({src}) at non-singleton \"\n                f\"dimension {d}. Target sizes must be the same, or -1, or the size of dimension in the original tensor must be 1.\"\n            )\n        out_shape.append(int(target))\n\n    # Effective input strides: 0 for broadcasted dims, original stride otherwise\n    in_stride_eff = [int(in_strides[d]) if in_shape[d] != 1 else 0 for d in range(out_ndim)]\n\n    # Prepare decomposition multipliers: product of sizes to the right for each dim\n    out_cumprod_right = [0] * out_ndim\n    prod = 1\n    for d in range(out_ndim - 1, -1, -1):\n        out_cumprod_right[d] = prod\n        prod *= out_shape[d]\n\n    # Allocate output\n    out = torch.empty(out_shape, dtype=x.dtype, device=x.device)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    # Triton kernel parameters\n    BLOCK_SIZE = 1024\n    MAX_DIMS = max(out_ndim, 1)  # at least 1\n    # Round up MAX_DIMS to a reasonable static upper bound for compilation (e.g., 16)\n    # but ensure arrays we pass match MAX_DIMS in kernel\n    STATIC_MAX = 16\n    if MAX_DIMS > STATIC_MAX:\n        STATIC_MAX = MAX_DIMS\n\n    # Create device arrays for shapes/strides with padding for MAX_DIMS\n    pad_len = STATIC_MAX - out_ndim\n    out_shape_arr = torch.tensor(out_shape + [1] * pad_len, dtype=torch.int64, device=x.device)\n    out_cumprod_arr = torch.tensor(out_cumprod_right + [1] * pad_len, dtype=torch.int64, device=x.device)\n    in_stride_arr = torch.tensor(in_stride_eff + [0] * pad_len, dtype=torch.int64, device=x.device)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _expand_kernel[grid](\n        x, out, n_elements, out_ndim,\n        out_shape_arr, out_cumprod_arr, in_stride_arr,\n        BLOCK_SIZE=BLOCK_SIZE,\n        MAX_DIMS=STATIC_MAX,\n    )\n    return out",
        "has_code": true
      }
    }
  },
  "existing_operators": {
    "total": 91,
    "selected": 4,
    "operators": {
      "sort": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0501728659727483,
            "flaggems_speedup_vs_cuda": 0.0628584,
            "speedup_vs_flaggems": 16.70696145579188
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef sort(x_ptr,            # *Pointer* to input values\n         out_ptr,          # *Pointer* to output sorted values\n         out_idx_ptr,      # *Pointer* to output indices\n         M,                # number of rows\n         N,                # number of elements per row\n         stride_in_row,    # input row stride (in elements)\n         stride_in_col,    # input col stride (in elements)\n         stride_out_row,   # output row stride (in elements)\n         stride_out_col,   # output col stride (in elements)\n         DESC: tl.constexpr,         # sort direction: False=ascending, True=descending\n         BLOCK_SIZE: tl.constexpr):  # power-of-two >= N\n    pid = tl.program_id(axis=0)\n    # Offsets within a row\n    i = tl.arange(0, BLOCK_SIZE)\n\n    # Base pointers for this row\n    in_row_base = x_ptr + pid * stride_in_row\n    out_row_base = out_ptr + pid * stride_out_row\n    out_idx_row_base = out_idx_ptr + pid * stride_out_row  # same layout as out values\n\n    # Validity mask for true elements (not padded)\n    mask_i = i < N\n\n    # Choose padding sentinel based on direction\n    other_val = -float('inf') if DESC else float('inf')\n\n    # Initialize output buffers with input values and initial indices\n    in_ptrs = in_row_base + i * stride_in_col\n    out_ptrs = out_row_base + i * stride_out_col\n    out_idx_ptrs = out_idx_row_base + i * stride_out_col\n\n    vals = tl.load(in_ptrs, mask=mask_i, other=other_val)\n    tl.store(out_ptrs, vals, mask=mask_i)\n    init_idx = i.to(tl.int64)\n    tl.store(out_idx_ptrs, init_idx, mask=mask_i)\n\n    # Bitonic sort network over BLOCK_SIZE\n    k = 2\n    while k <= BLOCK_SIZE:\n        j = k // 2\n        while j >= 1:\n            ixj = i ^ j\n            # Only one lane per pair performs the swap (i < ixj)\n            do_pair = (i < ixj)\n\n            # Determine direction for this pair (ascending segment if True)\n            up = ((i & k) == 0)\n            if DESC:\n                up = ~up  # invert for descending overall\n\n            # Pointers for the pair\n            ptr_i_val = out_row_base + i * stride_out_col\n            ptr_j_val = out_row_base + ixj * stride_out_col\n            ptr_i_idx = out_idx_row_base + i * stride_out_col\n            ptr_j_idx = out_idx_row_base + ixj * stride_out_col\n\n            mask_j = ixj < N\n\n            vi = tl.load(ptr_i_val, mask=mask_i, other=other_val)\n            vj = tl.load(ptr_j_val, mask=mask_j, other=other_val)\n            ii = tl.load(ptr_i_idx, mask=mask_i, other=0).to(tl.int64)\n            ij = tl.load(ptr_j_idx, mask=mask_j, other=0).to(tl.int64)\n\n            # Compute min/max pairs and corresponding indices\n            take_i = vi <= vj\n            lo_v = tl.where(take_i, vi, vj)\n            hi_v = tl.where(take_i, vj, vi)\n            lo_i = tl.where(take_i, ii, ij)\n            hi_i = tl.where(take_i, ij, ii)\n\n            # Select which goes to i and which to ixj based on segment direction\n            new_vi = tl.where(up, lo_v, hi_v)\n            new_vj = tl.where(up, hi_v, lo_v)\n            new_ii = tl.where(up, lo_i, hi_i)\n            new_ij = tl.where(up, hi_i, lo_i)\n\n            # Only the lane with i < ixj writes both positions; respect valid masks\n            store_mask_i = do_pair & mask_i\n            store_mask_j = do_pair & mask_j\n            tl.store(ptr_i_val, new_vi, mask=store_mask_i)\n            tl.store(ptr_j_val, new_vj, mask=store_mask_j)\n            tl.store(ptr_i_idx, new_ii, mask=store_mask_i)\n            tl.store(ptr_j_idx, new_ij, mask=store_mask_j)\n\n            j //= 2\n        k *= 2\n\n\n# Keep a handle to the Triton kernel before defining the Python wrapper with the same name.\nsort_kernel = sort\n\n\ndef sort(*args, **kwargs):\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n        rest_args = args[1:]\n    else:\n        # torch.sort expects first positional arg to be input tensor\n        x = kwargs.get('input', None)\n        rest_args = ()\n\n    if x is None:\n        raise TypeError(\"sort() missing required argument 'input' (pos 1)\")\n\n    # Parse dim, descending, stable, out\n    # torch.sort signature: torch.sort(input, dim=-1, descending=False, stable=False, *, out=None)\n    dim = kwargs.get('dim', -1)\n    descending = kwargs.get('descending', False)\n    stable = kwargs.get('stable', False)\n    out = kwargs.get('out', None)\n\n    # Override from positional arguments if provided\n    if len(rest_args) >= 1:\n        dim = rest_args[0]\n    if len(rest_args) >= 2:\n        descending = rest_args[1]\n    if len(rest_args) >= 3:\n        stable = rest_args[2]\n\n    if out is not None:\n        raise NotImplementedError(\"out=... is not supported in this Triton implementation\")\n\n    if not x.is_cuda:\n        # Fallback to torch.sort on CPU tensors\n        return torch.sort(x, dim=dim, descending=descending, stable=stable)\n\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        # Fallback for unsupported dtypes\n        return torch.sort(x, dim=dim, descending=descending, stable=stable)\n\n    # Normalize dim\n    ndim = x.ndim\n    if dim < 0:\n        dim += ndim\n    if not (0 <= dim < ndim):\n        raise ValueError(f\"dim out of range (expected to be in range of [{-ndim}, {ndim-1}], but got {dim - ndim if dim >= ndim else dim})\")\n\n    # Handle empty tensor quickly\n    if x.numel() == 0:\n        return torch.sort(x, dim=dim, descending=descending, stable=stable)\n\n    # Move sort dim to last and make contiguous\n    x_perm = x.movedim(dim, -1).contiguous()\n    *prefix, N = x_perm.shape\n    M = 1\n    for s in prefix:\n        M *= s\n    x2d = x_perm.view(M, N)\n\n    # Triton bitonic sort requires power-of-two block size; choose next pow2\n    def next_pow2(v: int) -> int:\n        if v <= 1:\n            return 1\n        return 1 << (v - 1).bit_length()\n\n    BLOCK_SIZE = next_pow2(N)\n    MAX_BLOCK_SIZE = 2048  # limit for practicality\n    if BLOCK_SIZE > MAX_BLOCK_SIZE:\n        # Fallback to torch.sort for very large N\n        return torch.sort(x, dim=dim, descending=descending, stable=stable)\n\n    # Allocate outputs\n    out_vals2d = torch.empty_like(x2d)\n    out_idx2d = torch.empty((M, N), device=x.device, dtype=torch.long)\n\n    # Strides in elements\n    stride_in_row, stride_in_col = x2d.stride()\n    stride_out_row, stride_out_col = out_vals2d.stride()\n\n    grid = (M,)\n\n    # Dispatch Triton kernel\n    sort_kernel[grid](\n        x2d,                         # x_ptr\n        out_vals2d,                  # out_ptr\n        out_idx2d,                   # out_idx_ptr\n        M, N,\n        stride_in_row, stride_in_col,\n        stride_out_row, stride_out_col,\n        DESC=bool(descending),\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Reshape back and move dim to original position\n    out_vals = out_vals2d.view(*prefix, N).movedim(-1, dim)\n    out_idx = out_idx2d.view(*prefix, N).movedim(-1, dim)\n\n    # Match PyTorch's return signature: (values, indices)\n    return out_vals, out_idx",
        "has_code": true,
        "best_speedup_vs_flaggems": 16.70696145579188,
        "meets_threshold": true
      },
      "randperm": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 2.2624663914792262,
            "flaggems_speedup_vs_cuda": 0.576537,
            "speedup_vs_flaggems": 3.924234509631171
          }
        },
        "code": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef randperm(output_ptr,  # *Pointer* to output vector (int64).\n             n_elements,  # Number of elements.\n             a,           # Multiplier for bijective map.\n             b,           # Addend for bijective map.\n             BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Compute permutation value: (a * i + b) % n\n    offs64 = offsets.to(tl.int64)\n    a64 = tl.full([1], a, tl.int64)\n    b64 = tl.full([1], b, tl.int64)\n    n64 = tl.full([1], n_elements, tl.int64)\n\n    vals = (offs64 * a64 + b64) % n64\n    tl.store(output_ptr + offsets, vals, mask=mask)\n\n\n# Keep a handle to the Triton kernel before defining the Python wrapper with the same name.\nrandperm_kernel = randperm\n\n\ndef randperm(*args, **kwargs):\n    # Parse arguments similar to torch.randperm\n    if len(args) >= 1:\n        n = args[0]\n    else:\n        n = kwargs.get('n', None)\n    if n is None or not isinstance(n, int):\n        raise TypeError(\"randperm expects an integer 'n' as the first positional argument or as keyword 'n'.\")\n\n    dtype = kwargs.get('dtype', torch.int64)\n    device = kwargs.get('device', None)\n    generator = kwargs.get('generator', None)\n    # layout, pin_memory, requires_grad are accepted but not used here\n    # to match PyTorch signature flexibility.\n    # layout = kwargs.get('layout', torch.strided)\n    # pin_memory = kwargs.get('pin_memory', False)\n    # requires_grad = kwargs.get('requires_grad', False)\n\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device = torch.device(device)\n\n    # If not on CUDA, fall back to PyTorch implementation\n    if device.type != 'cuda':\n        return torch.randperm(n, dtype=dtype, device=device, generator=generator)\n\n    # We implement the permutation as y[i] = (a*i + b) % n where gcd(a, n) == 1\n    # This is a bijection over [0, n), producing a valid permutation.\n    if n == 0:\n        return torch.empty((0,), dtype=dtype, device=device)\n\n    # Helper to draw random integers, optionally with a generator\n    def _randint(low, high):\n        if high <= low:\n            return low\n        return int(torch.randint(low, high, (1,), device=device, dtype=torch.int64, generator=generator).item())\n\n    if n <= 1:\n        a = 0\n        b = 0\n    else:\n        # Choose 'a' such that gcd(a, n) == 1\n        # Try a few times; for typical n this is very fast.\n        # Ensure a in [1, n-1]\n        while True:\n            a = _randint(1, n)\n            if math.gcd(a, n) == 1:\n                break\n        # Choose 'b' in [0, n-1]\n        b = _randint(0, n)\n\n    # Create output tensor (compute in int64, cast at end if needed)\n    out64 = torch.empty((n,), dtype=torch.int64, device=device)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n    randperm_kernel[grid](out64, n, int(a), int(b), BLOCK_SIZE=BLOCK_SIZE)\n\n    if dtype != torch.int64:\n        return out64.to(dtype)\n    return out64\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 3.924234509631171,
        "meets_threshold": true
      },
      "argmax": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9202054839678944,
            "flaggems_speedup_vs_cuda": 0.720586666666667,
            "speedup_vs_flaggems": 1.2770226352156029
          }
        },
        "code": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef argmax(x_ptr,  # float32* input, laid out as [n_rows, n_cols] contiguous along last dim\n           out_idx_ptr,  # int64* output indices of size n_rows\n           n_rows,\n           n_cols,\n           BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    if pid >= n_rows:\n        return\n    base = pid * n_cols\n\n    col_range = tl.arange(0, BLOCK_SIZE)\n    neg_inf = -float(\"inf\")\n    big_i64 = 0x7fffffffffffffff\n\n    best_val = tl.full((), neg_inf, dtype=tl.float32)\n    best_idx = tl.full((), big_i64, dtype=tl.int64)\n\n    col_start = 0\n    while col_start < n_cols:\n        offs = base + col_start + col_range\n        mask = (col_start + col_range) < n_cols\n        vals = tl.load(x_ptr + offs, mask=mask, other=neg_inf)\n        idxs = (col_start + col_range).to(tl.int64)\n\n        tile_max = tl.max(vals, axis=0)\n        eq = vals == tile_max\n        big_vec = tl.full((BLOCK_SIZE,), big_i64, dtype=tl.int64)\n        idxs_masked = tl.where(eq, idxs, big_vec)\n        tile_arg_local = tl.min(idxs_masked, axis=0)\n\n        better = tile_max > best_val\n        equal = tile_max == best_val\n        best_idx = tl.where(better, tile_arg_local, tl.where(equal, tl.minimum(best_idx, tile_arg_local), best_idx))\n        best_val = tl.where(better, tile_max, best_val)\n\n        col_start += BLOCK_SIZE\n\n    tl.store(out_idx_ptr + pid, best_idx)\n\n\n# Keep an alias to the kernel before defining the Python wrapper with the same name\nargmax_lastdim_kernel = argmax\n\n\n@triton.jit\ndef argmax_flat_stage1(x_ptr,  # float32* input flattened\n                       tmp_vals_ptr,  # float32* temporary max values per block\n                       tmp_idxs_ptr,  # int64* temporary argmax indices per block\n                       n_elements,\n                       BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    start = pid * BLOCK_SIZE\n    offs = start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n    neg_inf = -float(\"inf\")\n    vals = tl.load(x_ptr + offs, mask=mask, other=neg_inf)\n    idxs = offs.to(tl.int64)\n\n    tile_max = tl.max(vals, axis=0)\n    eq = vals == tile_max\n    big_i64 = 0x7fffffffffffffff\n    idxs_masked = tl.where(eq, idxs, tl.full((BLOCK_SIZE,), big_i64, dtype=tl.int64))\n    tile_arg = tl.min(idxs_masked, axis=0)\n\n    tl.store(tmp_vals_ptr + pid, tile_max)\n    tl.store(tmp_idxs_ptr + pid, tile_arg)\n\n\n@triton.jit\ndef argmax_flat_stage2(tmp_vals_ptr,  # float32* size n_blocks\n                       tmp_idxs_ptr,  # int64* size n_blocks\n                       out_idx_ptr,   # int64* scalar result\n                       n_blocks,\n                       BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    # Single program reduction\n    if pid != 0:\n        return\n\n    neg_inf = -float(\"inf\")\n    big_i64 = 0x7fffffffffffffff\n\n    best_val = tl.full((), neg_inf, dtype=tl.float32)\n    best_idx = tl.full((), big_i64, dtype=tl.int64)\n\n    idx_range = tl.arange(0, BLOCK_SIZE)\n    start = 0\n    while start < n_blocks:\n        offs = start + idx_range\n        mask = offs < n_blocks\n        vals = tl.load(tmp_vals_ptr + offs, mask=mask, other=neg_inf)\n        idxs = tl.load(tmp_idxs_ptr + offs, mask=mask, other=big_i64)\n\n        tile_max = tl.max(vals, axis=0)\n        eq = vals == tile_max\n        big_vec = tl.full((BLOCK_SIZE,), big_i64, dtype=tl.int64)\n        idxs_masked = tl.where(eq, idxs, big_vec)\n        tile_arg_local = tl.min(idxs_masked, axis=0)\n\n        better = tile_max > best_val\n        equal = tile_max == best_val\n        best_idx = tl.where(better, tile_arg_local, tl.where(equal, tl.minimum(best_idx, tile_arg_local), best_idx))\n        best_val = tl.where(better, tile_max, best_val)\n\n        start += BLOCK_SIZE\n\n    tl.store(out_idx_ptr, best_idx)\n\n\ndef argmax(*args, **kwargs):\n    # Parse arguments similar to torch.argmax\n    if len(args) == 0 and 'input' not in kwargs:\n        raise TypeError(\"argmax() missing required positional argument: 'input'\")\n    x = args[0] if len(args) > 0 else kwargs['input']\n\n    dim = None\n    keepdim = False\n    if len(args) >= 2:\n        dim = args[1]\n    elif 'dim' in kwargs:\n        dim = kwargs['dim']\n\n    if len(args) >= 3:\n        keepdim = args[2]\n    elif 'keepdim' in kwargs:\n        keepdim = kwargs['keepdim']\n\n    if not x.is_cuda:\n        raise ValueError(\"Input tensor must be on CUDA device\")\n\n    # Prepare working tensor in float32 for comparison stability\n    if x.dtype in (torch.float32, torch.float16, torch.bfloat16):\n        x_work = x\n        work_was_cast = False\n    else:\n        x_work = x.to(torch.float32)\n        work_was_cast = True  # only for clarity; not used\n\n    # Handle flatten (dim is None)\n    if dim is None:\n        if x_work.numel() == 0:\n            raise ValueError(\"argmax of an empty tensor\")\n        x_flat = x_work.reshape(-1).contiguous()\n        n_elements = x_flat.numel()\n\n        BLOCK_SIZE = 1024\n        n_blocks = triton.cdiv(n_elements, BLOCK_SIZE)\n\n        tmp_vals = torch.empty((n_blocks,), dtype=torch.float32, device=x_flat.device)\n        tmp_idxs = torch.empty((n_blocks,), dtype=torch.int64, device=x_flat.device)\n\n        grid1 = (n_blocks,)\n        argmax_flat_stage1[grid1](\n            x_flat,\n            tmp_vals,\n            tmp_idxs,\n            n_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n\n        out_idx = torch.empty((), dtype=torch.int64, device=x_flat.device)\n        grid2 = (1,)\n        argmax_flat_stage2[grid2](\n            tmp_vals,\n            tmp_idxs,\n            out_idx,\n            n_blocks,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return out_idx\n\n    # Argmax along a specified dimension\n    if not isinstance(dim, int):\n        raise TypeError(\"dim must be an int or None\")\n    nd = x_work.ndim\n    if nd == 0:\n        raise ValueError(\"argmax(): expected a tensor with at least one dimension when dim is specified\")\n    dim = dim % nd\n    if x_work.shape[dim] == 0:\n        raise ValueError(\"argmax(): reduction dimension has size 0\")\n\n    # Move reduction dim to the last and make contiguous\n    perm = [i for i in range(nd) if i != dim] + [dim]\n    x_perm = x_work.permute(perm).contiguous()\n    rows = x_perm.numel() // x_perm.shape[-1]\n    cols = x_perm.shape[-1]\n\n    # Launch kernel: one program per row\n    out_idx_flat = torch.empty((rows,), dtype=torch.int64, device=x_perm.device)\n    BLOCK_SIZE = 1024\n    grid = (rows,)\n    argmax_lastdim_kernel[grid](\n        x_perm,\n        out_idx_flat,\n        rows,\n        cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # Reshape to output shape in original order\n    out_shape = [x_work.shape[i] for i in range(nd) if i != dim]\n    out = out_idx_flat.reshape(out_shape)\n    if keepdim:\n        out = out.unsqueeze(dim)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.2770226352156029,
        "meets_threshold": true
      },
      "rand_like": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.5856657196087571,
            "flaggems_speedup_vs_cuda": 1.30903466666667,
            "speedup_vs_flaggems": 1.2113244667893337
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef rand_like(output_ptr, n_elements, seed, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets.to(tl.uint32)\n    s = tl.full((BLOCK_SIZE,), seed, tl.uint32)\n\n    # Linear Congruential Generator (LCG): X_{n+1} = (a * X_n + c) mod 2^32\n    # Using idx XOR seed as the base state.\n    a = tl.full((BLOCK_SIZE,), 1664525, tl.uint32)\n    c = tl.full((BLOCK_SIZE,), 1013904223, tl.uint32)\n    rnd = ((idx ^ s) * a + c) & tl.full((BLOCK_SIZE,), 0xFFFFFFFF, tl.uint32)\n\n    # Convert to float in [0, 1)\n    val = rnd.to(tl.float32) * (1.0 / 4294967296.0)\n\n    tl.store(output_ptr + offsets, val, mask=mask)\n\n\n# Preserve a reference to the kernel before defining the Python wrapper with the same name.\n_rand_like_kernel = rand_like\n\n\ndef rand_like(*args, **kwargs):\n    assert len(args) >= 1 and isinstance(args[0], torch.Tensor), \"First positional argument must be a torch.Tensor\"\n    inp = args[0]\n\n    dtype = kwargs.get(\"dtype\", None)\n    device = kwargs.get(\"device\", None)\n    requires_grad = kwargs.get(\"requires_grad\", False)\n    # Force contiguous to ensure correct addressing in kernel\n    memory_format = torch.contiguous_format\n\n    generator = kwargs.get(\"generator\", None)\n\n    if dtype is None:\n        if inp.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n            dtype = inp.dtype\n        else:\n            dtype = torch.get_default_dtype()\n\n    # Restrict to dtypes supported efficiently by Triton kernels\n    if dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        raise NotImplementedError(\"rand_like Triton implementation currently supports dtypes: float16, bfloat16, float32.\")\n\n    if device is None:\n        device = inp.device\n\n    if device.type != \"cuda\":\n        raise RuntimeError(\"rand_like Triton implementation requires a CUDA device\")\n\n    # Determine seed\n    if generator is not None:\n        try:\n            seed = int(generator.initial_seed() & 0xFFFFFFFF)\n        except Exception:\n            seed = int(torch.randint(0, 2**31 - 1, (1,), device='cpu').item())\n    else:\n        seed = int(torch.randint(0, 2**31 - 1, (1,), device='cpu').item())\n\n    output = torch.empty_like(\n        inp,\n        dtype=dtype,\n        device=device,\n        memory_format=memory_format,\n        requires_grad=requires_grad,\n    )\n\n    n_elements = output.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    _rand_like_kernel[grid](output, n_elements, seed, BLOCK_SIZE=BLOCK_SIZE)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.2113244667893337,
        "meets_threshold": true
      },
      "amax": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9904176099308408,
            "flaggems_speedup_vs_cuda": 0.829984666666667,
            "speedup_vs_flaggems": 1.1932962736630963
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef amax(\n    x_ptr,            # *Pointer* to input matrix flattened to [M, K]\n    out_ptr,          # *Pointer* to output vector of length M\n    M,                # Number of rows (product of non-reduced dims)\n    K,                # Number of columns (product of reduced dims)\n    stride_xm,        # Stride of x along M dimension\n    stride_xk,        # Stride of x along K dimension\n    stride_out,       # Stride of out along its single dimension (should be 1)\n    other_val,        # Fallback value for masked loads (e.g., -inf or dtype min)\n    BLOCK_SIZE_K: tl.constexpr,  # Block size along reduction dimension\n):\n    pid = tl.program_id(axis=0)  # Program id corresponds to row index m in [0, M)\n    # Accumulator for the maximum value of this row\n    max_val = other_val\n    # Iterate over the K dimension in BLOCK_SIZE_K chunks\n    for k_start in range(0, K, BLOCK_SIZE_K):\n        offs_k = k_start + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = offs_k < K\n        x = tl.load(x_ptr + pid * stride_xm + offs_k * stride_xk, mask=mask_k, other=other_val)\n        block_max = tl.max(x, axis=0)\n        max_val = tl.maximum(max_val, block_max)\n    tl.store(out_ptr + pid * stride_out, max_val)\n\n\n# Keep a reference to the Triton kernel before defining the Python wrapper with the same name\namax_kernel = amax\n\n\ndef amax(*args, **kwargs):\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    if x is None:\n        raise TypeError(\"amax() missing required argument 'input'\")\n\n    # Parse dim and keepdim\n    dim = kwargs.get('dim', None)\n    if dim is None and len(args) >= 2:\n        dim = args[1]\n    keepdim = kwargs.get('keepdim', False)\n    if len(args) >= 3:\n        keepdim = args[2]\n    out = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"input must be a torch.Tensor\")\n    if x.numel() == 0 and (dim is None or (isinstance(dim, (tuple, list)) and len(dim) > 0) or isinstance(dim, int)):\n        # PyTorch raises error when reducing over empty dimension\n        reduce_dims = tuple(range(x.ndim)) if dim is None else (tuple(dim) if isinstance(dim, (tuple, list)) else (dim,))\n        # Normalize and check sizes for reduced dims\n        nd = x.ndim\n        rd = []\n        for d in reduce_dims:\n            d = int(d)\n            if d < 0:\n                d += nd\n            if not (0 <= d < nd):\n                raise IndexError(f\"Dimension out of range (expected to be in range of [{-nd}, {nd-1}], but got {d - nd if d >= nd else d})\")\n            rd.append(d)\n        # If any reduced dimension size is 0, PyTorch raises RuntimeError\n        if any(x.size(d) == 0 for d in rd):\n            raise RuntimeError(\"cannot perform reduction function amax on an empty dimension\")\n\n    if x.is_complex():\n        raise NotImplementedError(\"amax Triton kernel currently does not support complex dtypes\")\n\n    # Normalize dim argument\n    if dim is None:\n        reduce_dims = tuple(range(x.ndim))\n    elif isinstance(dim, (tuple, list)):\n        reduce_dims = tuple(int(d) for d in dim)\n    else:\n        reduce_dims = (int(dim),)\n\n    # Handle empty reduction dims (dim=()): return input (or copy into out) to match PyTorch behavior\n    if len(reduce_dims) == 0:\n        result = x.clone()\n        if keepdim:\n            # When keepdim=True and dim=(), output has same shape as input\n            pass\n        if out is not None:\n            if out.dtype != x.dtype:\n                raise RuntimeError(\"out tensor must have the same dtype as input\")\n            if out.device != x.device:\n                raise RuntimeError(\"out tensor must be on the same device as input\")\n            if list(out.shape) != list(result.shape):\n                raise RuntimeError(\"out tensor has incorrect shape\")\n            out.copy_(result)\n            return out\n        return result\n\n    # Normalize and deduplicate dims, validate range\n    nd = x.ndim\n    rd_set = set()\n    rd = []\n    for d in reduce_dims:\n        if d < 0:\n            d += nd\n        if not (0 <= d < nd):\n            raise IndexError(f\"Dimension out of range (expected to be in range of [{-nd}, {nd-1}], but got {d - nd if d >= nd else d})\")\n        if d not in rd_set:\n            rd_set.add(d)\n            rd.append(d)\n    rd.sort()\n    # Compute non-reduced dims\n    non_rd = [d for d in range(nd) if d not in rd]\n\n    # Check for empty reduction (size 0 along any reduced dim)\n    if any(x.size(d) == 0 for d in rd):\n        raise RuntimeError(\"cannot perform reduction function amax on an empty dimension\")\n\n    # If M=0 (any non-reduced dim size 0), result is empty; no need to launch kernel\n    M_sizes = [x.size(d) for d in non_rd]\n    K_sizes = [x.size(d) for d in rd]\n    M = 1\n    for s in M_sizes:\n        M *= s\n    K = 1\n    for s in K_sizes:\n        K *= s\n\n    # Prepare output shape\n    if keepdim:\n        out_shape = [1 if i in rd else x.size(i) for i in range(nd)]\n    else:\n        out_shape = [x.size(i) for i in range(nd) if i not in rd]\n\n    # Handle out tensor if provided\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.dtype != x.dtype:\n            raise RuntimeError(\"out tensor must have the same dtype as input\")\n        if out.device != x.device:\n            raise RuntimeError(\"out tensor must be on the same device as input\")\n        if list(out.shape) != out_shape:\n            raise RuntimeError(\"out tensor has incorrect shape\")\n\n    # Fast path: if K == 1, amax over singleton dims is identity; just reshape\n    if K == 1:\n        # Equivalent to viewing/unsqueezing across reduced dims\n        result = x\n        if not keepdim:\n            result = result.permute(non_rd + rd)  # move reduced dims to the end\n            if len(non_rd) == 0:\n                result = result.reshape([])\n            else:\n                result = result.reshape([x.size(d) for d in non_rd])\n        else:\n            # keepdim: set reduced dims to 1\n            result = result.reshape(out_shape)\n        if out is not None:\n            out.copy_(result)\n            return out\n        return result\n\n    # Permute input so that non-reduced dims come first followed by reduced dims\n    permute_order = non_rd + rd\n    x_perm = x.permute(permute_order).contiguous()\n    # Flatten into [M, K]\n    x_2d = x_perm.view(M, K)\n\n    # Allocate temporary output of shape [M]\n    out_1d = torch.empty((M,), device=x.device, dtype=x.dtype)\n\n    # Compute strides for kernel\n    sxm, sxk = x_2d.stride()\n    sout = out_1d.stride()[0]\n\n    # Determine other_val for masked loads\n    if x.dtype.is_floating_point or x.dtype in (torch.float16, torch.bfloat16):\n        other_val = float('-inf')\n    elif x.dtype in (torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64):\n        other_val = int(torch.iinfo(x.dtype).min)\n    else:\n        # Fallback for unsupported dtypes\n        raise NotImplementedError(f\"amax Triton kernel does not support dtype {x.dtype}\")\n\n    # Handle M == 0: create empty result and return without launching kernel\n    if M == 0:\n        empty_result = torch.empty(out_shape, device=x.device, dtype=x.dtype)\n        if out is not None:\n            out.copy_(empty_result)\n            return out\n        return empty_result\n\n    # Launch Triton kernel\n    grid = (M,)\n    amax_kernel[grid](\n        x_2d, out_1d, M, K, sxm, sxk, sout, other_val,\n        BLOCK_SIZE_K=1024,\n    )\n\n    # Reshape output back to expected shape\n    if len(non_rd) == 0:\n        # Reduction over all dims => scalar (0-dim tensor) or keepdim with ones\n        if keepdim:\n            result = out_1d.reshape(out_shape)\n        else:\n            result = out_1d.reshape([])\n    else:\n        # Shape is product of non-reduced dims in their original order\n        result = out_1d.view([x.size(d) for d in non_rd])\n        if keepdim:\n            # Insert size-1 dims at reduced positions\n            for d in rd:\n                result = result.unsqueeze(dim=non_rd.index(d) if False else 0)  # placeholder\n            # The above unsqueeze is incorrect; do explicit reshape to target out_shape\n            result = result.reshape(out_shape)\n\n    if out is not None:\n        out.copy_(result)\n        return out\n    return result\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.1932962736630963,
        "meets_threshold": false
      },
      "aten::clamp_min_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0676841205756986,
            "flaggems_speedup_vs_cuda": 0.997013666666667,
            "speedup_vs_flaggems": 1.0708821315813106
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\nfrom numbers import Number\n\n\n@triton.jit\ndef clamp_min_inplace_kernel(\n    x_ptr,            # *Pointer* to input tensor (in-place mutated).\n    other_ptr,        # *Pointer* to other tensor if using Tensor min, unused otherwise.\n    n_elements,       # Number of elements in the tensor.\n    min_val,          # Scalar minimum value if not using Tensor min.\n    min_is_tensor: tl.constexpr,  # Whether to use tensor-based min.\n    BLOCK_SIZE: tl.constexpr      # Elements per program.\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    if min_is_tensor:\n        y = tl.load(other_ptr + offsets, mask=mask)\n    else:\n        y = min_val\n\n    out = tl.where(x < y, y, x)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _launch_clamp_min_inplace(x: torch.Tensor, other: torch.Tensor | None, min_val: Number | None):\n    assert x.is_cuda, \"Input tensor must be on CUDA device.\"\n    assert x.is_contiguous(), \"Input tensor must be contiguous.\"\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    if other is not None:\n        assert other.is_cuda and other.is_contiguous(), \"Other tensor must be CUDA and contiguous.\"\n        if other.numel() == 1:\n            # Treat single-element tensor as scalar min\n            min_scalar = other.item()\n            clamp_min_inplace_kernel[grid](x, x, n_elements, min_scalar, min_is_tensor=False, BLOCK_SIZE=BLOCK_SIZE)\n        else:\n            assert other.numel() == n_elements, \"Other tensor must have the same number of elements as input.\"\n            assert other.dtype == x.dtype, \"Dtype of other tensor must match input dtype.\"\n            clamp_min_inplace_kernel[grid](x, other, n_elements, 0.0, min_is_tensor=True, BLOCK_SIZE=BLOCK_SIZE)\n    else:\n        assert isinstance(min_val, Number), \"min must be a Python number when other tensor is not provided.\"\n        clamp_min_inplace_kernel[grid](x, x, n_elements, min_val, min_is_tensor=False, BLOCK_SIZE=BLOCK_SIZE)\n    return x\n\n\ndef clamp_min_(*args, **kwargs):\n    # Expected: (Tensor self, Scalar min)\n    # If a Tensor is provided for min, delegate to the Tensor variant.\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\"))\n    if x is None:\n        raise ValueError(\"clamp_min_: expected a tensor as the first argument\")\n\n    # Determine min argument\n    if len(args) >= 2:\n        min_arg = args[1]\n    else:\n        min_arg = kwargs.get(\"min\", kwargs.get(\"other\"))\n\n    if isinstance(min_arg, torch.Tensor):\n        return clamp_min__Tensor(x, min_arg)\n    else:\n        return _launch_clamp_min_inplace(x, None, min_arg)\n\n\ndef clamp_min__Tensor(*args, **kwargs):\n    # Expected: (Tensor self, Tensor min)\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\"))\n    if x is None:\n        raise ValueError(\"clamp_min__Tensor: expected a tensor as the first argument\")\n\n    if len(args) >= 2:\n        other = args[1]\n    else:\n        other = kwargs.get(\"min\", kwargs.get(\"other\"))\n    if other is None or not isinstance(other, torch.Tensor):\n        raise ValueError(\"clamp_min__Tensor: expected a Tensor for 'min' argument\")\n\n    return _launch_clamp_min_inplace(x, other, None)",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0708821315813106,
        "meets_threshold": false
      },
      "aten::celu": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0824575943800372,
            "flaggems_speedup_vs_cuda": 1.03031533333333,
            "speedup_vs_flaggems": 1.050608060813784
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef celu_kernel(x_ptr, out_ptr, n_elements, alpha, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    x_fp = x.to(tl.float32)\n    y_fp = tl.where(x_fp > 0.0, x_fp, alpha * (tl.exp(x_fp / alpha) - 1.0))\n    y = y_fp.to(x.dtype)\n\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _parse_alpha(alpha):\n    if isinstance(alpha, torch.Tensor):\n        if alpha.numel() != 1:\n            raise ValueError(\"alpha tensor must be a scalar (numel() == 1)\")\n        alpha = float(alpha.item())\n    else:\n        alpha = float(alpha)\n    if alpha == 0.0:\n        raise ValueError(\"alpha must be non-zero\")\n    return alpha\n\n\ndef celu(input: torch.Tensor, alpha: float = 1.0):\n    alpha = _parse_alpha(alpha)\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(\"input must be a torch.Tensor\")\n    if not input.is_cuda:\n        raise ValueError(\"input must be on CUDA device\")\n    if not torch.is_floating_point(input):\n        raise TypeError(\"input must be a floating point tensor\")\n\n    x_contig = input.contiguous()\n    out = torch.empty_like(x_contig)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    celu_kernel[grid](x_contig, out, n_elements, alpha, BLOCK_SIZE=1024)\n    return out\n\n\ndef celu_out(input: torch.Tensor, alpha: float = 1.0, out: torch.Tensor = None):\n    alpha = _parse_alpha(alpha)\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(\"input must be a torch.Tensor\")\n    if out is None or not isinstance(out, torch.Tensor):\n        raise TypeError(\"out must be a preallocated torch.Tensor\")\n    if not input.is_cuda or not out.is_cuda:\n        raise ValueError(\"input and out must be on CUDA device\")\n    if not torch.is_floating_point(input) or not torch.is_floating_point(out):\n        raise TypeError(\"input and out must be floating point tensors\")\n    if out.shape != input.shape:\n        raise ValueError(\"out must have the same shape as input\")\n    if out.dtype != input.dtype:\n        raise ValueError(\"out must have the same dtype as input\")\n\n    x_contig = input.contiguous()\n    if out.is_contiguous():\n        out_contig = out\n    else:\n        out_contig = torch.empty_like(x_contig)\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        if out_contig is not out:\n            out.copy_(out_contig)\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    celu_kernel[grid](x_contig, out_contig, n_elements, alpha, BLOCK_SIZE=1024)\n\n    if out_contig is not out:\n        out.copy_(out_contig)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.050608060813784,
        "meets_threshold": false
      },
      "aten::fill_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0181838726941386,
            "flaggems_speedup_vs_cuda": 0.974193333333333,
            "speedup_vs_flaggems": 1.0451558616299355
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fill_kernel(out_ptr, value_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val = tl.load(value_ptr)  # load scalar value (1-element tensor)\n    tl.store(out_ptr + offsets, val, mask=mask)\n\n\ndef fill__Scalar(*args, **kwargs):\n    # Expecting (self, value)\n    if len(args) >= 2:\n        self = args[0]\n        value = args[1]\n    else:\n        self = kwargs.get('self') or kwargs.get('input') or kwargs.get('tensor')\n        value = kwargs.get('value')\n    if self is None:\n        raise ValueError(\"fill__Scalar requires a target tensor 'self'.\")\n    if value is None:\n        raise ValueError(\"fill__Scalar requires a scalar 'value'.\")\n    assert self.is_cuda, \"fill__Scalar requires a CUDA tensor.\"\n    assert self.is_contiguous(), \"fill__Scalar currently supports only contiguous tensors.\"\n    # Create a 1-element tensor on device with matching dtype\n    value_t = torch.tensor([value], dtype=self.dtype, device=self.device)\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    fill_kernel[grid](self, value_t, n_elements, BLOCK_SIZE=1024)\n    return self\n\n\ndef fill__Tensor(*args, **kwargs):\n    # Expecting (self, other)\n    if len(args) >= 2:\n        self = args[0]\n        other = args[1]\n    else:\n        self = kwargs.get('self') or kwargs.get('input') or kwargs.get('tensor')\n        other = kwargs.get('other') or kwargs.get('value') or kwargs.get('src')\n    if self is None:\n        raise ValueError(\"fill__Tensor requires a target tensor 'self'.\")\n    if other is None:\n        raise ValueError(\"fill__Tensor requires a source tensor 'other'.\")\n    assert self.is_cuda, \"fill__Tensor requires a CUDA tensor.\"\n    assert self.is_contiguous(), \"fill__Tensor currently supports only contiguous tensors.\"\n    if other.numel() != 1:\n        raise RuntimeError(\"fill__Tensor expects 'other' to be a 0-dim or 1-element tensor.\")\n    # Move/convert 'other' to match device and dtype of 'self'\n    value_t = other.to(device=self.device, dtype=self.dtype).reshape(1)\n    n_elements = self.numel()\n    if n_elements == 0:\n        return self\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    fill_kernel[grid](self, value_t, n_elements, BLOCK_SIZE=1024)\n    return self",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0451558616299355,
        "meets_threshold": false
      },
      "embedding": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.5149854454025014,
            "flaggems_speedup_vs_cuda": 1.457967,
            "speedup_vs_flaggems": 1.0391081865381735
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef embedding(weight_ptr,\n              indices_ptr,\n              output_ptr,\n              N, D,\n              stride_w0, stride_w1,\n              stride_out0, stride_out1,\n              padding_idx,\n              HAS_PADDING: tl.constexpr,\n              BLOCK_SIZE_D: tl.constexpr):\n    pid_n = tl.program_id(axis=0)\n    pid_d = tl.program_id(axis=1)\n\n    cols = pid_d * BLOCK_SIZE_D + tl.arange(0, BLOCK_SIZE_D)\n    mask = cols < D\n\n    idx = tl.load(indices_ptr + pid_n)\n    row_w = idx * stride_w0\n    row_out = pid_n * stride_out0\n\n    vals = tl.load(weight_ptr + row_w + cols * stride_w1, mask=mask, other=0)\n\n    if HAS_PADDING:\n        is_pad = idx == padding_idx\n        vals = tl.where(is_pad, 0, vals)\n\n    tl.store(output_ptr + row_out + cols * stride_out1, vals, mask=mask)\n\n\nembedding_kernel = embedding\n\n\ndef embedding(*args, **kwargs):\n    if len(args) >= 2:\n        weight = args[0]\n        indices = args[1]\n        padding_idx = args[2] if len(args) >= 3 else kwargs.get('padding_idx', None)\n    else:\n        weight = kwargs.get('weight', None)\n        indices = kwargs.get('indices', None) if 'indices' in kwargs else kwargs.get('input', None)\n        padding_idx = kwargs.get('padding_idx', None)\n\n    if weight is None or indices is None:\n        raise ValueError(\"embedding requires 'weight' and 'indices' tensors.\")\n    if not weight.is_cuda or not indices.is_cuda:\n        raise ValueError(\"Triton embedding kernel requires CUDA tensors.\")\n    if weight.ndim != 2:\n        raise ValueError(\"weight must be a 2D tensor.\")\n\n    indices_flat = indices.contiguous().view(-1).to(torch.int64)\n    N = indices_flat.numel()\n    D = weight.shape[1]\n\n    out = torch.empty((*indices.shape, D), dtype=weight.dtype, device=weight.device)\n\n    weight_s0, weight_s1 = weight.stride(0), weight.stride(1)\n    out_2d = out.view(N, D)\n    out_s0, out_s1 = out_2d.stride(0), out_2d.stride(1)\n\n    has_padding = padding_idx is not None\n    pad_idx_val = int(padding_idx) if has_padding else 0\n\n    BLOCK_SIZE_D = 128\n    grid = (N, triton.cdiv(D, BLOCK_SIZE_D))\n\n    embedding_kernel[grid](\n        weight, indices_flat, out_2d,\n        N, D,\n        weight_s0, weight_s1,\n        out_s0, out_s1,\n        pad_idx_val,\n        HAS_PADDING=has_padding,\n        BLOCK_SIZE_D=BLOCK_SIZE_D,\n    )\n\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0391081865381735,
        "meets_threshold": false
      },
      "silu": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.025061184873499,
            "flaggems_speedup_vs_cuda": 0.986949,
            "speedup_vs_flaggems": 1.0386161644355474
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef silu(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    x_f32 = x.to(tl.float32)\n    y_f32 = x_f32 / (1.0 + tl.exp(-x_f32))\n    y_cast = y_f32.to(x.dtype)\n    tl.store(y_ptr + offsets, y_cast, mask=mask)\n\n\n_silu_kernel = silu\n\n\ndef silu(*args, **kwargs):\n    # Extract input tensor and inplace flag following torch.nn.functional.silu signature\n    if len(args) > 0:\n        x = args[0]\n        inplace = kwargs.get('inplace', False)\n        if len(args) > 1:\n            inplace = bool(args[1])\n    else:\n        if 'input' not in kwargs:\n            raise TypeError(\"silu() missing 1 required positional argument: 'input'\")\n        x = kwargs['input']\n        inplace = kwargs.get('inplace', False)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"silu() expected a torch.Tensor as input\")\n\n    # Fallback for unsupported device/dtype/strides\n    supported_dtypes = (torch.float16, torch.bfloat16, torch.float32)\n    if x.device.type != 'cuda' or x.dtype not in supported_dtypes:\n        # CPU or unsupported dtype\n        return torch.nn.functional.silu(x, inplace=inplace)\n\n    # For in-place, require contiguous; otherwise create contiguous copies for kernel\n    if inplace:\n        if not x.is_contiguous():\n            return torch.nn.functional.silu(x, inplace=True)\n        y = x  # operate in-place\n        x_contig = x\n        y_contig = y\n    else:\n        # out-of-place\n        y = torch.empty_like(x)\n        if not x.is_contiguous() or not y.is_contiguous():\n            # Fallback for non-contiguous tensors\n            return torch.nn.functional.silu(x, inplace=False)\n        x_contig = x\n        y_contig = y\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        return y\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _silu_kernel[grid](x_contig, y_contig, n_elements, BLOCK_SIZE=1024)\n    return y\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0386161644355474,
        "meets_threshold": false
      },
      "aten::div_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.064721166175036,
            "flaggems_speedup_vs_cuda": 1.02543833333333,
            "speedup_vs_flaggems": 1.0383083327049143
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef div_inplace_kernel(\n    x_ptr,         # Pointer to self tensor (in-place write-back)\n    y_ptr,         # Pointer to other tensor (can be 1-element for scalar-like)\n    n_elements,    # Number of elements in self\n    OTHER_IS_SCALAR: tl.constexpr,  # Whether 'other' is a scalar/1-element tensor\n    ROUNDING: tl.constexpr,         # 0: true division (floats), 1: trunc (ints), 2: floor (ints)\n    BLOCK_SIZE: tl.constexpr        # Block size\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if OTHER_IS_SCALAR:\n        y0 = tl.load(y_ptr)  # single element\n        y = y0\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask)\n\n    if ROUNDING == 0:\n        out = x / y\n    else:\n        # integer division modes\n        q = x / y  # truncation toward zero for integer types\n        if ROUNDING == 2:\n            # floor division adjustment for negatives with non-zero remainder\n            r = x - q * y\n            sign_diff = (x < 0) != (y < 0)\n            need_adj = (r != 0) & sign_diff\n            q = tl.where(need_adj, q - 1, q)\n        out = q\n\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _div__inplace_impl(self: torch.Tensor, other, rounding_mode=None):\n    if not self.is_cuda:\n        raise RuntimeError(\"Triton kernel requires CUDA tensors.\")\n    if self.is_complex():\n        raise RuntimeError(\"div_ Triton implementation does not support complex dtypes.\")\n    if self.dtype == torch.bool:\n        raise RuntimeError(\"div_ Triton implementation does not support bool dtype.\")\n\n    # Validate rounding_mode\n    valid_modes = (None, 'trunc', 'floor')\n    if rounding_mode not in valid_modes:\n        raise ValueError(\"rounding_mode must be one of None, 'trunc', or 'floor'.\")\n\n    # Determine dtype category\n    is_floating = self.is_floating_point()\n    is_integer = self.dtype in (torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64)\n\n    if is_floating:\n        if rounding_mode is not None:\n            raise RuntimeError(\"rounding_mode must be None for floating-point division in div_.\")\n        rounding_code = 0\n    elif is_integer:\n        if rounding_mode is None:\n            raise RuntimeError(\"Integer in-place div_ requires rounding_mode 'trunc' or 'floor'.\")\n        rounding_code = 1 if rounding_mode == 'trunc' else 2\n    else:\n        raise RuntimeError(f\"Unsupported dtype {self.dtype} for div_ Triton implementation.\")\n\n    # Prepare 'other' as tensor on device with compatible dtype\n    if isinstance(other, torch.Tensor):\n        other_t = other\n    else:\n        other_t = self.new_tensor(other)\n\n    if not other_t.is_cuda:\n        other_t = other_t.to(device=self.device)\n\n    # Cast other to self dtype if needed\n    if other_t.dtype != self.dtype:\n        # For integer modes, enforce integer dtype\n        if is_integer:\n            # Cast to same integer dtype (PyTorch div_ expects same dtype for in-place)\n            other_t = other_t.to(self.dtype)\n        else:\n            other_t = other_t.to(self.dtype)\n\n    # Broadcasting support: allow other be 1-element or same numel\n    if other_t.numel() not in (1, self.numel()):\n        raise RuntimeError(\"This Triton div_ implementation supports 'other' with numel == 1 or equal to 'self' numel.\")\n    if not self.is_contiguous():\n        raise RuntimeError(\"This Triton div_ implementation currently requires 'self' to be contiguous.\")\n    if other_t.numel() != 1 and not other_t.is_contiguous():\n        raise RuntimeError(\"This Triton div_ implementation currently requires 'other' to be contiguous when not scalar-like.\")\n\n    n_elements = self.numel()\n    other_is_scalar = (other_t.numel() == 1)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    div_inplace_kernel[grid](\n        self, other_t, n_elements,\n        OTHER_IS_SCALAR=other_is_scalar,\n        ROUNDING=rounding_code,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return self\n\n\ndef div__Tensor(self: torch.Tensor, other: torch.Tensor):\n    return _div__inplace_impl(self, other, rounding_mode=None)\n\n\ndef div__Tensor_mode(self: torch.Tensor, other: torch.Tensor, rounding_mode=None):\n    return _div__inplace_impl(self, other, rounding_mode=rounding_mode)\n\n\ndef div__Scalar(self: torch.Tensor, other):\n    return _div__inplace_impl(self, other, rounding_mode=None)\n\n\ndef div__Scalar_mode(self: torch.Tensor, other, rounding_mode=None):\n    return _div__inplace_impl(self, other, rounding_mode=rounding_mode)",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0383083327049143,
        "meets_threshold": false
      },
      "bitwise_not": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0143202236155122,
            "flaggems_speedup_vs_cuda": 0.984588,
            "speedup_vs_flaggems": 1.0301976294810746
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef bitwise_not(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr, IS_BOOL: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if IS_BOOL:\n        # For boolean tensors, bitwise_not is logical not\n        out = x == 0\n    else:\n        out = ~x\n\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\n# Preserve the kernel object before defining the Python wrapper with the same name\n_bitwise_not_kernel = bitwise_not\n\n\ndef bitwise_not(*args, **kwargs):\n    # Parse input tensor\n    if len(args) >= 1:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    else:\n        raise ValueError(\"bitwise_not expects an 'input' tensor as a positional or keyword argument 'input'\")\n\n    # Validate dtype\n    if x.dtype not in (\n        torch.bool,\n        torch.uint8,\n        torch.int8,\n        torch.int16,\n        torch.int32,\n        torch.int64,\n    ):\n        raise TypeError(f\"Unsupported dtype {x.dtype} for bitwise_not. Supported dtypes: \"\n                        \"bool, uint8, int8, int16, int32, int64\")\n\n    # Handle 'out' keyword (may be a Tensor or a single-element tuple)\n    out = kwargs.get('out', None)\n    if out is not None and isinstance(out, tuple):\n        if len(out) != 1:\n            raise ValueError(\"out must be a single-element tuple or a Tensor\")\n        out = out[0]\n\n    if out is not None:\n        if out.dtype != x.dtype:\n            raise TypeError(\"out tensor must have the same dtype as input\")\n        if out.numel() != x.numel():\n            raise ValueError(\"out tensor must have the same number of elements as input\")\n        if not out.is_contiguous():\n            raise ValueError(\"out tensor must be contiguous\")\n        output = out\n    else:\n        output = torch.empty_like(x)\n\n    # Ensure contiguous input and output\n    x_c = x.contiguous()\n    if not output.is_contiguous():\n        # torch.empty_like is contiguous by default; out must be contiguous if provided\n        raise ValueError(\"Output tensor must be contiguous\")\n\n    n_elements = x_c.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _bitwise_not_kernel[grid](\n        x_c, output, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        IS_BOOL=(x.dtype == torch.bool)\n    )\n\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0301976294810746,
        "meets_threshold": false
      },
      "aten::elu_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0408910608534836,
            "flaggems_speedup_vs_cuda": 1.011564,
            "speedup_vs_flaggems": 1.0289917996819615
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef elu_(x_ptr,  # pointer to input/output tensor (in-place)\n         n_elements,  # number of elements\n         alpha,  # scalar alpha\n         scale,  # scalar scale\n         input_scale,  # scalar input_scale\n         BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x_f32 = tl.cast(x, tl.float32)\n\n    pos = x_f32 > 0\n    neg = alpha * (tl.exp(x_f32 * input_scale) - 1.0)\n    y_f32 = tl.where(pos, x_f32, neg) * scale\n\n    y = tl.cast(y_f32, x.dtype)\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n\n# keep an alias to the kernel before defining the Python wrapper with the same name\nelu__kernel = elu_\n\n\ndef elu_(*args, **kwargs):\n    # Parse input tensor\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"self\", kwargs.get(\"input\", None))\n    if x is None:\n        raise ValueError(\"elu_: expected an input tensor as the first argument\")\n\n    # Defaults following aten.elu_ signature\n    alpha = 1.0\n    scale = 1.0\n    input_scale = 1.0\n\n    # Positional overrides\n    if len(args) >= 2:\n        alpha = args[1]\n    if len(args) >= 3:\n        scale = args[2]\n    if len(args) >= 4:\n        input_scale = args[3]\n\n    # Keyword overrides\n    if \"alpha\" in kwargs:\n        alpha = kwargs[\"alpha\"]\n    if \"scale\" in kwargs:\n        scale = kwargs[\"scale\"]\n    if \"input_scale\" in kwargs:\n        input_scale = kwargs[\"input_scale\"]\n\n    # Convert to Python floats if tensors or other numeric types\n    if isinstance(alpha, torch.Tensor):\n        alpha = float(alpha.item())\n    else:\n        alpha = float(alpha)\n    if isinstance(scale, torch.Tensor):\n        scale = float(scale.item())\n    else:\n        scale = float(scale)\n    if isinstance(input_scale, torch.Tensor):\n        input_scale = float(input_scale.item())\n    else:\n        input_scale = float(input_scale)\n\n    # Fallback conditions: non-CUDA, non-contiguous, unsupported dtype, or zero elements\n    supported_dtypes = {torch.float16, torch.bfloat16, torch.float32}\n    if (not x.is_cuda) or (not x.is_contiguous()) or (x.dtype not in supported_dtypes) or (x.numel() == 0):\n        torch.ops.aten.elu_(x, alpha, scale, input_scale)\n        return x\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    elu__kernel[grid](x, n_elements, alpha, scale, input_scale, BLOCK_SIZE=BLOCK_SIZE)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0289917996819615,
        "meets_threshold": false
      },
      "aten::fill": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0141922128249632,
            "flaggems_speedup_vs_cuda": 0.986006,
            "speedup_vs_flaggems": 1.028586248790538
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fill_kernel(\n    out_ptr,\n    n_elements,\n    value,\n    BLOCK_SIZE: tl.constexpr,\n    OUT_DTYPE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val_vec = tl.full((BLOCK_SIZE,), value, dtype=OUT_DTYPE)\n    tl.store(out_ptr + offsets, val_vec, mask=mask)\n\n\ndef _torch_dtype_to_triton(dtype: torch.dtype):\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    if dtype == torch.float64:\n        return tl.float64\n    if dtype == torch.int8:\n        return tl.int8\n    if dtype == torch.uint8:\n        return tl.uint8\n    if dtype == torch.int16:\n        return tl.int16\n    if dtype == torch.int32:\n        return tl.int32\n    if dtype == torch.int64:\n        return tl.int64\n    if dtype == torch.bool:\n        return tl.int1\n    raise NotImplementedError(f\"Unsupported dtype: {dtype}\")\n\n\ndef _launch_fill_kernel(out: torch.Tensor, value, block_size: int = 1024):\n    if out.numel() == 0:\n        return out\n    assert out.is_cuda, \"Tensor must be on CUDA device to use Triton kernels\"\n\n    # Ensure contiguous target for kernel; copy back if needed\n    needs_copy_back = False\n    tgt = out\n    if not out.is_contiguous():\n        tgt = torch.empty_like(out, memory_format=torch.contiguous_format)\n        needs_copy_back = True\n\n    n_elements = tgt.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    out_dtype = _torch_dtype_to_triton(tgt.dtype)\n\n    fill_kernel[grid](tgt, n_elements, value, BLOCK_SIZE=block_size, OUT_DTYPE=out_dtype)\n\n    if needs_copy_back:\n        out.copy_(tgt)\n    return out\n\n\ndef fill_Scalar(input: torch.Tensor, value):\n    out = torch.empty_like(input)\n    _launch_fill_kernel(out, value)\n    return out\n\n\ndef fill_Scalar_out(input: torch.Tensor, value, out: torch.Tensor):\n    _launch_fill_kernel(out, value)\n    return out\n\n\ndef fill_Tensor(input: torch.Tensor, value_tensor: torch.Tensor):\n    assert value_tensor.numel() == 1, \"fill_Tensor expects a 0-dim or single-element tensor as value\"\n    value = value_tensor.item()\n    out = torch.empty_like(input)\n    _launch_fill_kernel(out, value)\n    return out\n\n\ndef fill_Tensor_out(input: torch.Tensor, value_tensor: torch.Tensor, out: torch.Tensor):\n    assert value_tensor.numel() == 1, \"fill_Tensor_out expects a 0-dim or single-element tensor as value\"\n    value = value_tensor.item()\n    _launch_fill_kernel(out, value)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.028586248790538,
        "meets_threshold": false
      },
      "logical_not": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0028209197444846,
            "flaggems_speedup_vs_cuda": 0.976009666666667,
            "speedup_vs_flaggems": 1.0274702741104862
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logical_not(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    zero = tl.zeros([BLOCK_SIZE], dtype=x.dtype)\n    res_mask = x == zero\n    res = res_mask.to(tl.uint8)\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\n_logical_not_kernel = logical_not\n\n\ndef logical_not(*args, **kwargs):\n    if len(args) > 0:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    else:\n        raise TypeError(\"logical_not() missing required argument 'input'\")\n\n    out = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"input must be a torch.Tensor\")\n    if not x.is_cuda:\n        raise RuntimeError(\"logical_not: input must be a CUDA tensor\")\n\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.dtype != torch.bool:\n            raise TypeError(\"out dtype must be torch.bool\")\n        if out.numel() != x.numel():\n            raise RuntimeError(\"out must have the same number of elements as input\")\n        if out.device != x.device:\n            raise RuntimeError(\"out must be on the same device as input\")\n        output = out\n    else:\n        output = torch.empty_like(x, dtype=torch.bool, device=x.device)\n\n    x_contig = x.contiguous()\n    output_contig = output.contiguous()\n    n_elements = x_contig.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _logical_not_kernel[grid](x_contig, output_contig, n_elements, BLOCK_SIZE=1024)\n    if output_contig.data_ptr() != output.data_ptr():\n        output.copy_(output_contig)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0274702741104862,
        "meets_threshold": false
      },
      "elu": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0709168984897042,
            "flaggems_speedup_vs_cuda": 1.04276166666667,
            "speedup_vs_flaggems": 1.0270006394778937
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef elu(x_ptr, out_ptr, n_elements, alpha, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    x32 = x.to(tl.float32)\n    res32 = tl.where(x32 > 0.0, x32, alpha * (tl.exp(x32) - 1.0))\n    y = tl.cast(res32, x.dtype)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\nelu_kernel = elu\n\n\ndef elu(*args, **kwargs):\n    # Parse inputs similar to torch.nn.functional.elu\n    if len(args) > 0:\n        x = args[0]\n        alpha = args[1] if len(args) > 1 else kwargs.get('alpha', 1.0)\n        inplace = args[2] if len(args) > 2 else kwargs.get('inplace', False)\n    else:\n        if 'input' in kwargs:\n            x = kwargs['input']\n        else:\n            raise TypeError(\"elu() missing required argument 'input'\")\n        alpha = kwargs.get('alpha', 1.0)\n        inplace = kwargs.get('inplace', False)\n\n    if not isinstance(alpha, (int, float)):\n        alpha = float(alpha)\n\n    # Fallback for unsupported cases\n    if (\n        x.device.type != 'cuda'\n        or x.dtype not in (torch.float16, torch.float32, torch.bfloat16)\n        or not x.is_contiguous()\n        or x.numel() == 0\n    ):\n        return torch.nn.functional.elu(*args, **kwargs)\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    if inplace:\n        out = x\n        x_in = x\n    else:\n        out = torch.empty_like(x)\n        x_in = x\n\n    elu_kernel[grid](x_in, out, n_elements, float(alpha), BLOCK_SIZE=BLOCK_SIZE)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0270006394778937,
        "meets_threshold": false
      },
      "isnan": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9946065092169111,
            "flaggems_speedup_vs_cuda": 0.970021666666667,
            "speedup_vs_flaggems": 1.0253446323881878
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef isnan(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    res = x != x  # True for NaN, False otherwise (works for float/bfloat/half; integers always False)\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\n# Preserve a handle to the Triton kernel before defining the Python wrapper with the same name.\n_isnan_kernel = isnan\n\n\ndef isnan(*args, **kwargs):\n    # Extract input tensor following torch.isnan signature\n    if 'input' in kwargs:\n        x = kwargs['input']\n    elif len(args) >= 1:\n        x = args[0]\n    else:\n        raise TypeError(\"isnan() missing required argument 'input' (pos 1)\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"isnan() expected a torch.Tensor as input\")\n\n    # Complex tensors: isnan is True if either real or imag is NaN\n    if x.is_complex():\n        real_mask = isnan(x.real)\n        imag_mask = isnan(x.imag)\n        return torch.logical_or(real_mask, imag_mask)\n\n    # Ensure CUDA device for Triton execution\n    assert x.is_cuda, \"Input tensor must be on a CUDA device for Triton execution\"\n\n    x_buf = x.contiguous()\n    out_buf = torch.empty_like(x_buf, dtype=torch.bool)\n\n    n_elements = out_buf.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _isnan_kernel[grid](x_buf, out_buf, n_elements, BLOCK_SIZE=1024)\n    return out_buf.view(x.shape)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0253446323881878,
        "meets_threshold": false
      },
      "aten::gelu": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9975905831327504,
            "flaggems_speedup_vs_cuda": 0.974782666666667,
            "speedup_vs_flaggems": 1.023397950380136
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gelu_kernel(x_ptr, out_ptr, n_elements, APPROX: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_f32 = x.to(tl.float32)\n\n    if APPROX:\n        # tanh approximation: 0.5*x*(1 + tanh(sqrt(2/pi)*(x + 0.044715*x^3)))\n        c = 0.7978845608028654  # sqrt(2/pi)\n        z = c * (x_f32 + 0.044715 * x_f32 * x_f32 * x_f32)\n        e2z = tl.exp(2.0 * z)\n        tanh_z = (e2z - 1.0) / (e2z + 1.0)\n        y = 0.5 * x_f32 * (1.0 + tanh_z)\n    else:\n        # exact (using erf approximation) : x * 0.5 * (1 + erf(x / sqrt(2)))\n        inv_sqrt2 = 0.7071067811865476\n        z = x_f32 * inv_sqrt2\n        abs_z = tl.abs(z)\n        t = 1.0 / (1.0 + 0.3275911 * abs_z)\n        # Abramowitz and Stegun formula coefficients\n        a1 = 0.254829592\n        a2 = -0.284496736\n        a3 = 1.421413741\n        a4 = -1.453152027\n        a5 = 1.061405429\n        poly = (((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t)\n        erf_approx = tl.where(z >= 0, 1.0, -1.0) * (1.0 - poly * tl.exp(-z * z))\n        cdf = 0.5 * (1.0 + erf_approx)\n        y = x_f32 * cdf\n\n    y = y.to(x.dtype)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef gelu(input: torch.Tensor, approximate: str = \"none\"):\n    out = torch.empty_like(input)\n    n_elements = out.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    approx_flag = (approximate == \"tanh\")\n    gelu_kernel[grid](input, out, n_elements, APPROX=approx_flag, BLOCK_SIZE=1024)\n    return out\n\n\ndef gelu_out(input: torch.Tensor, out: torch.Tensor, approximate: str = \"none\"):\n    assert out.shape == input.shape and out.dtype == input.dtype and out.device == input.device\n    n_elements = out.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    approx_flag = (approximate == \"tanh\")\n    gelu_kernel[grid](input, out, n_elements, APPROX=approx_flag, BLOCK_SIZE=1024)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.023397950380136,
        "meets_threshold": false
      },
      "bitwise_or": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0102434993961964,
            "flaggems_speedup_vs_cuda": 0.988733333333333,
            "speedup_vs_flaggems": 1.0217552755001653
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef bitwise_or_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    z = x | y\n    tl.store(out_ptr + offsets, z, mask=mask)\n\n\ndef _check_valid_bitwise_dtype(dtype: torch.dtype):\n    if dtype.is_floating_point or getattr(dtype, \"is_complex\", False):\n        raise TypeError(\"bitwise_or is only supported for integer and boolean dtypes\")\n\n\ndef _bitwise_or_impl(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor | None = None) -> torch.Tensor:\n    if x.device != y.device:\n        raise RuntimeError(\"bitwise_or: expected both inputs on the same device\")\n    _check_valid_bitwise_dtype(x.dtype)\n    _check_valid_bitwise_dtype(y.dtype)\n\n    # Type promotion\n    result_dtype = torch.result_type(x, y)\n    _check_valid_bitwise_dtype(result_dtype)\n    x = x.to(result_dtype)\n    y = y.to(result_dtype)\n\n    # Broadcasting\n    xb, yb = torch.broadcast_tensors(x, y)\n\n    # Prepare output\n    if out is None:\n        out = torch.empty_like(xb, dtype=result_dtype, device=xb.device)\n    else:\n        # Ensure 'out' is compatible\n        if out.device != xb.device:\n            raise RuntimeError(\"bitwise_or: 'out' tensor must be on the same device as inputs\")\n        if out.shape != xb.shape:\n            raise RuntimeError(\"bitwise_or: 'out' tensor has incorrect shape for the broadcasted result\")\n        if out.dtype != result_dtype:\n            raise RuntimeError(\"bitwise_or: 'out' tensor has incorrect dtype for the result\")\n\n    # Flatten and make contiguous for the kernel\n    xb_flat = xb.contiguous().view(-1)\n    yb_flat = yb.contiguous().view(-1)\n    out_flat = out.view(-1)\n\n    n_elements = out_flat.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    bitwise_or_kernel[grid](xb_flat, yb_flat, out_flat, n_elements, BLOCK_SIZE=1024)\n    return out\n\n\ndef bitwise_or_Tensor(*args, **kwargs) -> torch.Tensor:\n    # Expected signature: (Tensor self, Tensor other, *, out=None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get(\"input\", kwargs.get(\"self\"))\n        y = kwargs.get(\"other\")\n    out = kwargs.get(\"out\", None)\n    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n        raise TypeError(\"bitwise_or_Tensor expects two Tensor arguments\")\n    return _bitwise_or_impl(x, y, out=out)\n\n\ndef bitwise_or_Scalar(*args, **kwargs) -> torch.Tensor:\n    # Expected signature: (Tensor self, Scalar other, *, out=None)\n    if len(args) >= 2:\n        x, s = args[0], args[1]\n    else:\n        x = kwargs.get(\"input\", kwargs.get(\"self\"))\n        s = kwargs.get(\"other\")\n    out = kwargs.get(\"out\", None)\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"bitwise_or_Scalar expects (Tensor, Scalar)\")\n    # Convert scalar to tensor on the same device\n    if isinstance(s, torch.Tensor):\n        if s.numel() != 1:\n            raise TypeError(\"bitwise_or_Scalar expects a scalar for 'other'\")\n        s_t = s.to(device=x.device)\n    else:\n        s_t = torch.tensor(s, device=x.device)\n    _check_valid_bitwise_dtype(x.dtype)\n    _check_valid_bitwise_dtype(torch.result_type(x, s_t))\n    return _bitwise_or_impl(x, s_t, out=out)\n\n\ndef bitwise_or_Scalar_Tensor(*args, **kwargs) -> torch.Tensor:\n    # Expected signature: (Scalar self, Tensor other, *, out=None)\n    if len(args) >= 2:\n        s, y = args[0], args[1]\n    else:\n        s = kwargs.get(\"input\", kwargs.get(\"self\"))\n        y = kwargs.get(\"other\")\n    out = kwargs.get(\"out\", None)\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"bitwise_or_Scalar_Tensor expects (Scalar, Tensor)\")\n    # Convert scalar to tensor on the same device\n    if isinstance(s, torch.Tensor):\n        if s.numel() != 1:\n            raise TypeError(\"bitwise_or_Scalar_Tensor expects a scalar for 'self'\")\n        s_t = s.to(device=y.device)\n    else:\n        s_t = torch.tensor(s, device=y.device)\n    _check_valid_bitwise_dtype(y.dtype)\n    _check_valid_bitwise_dtype(torch.result_type(s_t, y))\n    return _bitwise_or_impl(s_t, y, out=out)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0217552755001653,
        "meets_threshold": false
      },
      "aten::celu_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.048720939741635,
            "flaggems_speedup_vs_cuda": 1.026429,
            "speedup_vs_flaggems": 1.0217179558855363
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef celu_(x_ptr,  # Pointer to input tensor (will be modified in-place)\n          n_elements,  # Number of elements in the tensor\n          alpha,  # CELU alpha parameter (scalar)\n          BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x32 = tl.cast(x, tl.float32)\n    alpha32 = tl.cast(alpha, tl.float32)\n\n    neg_part = alpha32 * (tl.exp(x32 / alpha32) - 1.0)\n    y32 = tl.where(x32 > 0, x32, neg_part)\n    y = tl.cast(y32, x.dtype)\n\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n\n# Preserve reference to the Triton kernel before defining the Python wrapper with the same name\ncelu_kernel = celu_\n\n\ndef celu_(x: torch.Tensor, alpha: float = 1.0):\n    assert x.is_cuda, \"Input tensor must be on CUDA device.\"\n    assert x.is_floating_point(), \"CELU requires a floating point tensor.\"\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    celu_kernel[grid](x, n_elements, alpha, BLOCK_SIZE=1024)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0217179558855363,
        "meets_threshold": false
      },
      "minimum": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0140143084264583,
            "flaggems_speedup_vs_cuda": 0.994697,
            "speedup_vs_flaggems": 1.0194202942468493
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef minimum(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    out = tl.where(x < y, x, y)\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\n_minimum_kernel = minimum\n\n\ndef minimum(*args, **kwargs):\n    # Parse inputs similar to torch.minimum(input, other, *, out=None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get(\"input\", None)\n        y = kwargs.get(\"other\", None)\n        if x is None or y is None:\n            raise TypeError(\"minimum() missing required positional arguments: 'input' and 'other'\")\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n        raise TypeError(\"minimum() expects torch.Tensor inputs\")\n    if not x.is_cuda or not y.is_cuda:\n        raise ValueError(\"minimum() expects CUDA tensors\")\n    if x.device != y.device:\n        raise ValueError(\"Input tensors must be on the same device\")\n\n    # Determine result dtype via type promotion\n    result_dtype = torch.result_type(x, y)\n\n    # Broadcast to common shape\n    xb, yb = torch.broadcast_tensors(x.to(result_dtype), y.to(result_dtype))\n    xb = xb.contiguous()\n    yb = yb.contiguous()\n    out_shape = xb.shape\n    device = x.device\n\n    # Prepare output tensor\n    if out is None:\n        output = torch.empty(out_shape, dtype=result_dtype, device=device)\n    else:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if not out.is_cuda:\n            raise ValueError(\"out must be a CUDA tensor\")\n        if out.device != device:\n            raise ValueError(\"out must be on the same device as inputs\")\n        if out.dtype != result_dtype:\n            raise ValueError(f\"out dtype must be {result_dtype}, got {out.dtype}\")\n        if out.shape != out_shape:\n            raise ValueError(f\"out shape must be {out_shape}, got {out.shape}\")\n        output = out\n\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _minimum_kernel[grid](xb, yb, output, n_elements, BLOCK_SIZE=1024)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0194202942468493,
        "meets_threshold": false
      },
      "exp": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0042477462050325,
            "flaggems_speedup_vs_cuda": 0.985438666666667,
            "speedup_vs_flaggems": 1.0190870118806976
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef exp(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.exp(x.to(tl.float32))\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\n# Preserve a handle to the Triton kernel before defining the Python wrapper with the same name.\nexp_kernel = exp\n\n\ndef exp(*args, **kwargs):\n    # Extract input tensor following torch.exp signature: exp(input, *, out=None)\n    if len(args) > 1:\n        # Defer to PyTorch for unsupported calling patterns\n        return torch.exp(*args, **kwargs)\n\n    x = None\n    if len(args) == 1:\n        x = args[0]\n    else:\n        # Try common keyword for input\n        x = kwargs.get(\"input\", None)\n\n    if not torch.is_tensor(x):\n        return torch.exp(*args, **kwargs)\n\n    out = kwargs.get(\"out\", None)\n\n    # Fallback conditions: unsupported device or dtype or complex\n    if (not x.is_cuda) or x.is_complex() or x.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        return torch.exp(*args, **kwargs)\n\n    # Validate 'out' when provided; otherwise, allocate\n    if out is not None:\n        # If out doesn't match requirements, fallback to PyTorch for correct error semantics\n        if (not torch.is_tensor(out)) or (out.device != x.device) or (out.shape != x.shape) or (out.dtype != x.dtype):\n            return torch.exp(*args, **kwargs)\n        out_target = out if out.is_contiguous() else torch.empty_like(x, device=x.device)\n    else:\n        out_target = torch.empty_like(x, device=x.device)\n\n    # Ensure input is contiguous for the kernel\n    x_contig = x if x.is_contiguous() else x.contiguous()\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        if out is not None and not out.is_contiguous():\n            out.copy_(out_target)\n            return out\n        return out_target\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    exp_kernel[grid](x_contig, out_target, n_elements, BLOCK_SIZE=1024)\n\n    # If 'out' was provided and non-contiguous, copy results back\n    if out is not None:\n        if not out.is_contiguous():\n            out.copy_(out_target)\n        return out\n\n    return out_target",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0190870118806976,
        "meets_threshold": false
      },
      "aten::relu_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0135164395555174,
            "flaggems_speedup_vs_cuda": 0.994701,
            "speedup_vs_flaggems": 1.0189156737105094
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef relu_(x_ptr,  # *Pointer* to input/output tensor (in-place).\n          n_elements,  # Number of elements.\n          BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    zero = x * 0\n    y = tl.where(x > 0, x, zero)\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n\n# Keep a reference to the Triton kernel before defining the Python wrapper with the same name.\nrelu__kernel = relu_\n\n\ndef relu_(*args, **kwargs):\n    # Expect the first positional argument to be the tensor.\n    x = args[0] if len(args) > 0 else kwargs.get('input', kwargs.get('x', None))\n    if x is None:\n        raise ValueError(\"relu_ expects a tensor as the first positional argument.\")\n    if not x.is_cuda:\n        raise ValueError(\"relu_ Triton implementation requires a CUDA tensor.\")\n    if not x.is_contiguous():\n        raise ValueError(\"relu_ Triton implementation requires a contiguous tensor.\")\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    relu__kernel[grid](x, n_elements, BLOCK_SIZE=1024)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0189156737105094,
        "meets_threshold": false
      },
      "abs": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0156313291279528,
            "flaggems_speedup_vs_cuda": 0.997223,
            "speedup_vs_flaggems": 1.0184595914133077
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef abs(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.abs(x)\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\n_abs_kernel = abs\n\n\ndef abs(*args, **kwargs):\n    # Parse input tensor\n    x = None\n    if len(args) >= 1 and torch.is_tensor(args[0]):\n        x = args[0]\n    elif 'input' in kwargs and torch.is_tensor(kwargs['input']):\n        x = kwargs['input']\n    else:\n        return torch.abs(*args, **kwargs)\n\n    out = kwargs.get('out', None)\n\n    # Fallbacks for unsupported cases\n    if not x.is_cuda:\n        return torch.abs(*args, **kwargs)\n    if torch.is_complex(x) or x.dtype == torch.bool:\n        return torch.abs(*args, **kwargs)\n\n    # Prepare contiguous input\n    x_contig = x.contiguous()\n    n_elements = x_contig.numel()\n\n    # Prepare output tensor\n    if out is None:\n        y = torch.empty_like(x_contig)\n        need_copy_back = False\n        out_tensor = None\n    else:\n        if (not torch.is_tensor(out) or out.device != x.device or out.dtype != x.dtype\n                or out.numel() != x.numel()):\n            return torch.abs(*args, **kwargs)\n        if out.is_contiguous():\n            y = out\n            need_copy_back = False\n            out_tensor = out\n        else:\n            y = out.contiguous()\n            need_copy_back = True\n            out_tensor = out\n\n    # Launch kernel\n    if n_elements > 0:\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _abs_kernel[grid](x_contig, y, n_elements, BLOCK_SIZE=1024)\n\n    # Return or copy to out\n    if out is None:\n        return y.view(x.shape)\n    else:\n        if need_copy_back:\n            out_tensor.copy_(y)\n        return out_tensor\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0184595914133077,
        "meets_threshold": false
      },
      "aten::reciprocal": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.021824661021354,
            "flaggems_speedup_vs_cuda": 1.00518566666667,
            "speedup_vs_flaggems": 1.0165531552094857
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef reciprocal_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    one = tl.full([BLOCK_SIZE], 1, x.dtype)\n    y = one / x\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _reciprocal_impl(x: torch.Tensor, out: torch.Tensor = None):\n    # Fallback for unsupported dtypes/devices\n    if not x.is_cuda or x.is_complex():\n        if out is None:\n            return torch.ops.aten.reciprocal(x)\n        else:\n            return torch.ops.aten.reciprocal.out(x, out=out)\n\n    if out is None:\n        out = torch.empty_like(x)\n\n    # Ensure same device and dtype\n    assert out.device == x.device, \"Input and output must be on the same device\"\n    assert out.dtype == x.dtype, \"Output dtype must match input dtype\"\n    assert out.numel() == x.numel(), \"Output must have the same number of elements as input\"\n\n    x_contig = x.contiguous()\n    out_contig = out.contiguous()\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        return out  # nothing to do\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    reciprocal_kernel[grid](x_contig, out_contig, n_elements, BLOCK_SIZE=1024)\n\n    if out is not out_contig:\n        out.copy_(out_contig)\n    return out\n\n\n# ('reciprocal', <Autograd.disable: False>)\ndef reciprocal(*args, **kwargs):\n    # Accept a single tensor argument\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        # Try common keyword names\n        x = kwargs.get(\"input\", kwargs.get(\"self\", kwargs.get(\"a\", kwargs.get(\"args\", None))))\n    if x is None:\n        raise ValueError(\"reciprocal expects a tensor as the first argument\")\n    return _reciprocal_impl(x)\n\n\n# ('reciprocal.out', <Autograd.disable: False>)\ndef reciprocal_out(*args, **kwargs):\n    # Accept (x, out) or keyword args self/input and out\n    x = None\n    out = None\n    if len(args) >= 2:\n        x, out = args[0], args[1]\n    elif len(args) == 1:\n        x = args[0]\n        out = kwargs.get(\"out\", None)\n    else:\n        x = kwargs.get(\"input\", kwargs.get(\"self\", kwargs.get(\"a\", None)))\n        out = kwargs.get(\"out\", None)\n\n    if x is None or out is None:\n        raise ValueError(\"reciprocal_out expects arguments (input, out)\")\n\n    _reciprocal_impl(x, out=out)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0165531552094857,
        "meets_threshold": false
      },
      "aten::mse_loss": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.681649776463889,
            "flaggems_speedup_vs_cuda": 1.657178,
            "speedup_vs_flaggems": 1.0147671381492447
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _mse_elemwise_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    diff = x - y\n    sq = diff * diff\n    tl.store(out_ptr + offsets, sq, mask=mask)\n\n\n@triton.jit\ndef _mse_reduce_kernel(x_ptr, y_ptr, acc_ptr, n_elements, scale, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # load as float32 for stable accumulation\n    x = tl.load(x_ptr + offsets, mask=mask, other=0).to(tl.float32)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0).to(tl.float32)\n    diff = x - y\n    sq = diff * diff\n    sq = sq * scale\n    block_sum = tl.sum(sq, axis=0)\n    tl.atomic_add(acc_ptr, block_sum)\n\n\ndef _parse_reduction(reduction):\n    # Accept both strings and integers consistent with ATen Reduction enum:\n    # 0: 'none', 1: 'mean', 2: 'sum'\n    if isinstance(reduction, str):\n        r = reduction.lower()\n        if r == \"none\":\n            return 0\n        if r == \"mean\":\n            return 1\n        if r == \"sum\":\n            return 2\n        raise ValueError(f\"Invalid reduction string: {reduction}\")\n    # Assume integer\n    if reduction in (0, 1, 2):\n        return int(reduction)\n    raise ValueError(f\"Invalid reduction value: {reduction}\")\n\n\ndef _ensure_supported_dtype(t: torch.Tensor, op_name=\"mse_loss\"):\n    if t.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        raise TypeError(\n            f\"{op_name} Triton kernel supports float16, bfloat16, and float32 dtypes, got {t.dtype}.\"\n        )\n\n\ndef _launch_mse_elemwise(x, y, out):\n    n_elements = out.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _mse_elemwise_kernel[grid](x, y, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n\ndef _launch_mse_reduce(x, y, n_elements, scale):\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    acc = torch.zeros((), device=x.device, dtype=torch.float32)\n    _mse_reduce_kernel[grid](x, y, acc, n_elements, float(scale), BLOCK_SIZE=BLOCK_SIZE)\n    return acc\n\n\ndef mse_loss(*args, **kwargs):\n    # Expected calling pattern: mse_loss(self, target, reduction=Mean)\n    if len(args) < 2:\n        raise TypeError(\"mse_loss requires at least 2 positional arguments: (input, target)\")\n    inp = args[0]\n    target = args[1]\n    reduction = kwargs.get(\"reduction\", args[2] if len(args) > 2 else 1)\n    reduction = _parse_reduction(reduction)\n\n    if not isinstance(inp, torch.Tensor) or not isinstance(target, torch.Tensor):\n        raise TypeError(\"mse_loss expects tensor inputs\")\n\n    if inp.numel() != target.numel():\n        raise ValueError(\"mse_loss: input and target must have the same number of elements\")\n\n    if inp.device != target.device:\n        raise ValueError(\"mse_loss: input and target must be on the same device\")\n\n    _ensure_supported_dtype(inp, \"mse_loss\")\n    _ensure_supported_dtype(target, \"mse_loss\")\n\n    x = inp.contiguous()\n    y = target.contiguous()\n\n    n_elements = x.numel()\n\n    if reduction == 0:  # 'none'\n        out = torch.empty_like(x)\n        if not out.is_contiguous():\n            # Ensure output is contiguous for Triton; then copy back\n            tmp = torch.empty_like(x, memory_format=torch.contiguous_format)\n            _launch_mse_elemwise(x, y, tmp)\n            out.copy_(tmp)\n        else:\n            _launch_mse_elemwise(x, y, out)\n        return out.reshape_as(inp)\n\n    # sum or mean -> scalar\n    if n_elements == 0:\n        # Follow a simple convention: return 0 for empty tensors\n        zero = torch.zeros((), device=x.device, dtype=inp.dtype)\n        return zero\n\n    scale = 1.0 if reduction == 2 else (1.0 / float(n_elements))  # sum or mean\n    acc = _launch_mse_reduce(x, y, n_elements, scale)\n    result = acc.to(dtype=inp.dtype)\n    return result\n\n\ndef mse_loss_out(*args, **kwargs):\n    # Expected calling pattern: mse_loss_out(self, target, reduction=Mean, *, out)\n    if len(args) < 2:\n        raise TypeError(\"mse_loss_out requires at least 2 positional arguments: (input, target)\")\n    inp = args[0]\n    target = args[1]\n    reduction = kwargs.get(\"reduction\", args[2] if len(args) > 2 else 1)\n    out = kwargs.get(\"out\", args[3] if len(args) > 3 else None)\n\n    if out is None:\n        raise TypeError(\"mse_loss_out requires an 'out' tensor\")\n\n    reduction = _parse_reduction(reduction)\n\n    if not isinstance(inp, torch.Tensor) or not isinstance(target, torch.Tensor):\n        raise TypeError(\"mse_loss_out expects tensor inputs\")\n\n    if inp.numel() != target.numel():\n        raise ValueError(\"mse_loss_out: input and target must have the same number of elements\")\n\n    if inp.device != target.device:\n        raise ValueError(\"mse_loss_out: input and target must be on the same device\")\n\n    _ensure_supported_dtype(inp, \"mse_loss_out\")\n    _ensure_supported_dtype(target, \"mse_loss_out\")\n\n    x = inp.contiguous()\n    y = target.contiguous()\n    n_elements = x.numel()\n\n    if reduction == 0:  # 'none'\n        # out must have same shape as input\n        if out.numel() != n_elements:\n            raise ValueError(\"mse_loss_out (reduction='none'): 'out' must have the same number of elements as input\")\n        if out.device != x.device:\n            raise ValueError(\"mse_loss_out: 'out' must be on the same device as input\")\n        if out.dtype != inp.dtype:\n            raise TypeError(\"mse_loss_out (reduction='none'): 'out' dtype must match input dtype\")\n\n        if out.is_contiguous():\n            _launch_mse_elemwise(x, y, out)\n        else:\n            tmp = torch.empty_like(x, memory_format=torch.contiguous_format)\n            _launch_mse_elemwise(x, y, tmp)\n            out.copy_(tmp)\n        return out\n\n    # sum or mean\n    if out.device != x.device:\n        raise ValueError(\"mse_loss_out: 'out' must be on the same device as input\")\n    if out.numel() != 1:\n        raise ValueError(\"mse_loss_out (reduction in ['sum','mean']): 'out' must be a scalar tensor\")\n    # out dtype must be a supported float dtype\n    if out.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        raise TypeError(\"mse_loss_out: 'out' dtype must be one of float16, bfloat16, or float32 for Triton kernel\")\n\n    if n_elements == 0:\n        out.fill_(0)\n        return out\n\n    scale = 1.0 if reduction == 2 else (1.0 / float(n_elements))\n    acc = _launch_mse_reduce(x, y, n_elements, scale)\n    out.fill_(acc.to(dtype=out.dtype))\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0147671381492447,
        "meets_threshold": false
      },
      "diag_embed": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0618698585195234,
            "flaggems_speedup_vs_cuda": 1.04830266666667,
            "speedup_vs_flaggems": 1.0129420560342497
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef diag_embed_kernel(\n    x_ptr,\n    out_ptr,\n    B,\n    N,\n    M,\n    abs_offset,\n    is_pos,\n    BLOCK_N: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    n_start = pid_n * BLOCK_N\n    i = n_start + tl.arange(0, BLOCK_N)\n    mask = i < N\n\n    # Compute row/col indices based on offset sign\n    # If offset >= 0: row = i, col = i + abs_offset\n    # If offset < 0:  row = i + abs_offset, col = i\n    row = i + abs_offset * (1 - is_pos)\n    col = i + abs_offset * is_pos\n\n    out_index = pid_b * M * M + row * M + col\n    x_index = pid_b * N + i\n\n    vals = tl.load(x_ptr + x_index, mask=mask, other=0)\n    tl.store(out_ptr + out_index, vals, mask=mask)\n\n\ndef diag_embed(*args, **kwargs):\n    # Parse args following torch.diag_embed signature: (input, offset=0, dim1=-2, dim2=-1)\n    if len(args) > 0:\n        x = args[0]\n        offset = args[1] if len(args) > 1 else kwargs.get(\"offset\", 0)\n        dim1 = args[2] if len(args) > 2 else kwargs.get(\"dim1\", -2)\n        dim2 = args[3] if len(args) > 3 else kwargs.get(\"dim2\", -1)\n    else:\n        x = kwargs.get(\"input\", None)\n        offset = kwargs.get(\"offset\", 0)\n        dim1 = kwargs.get(\"dim1\", -2)\n        dim2 = kwargs.get(\"dim2\", -1)\n\n    if x is None:\n        raise ValueError(\"diag_embed requires a 'input' tensor as the first argument.\")\n    if x.ndim == 0:\n        # For scalar input, treat as vector of length 1\n        x = x.unsqueeze(0)\n\n    if not x.is_cuda:\n        raise AssertionError(\"Input tensor must be on CUDA device.\")\n    if x.numel() == 0:\n        # Handle empty input gracefully\n        # Determine output shape based on offset and dims\n        batch_shape = list(x.shape[:-1])\n        N = x.shape[-1]\n        abs_offset = abs(int(offset))\n        M = N + abs_offset\n        out_rank = len(batch_shape) + 2\n\n        # Normalize dims\n        def _norm(d, r):\n            return d + r if d < 0 else d\n\n        d1 = _norm(int(dim1), out_rank)\n        d2 = _norm(int(dim2), out_rank)\n        if not (0 <= d1 < out_rank and 0 <= d2 < out_rank) or d1 == d2:\n            raise ValueError(\"diag_embed: dim1 and dim2 must be distinct and valid dimensions.\")\n\n        # Build final shape by inserting M at positions d1 and d2 into batch_shape\n        final_shape = []\n        bi = 0\n        for pos in range(out_rank):\n            if pos == d1 or pos == d2:\n                final_shape.append(M)\n            else:\n                final_shape.append(batch_shape[bi])\n                bi += 1\n        return torch.zeros(final_shape, device=x.device, dtype=x.dtype)\n\n    # Compute sizes\n    N = x.shape[-1]\n    abs_offset = abs(int(offset))\n    M = N + abs_offset\n\n    # Flatten batch dims\n    batch_shape = list(x.shape[:-1])\n    B = 1\n    for s in batch_shape:\n        B *= s\n\n    x_flat = x.contiguous().view(B, N)\n    out = torch.zeros((B, M, M), device=x.device, dtype=x.dtype)\n\n    # Launch Triton kernel\n    BLOCK_N = 256\n    grid = (B, triton.cdiv(N, BLOCK_N))\n    is_pos = 1 if int(offset) >= 0 else 0\n    diag_embed_kernel[grid](\n        x_flat,\n        out,\n        B,\n        N,\n        M,\n        abs_offset,\n        is_pos,\n        BLOCK_N=BLOCK_N,\n    )\n\n    # Reshape to batch dims and place M dims at positions dim1, dim2\n    out_view = out.view(*batch_shape, M, M)\n    out_rank = len(batch_shape) + 2\n\n    def _norm(d, r):\n        return d + r if d < 0 else d\n\n    d1 = _norm(int(dim1), out_rank)\n    d2 = _norm(int(dim2), out_rank)\n    if not (0 <= d1 < out_rank and 0 <= d2 < out_rank) or d1 == d2:\n        raise ValueError(\"diag_embed: dim1 and dim2 must be distinct and valid dimensions.\")\n\n    # Current order: [0..len(batch_shape)-1, len(batch_shape), len(batch_shape)+1]\n    base_pos1 = len(batch_shape)\n    base_pos2 = len(batch_shape) + 1\n\n    new_order = [None] * out_rank\n    new_order[d1] = base_pos1\n    new_order[d2] = base_pos2\n    remaining_old = [i for i in range(out_rank) if i not in (base_pos1, base_pos2)]\n    remaining_new_positions = [i for i in range(out_rank) if i not in (d1, d2)]\n    for idx, pos in enumerate(remaining_new_positions):\n        new_order[pos] = remaining_old[idx]\n\n    return out_view.permute(new_order)",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0129420560342497,
        "meets_threshold": false
      },
      "aten::elu": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0558887814367959,
            "flaggems_speedup_vs_cuda": 1.04276166666667,
            "speedup_vs_flaggems": 1.0125887968360867
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef elu_kernel(x_ptr, out_ptr, n_elements, alpha, scale, input_scale, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    x32 = x.to(tl.float32)\n\n    pos = x32 > 0.0\n    neg = alpha * (tl.exp(input_scale * x32) - 1.0)\n    y32 = tl.where(pos, x32, neg)\n    y32 = scale * y32\n\n    y = y32.to(x.dtype)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _parse_elu_args(args, kwargs, expect_out: bool = False):\n    x = None\n    if len(args) > 0 and isinstance(args[0], torch.Tensor):\n        x = args[0]\n        arg_idx = 1\n    else:\n        x = kwargs.get(\"input\", kwargs.get(\"self\", kwargs.get(\"x\", None)))\n        arg_idx = 0\n\n    if x is None:\n        raise ValueError(\"elu expects a Tensor as the first argument (input/self/x).\")\n\n    def _get_scalar(name, default, idx):\n        if name in kwargs:\n            return float(kwargs[name])\n        elif len(args) > idx:\n            return float(args[idx])\n        else:\n            return float(default)\n\n    alpha = _get_scalar(\"alpha\", 1.0, arg_idx + 0)\n    scale = _get_scalar(\"scale\", 1.0, arg_idx + 1)\n    input_scale = _get_scalar(\"input_scale\", 1.0, arg_idx + 2)\n\n    out = None\n    if expect_out:\n        if \"out\" in kwargs and isinstance(kwargs[\"out\"], torch.Tensor):\n            out = kwargs[\"out\"]\n        elif len(args) > arg_idx + 3 and isinstance(args[arg_idx + 3], torch.Tensor):\n            out = args[arg_idx + 3]\n        elif len(args) > arg_idx + 4 and isinstance(args[arg_idx + 4], torch.Tensor):\n            out = args[arg_idx + 4]\n        else:\n            raise ValueError(\"elu_out expects an 'out' tensor argument.\")\n\n    return x, alpha, scale, input_scale, out\n\n\ndef _launch_elu_kernel(x: torch.Tensor, out: torch.Tensor, alpha: float, scale: float, input_scale: float):\n    if not x.is_cuda or not out.is_cuda:\n        raise RuntimeError(\"elu Triton kernel requires CUDA tensors.\")\n    if x.numel() != out.numel():\n        raise ValueError(\"Input and output must have the same number of elements.\")\n    if x.dtype != out.dtype:\n        raise ValueError(\"Input and output must have the same dtype.\")\n    if not x.is_contiguous() or not out.is_contiguous():\n        raise ValueError(\"Input and output must be contiguous tensors.\")\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    elu_kernel[grid](\n        x,\n        out,\n        n_elements,\n        float(alpha),\n        float(scale),\n        float(input_scale),\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n\ndef elu(*args, **kwargs):\n    x, alpha, scale, input_scale, _ = _parse_elu_args(args, kwargs, expect_out=False)\n    out = torch.empty_like(x)\n    _launch_elu_kernel(x.contiguous(), out, alpha, scale, input_scale)\n    return out\n\n\ndef elu_out(*args, **kwargs):\n    x, alpha, scale, input_scale, out = _parse_elu_args(args, kwargs, expect_out=True)\n    if out is None:\n        raise ValueError(\"elu_out requires an 'out' tensor.\")\n    _launch_elu_kernel(x.contiguous(), out, alpha, scale, input_scale)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0125887968360867,
        "meets_threshold": false
      },
      "aten::threshold": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9969456103702411,
            "flaggems_speedup_vs_cuda": 0.985151333333333,
            "speedup_vs_flaggems": 1.0119720459566361
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef threshold_kernel(\n    x_ptr,         # *Pointer* to input tensor\n    y_ptr,         # *Pointer* to output tensor\n    n_elements,    # Number of elements\n    threshold,     # Scalar threshold\n    value,         # Scalar value to use when x <= threshold\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.where(x > threshold, x, value)\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n\ndef _coerce_scalars_for_dtype(dtype, threshold, value):\n    if dtype.is_complex:\n        raise TypeError(\"aten.threshold does not support complex dtypes.\")\n    if dtype == torch.bool:\n        raise TypeError(\"aten.threshold does not support bool dtype.\")\n    if dtype.is_floating_point:\n        thr = float(threshold)\n        val = float(value)\n    else:\n        thr = int(threshold)\n        val = int(value)\n    return thr, val\n\n\ndef threshold(input: torch.Tensor, threshold, value):\n    if input.device.type != \"cuda\":\n        raise RuntimeError(\"This Triton implementation requires CUDA tensors.\")\n    x = input.contiguous()\n    n_elements = x.numel()\n    out = torch.empty_like(x)\n\n    if n_elements == 0:\n        return out\n\n    thr_scalar, val_scalar = _coerce_scalars_for_dtype(x.dtype, threshold, value)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    threshold_kernel[grid](\n        x, out, n_elements,\n        thr_scalar, val_scalar,\n        BLOCK_SIZE=1024,\n    )\n    return out\n\n\ndef threshold_out(input: torch.Tensor, threshold, value, out: torch.Tensor):\n    if input.device.type != \"cuda\" or out.device.type != \"cuda\":\n        raise RuntimeError(\"This Triton implementation requires CUDA tensors.\")\n    if out.shape != input.shape:\n        raise RuntimeError(f\"out shape {out.shape} must match input shape {input.shape}\")\n    if out.dtype != input.dtype:\n        raise RuntimeError(f\"out dtype {out.dtype} must match input dtype {input.dtype}\")\n\n    x = input.contiguous()\n    n_elements = x.numel()\n\n    # Prepare output (contiguous temp if needed)\n    y = out if out.is_contiguous() else torch.empty_like(x)\n\n    if n_elements == 0:\n        if y is not out:\n            out.copy_(y)\n        return out\n\n    thr_scalar, val_scalar = _coerce_scalars_for_dtype(x.dtype, threshold, value)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    threshold_kernel[grid](\n        x, y, n_elements,\n        thr_scalar, val_scalar,\n        BLOCK_SIZE=1024,\n    )\n\n    if y is not out:\n        out.copy_(y)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0119720459566361,
        "meets_threshold": false
      },
      "ones_like": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0061817375257227,
            "flaggems_speedup_vs_cuda": 0.997718,
            "speedup_vs_flaggems": 1.008483095950682
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ones_like(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr, DTYPE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    ones_val = tl.cast(offsets >= offsets, DTYPE)\n    tl.store(output_ptr + offsets, ones_val, mask=mask)\n\n\n_triton_ones_like = ones_like\n\n\ndef ones_like(*args, **kwargs):\n    if len(args) == 0 or not isinstance(args[0], torch.Tensor):\n        return torch.ones_like(*args, **kwargs)\n\n    x = args[0]\n    dtype = kwargs.get('dtype', None)\n    device = kwargs.get('device', x.device)\n    requires_grad = kwargs.get('requires_grad', False)\n\n    # Triton works on CUDA tensors; fallback otherwise\n    if device.type != 'cuda':\n        return torch.ones_like(*args, **kwargs)\n\n    out_dtype = dtype if dtype is not None else x.dtype\n\n    TORCH_TO_TL = {\n        torch.float16: tl.float16,\n        torch.bfloat16: tl.bfloat16,\n        torch.float32: tl.float32,\n        torch.int32: tl.int32,\n        torch.int64: tl.int64,\n        torch.bool: tl.int1,\n    }\n\n    tl_dtype = TORCH_TO_TL.get(out_dtype, None)\n    if tl_dtype is None:\n        return torch.ones_like(*args, **kwargs)\n\n    # Preserve format if possible; fallback to PyTorch if non-contiguous\n    output = torch.empty_like(x, dtype=out_dtype, device=device, requires_grad=requires_grad)\n    if not output.is_contiguous():\n        return torch.ones_like(*args, **kwargs)\n\n    n_elements = output.numel()\n    if n_elements == 0:\n        return output\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _triton_ones_like[grid](output, n_elements, BLOCK_SIZE=BLOCK_SIZE, DTYPE=tl_dtype)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.008483095950682,
        "meets_threshold": false
      },
      "le": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.01335417165957,
            "flaggems_speedup_vs_cuda": 1.00514333333333,
            "speedup_vs_flaggems": 1.0081688233448365
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef le_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0)\n    cmp = x <= y\n    tl.store(out_ptr + offsets, cmp, mask=mask)\n\n\ndef _prepare_common_dtype_tensors(x: torch.Tensor, y: torch.Tensor):\n    # Determine common dtype according to PyTorch's type promotion\n    common_dtype = torch.result_type(x, y)\n    # For boolean comparisons, operate on uint8 (0/1) to be explicit\n    if common_dtype == torch.bool:\n        x_cast = x.to(torch.uint8)\n        y_cast = y.to(torch.uint8)\n        compute_dtype = torch.uint8\n    else:\n        x_cast = x.to(common_dtype)\n        y_cast = y.to(common_dtype)\n        compute_dtype = common_dtype\n    return x_cast, y_cast, compute_dtype\n\n\ndef _launch_le_kernel(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and y.is_cuda and out.is_cuda, \"All tensors must be on CUDA device\"\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    # Ensure contiguous buffers for kernel\n    x_c = x.contiguous()\n    y_c = y.contiguous()\n    out_c = out.contiguous()\n    le_kernel[grid](x_c, y_c, out_c, n_elements, BLOCK_SIZE=1024)\n    if out.data_ptr() != out_c.data_ptr():\n        out.copy_(out_c)\n    return out\n\n\ndef le_Tensor(*args, **kwargs):\n    # Expected signature: le_Tensor(input, other, *, out=None)\n    if len(args) >= 2:\n        input_tensor = args[0]\n        other_tensor = args[1]\n    else:\n        input_tensor = kwargs.get('input', None)\n        other_tensor = kwargs.get('other', None)\n    out = kwargs.get('out', None)\n\n    assert isinstance(input_tensor, torch.Tensor) and isinstance(other_tensor, torch.Tensor), \"input and other must be Tensors\"\n    assert input_tensor.is_cuda and other_tensor.is_cuda, \"Tensors must be on CUDA device\"\n\n    # Broadcast to common shape\n    x_b, y_b = torch.broadcast_tensors(input_tensor, other_tensor)\n    # Determine common compute dtype and cast\n    x_cast, y_cast, _ = _prepare_common_dtype_tensors(x_b, y_b)\n\n    # Prepare output\n    if out is None:\n        out_tensor = torch.empty(x_cast.shape, dtype=torch.bool, device=x_cast.device)\n    else:\n        assert isinstance(out, torch.Tensor), \"out must be a Tensor\"\n        assert out.device == x_cast.device, \"out must be on the same device\"\n        assert out.dtype == torch.bool, \"out must have dtype torch.bool\"\n        assert tuple(out.shape) == tuple(x_cast.shape), \"out must have the broadcasted shape\"\n        out_tensor = out\n\n    return _launch_le_kernel(x_cast, y_cast, out_tensor)\n\n\ndef le_Scalar(*args, **kwargs):\n    # Expected signature: le_Scalar(input, other, *, out=None)\n    if len(args) >= 2:\n        input_tensor = args[0]\n        other_scalar = args[1]\n    else:\n        input_tensor = kwargs.get('input', None)\n        other_scalar = kwargs.get('other', None)\n    out = kwargs.get('out', None)\n\n    assert isinstance(input_tensor, torch.Tensor), \"input must be a Tensor\"\n    assert input_tensor.is_cuda, \"input must be on CUDA device\"\n    # Create a tensor for the scalar to determine type promotion\n    scalar_tensor = torch.tensor(other_scalar, device=input_tensor.device)\n    common_dtype = torch.result_type(input_tensor, scalar_tensor)\n\n    # Handle boolean specifically to compare as 0/1\n    if common_dtype == torch.bool:\n        x_cast = input_tensor.to(torch.uint8)\n        y_full = torch.full(input_tensor.shape, int(bool(other_scalar)), dtype=torch.uint8, device=input_tensor.device)\n    else:\n        x_cast = input_tensor.to(common_dtype)\n        y_full = torch.full(input_tensor.shape, other_scalar, dtype=common_dtype, device=input_tensor.device)\n\n    # Prepare output\n    if out is None:\n        out_tensor = torch.empty(x_cast.shape, dtype=torch.bool, device=x_cast.device)\n    else:\n        assert isinstance(out, torch.Tensor), \"out must be a Tensor\"\n        assert out.device == x_cast.device, \"out must be on the same device\"\n        assert out.dtype == torch.bool, \"out must have dtype torch.bool\"\n        assert tuple(out.shape) == tuple(x_cast.shape), \"out must have the same shape as input\"\n        out_tensor = out\n\n    return _launch_le_kernel(x_cast, y_full, out_tensor)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0081688233448365,
        "meets_threshold": false
      },
      "maximum": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.011044669709273,
            "flaggems_speedup_vs_cuda": 1.00460633333333,
            "speedup_vs_flaggems": 1.006408815236691
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef maximum(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    z = tl.maximum(x, y)\n    tl.store(out_ptr + offsets, z, mask=mask)\n\n\n_maximum_kernel = maximum\n\n\ndef maximum(*args, **kwargs):\n    # Parse inputs similar to torch.maximum(input, other, *, out=None)\n    out = kwargs.pop('out', None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get('input', kwargs.get('x', None))\n        y = kwargs.get('other', kwargs.get('y', None))\n    if x is None or y is None:\n        raise TypeError(\"maximum() missing required positional arguments: 'input' and 'other'\")\n\n    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n        raise TypeError(\"maximum() expects both 'input' and 'other' to be torch.Tensor\")\n\n    # Determine target device\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor if provided\")\n        if not out.is_cuda:\n            raise RuntimeError(\"Triton maximum requires 'out' to be on a CUDA device\")\n        device = out.device\n    else:\n        device = None\n        if x.is_cuda:\n            device = x.device\n        if y.is_cuda:\n            device = y.device if device is None else device\n        if device is None:\n            raise RuntimeError(\"Triton maximum requires at least one input on a CUDA device\")\n\n    # Compute result dtype via type promotion\n    dtype = torch.result_type(x, y)\n\n    # Broadcast inputs to a common shape\n    xb, yb = torch.broadcast_tensors(x, y)\n\n    # Prepare contiguous tensors on the correct device and dtype\n    xb = xb.to(device=device, dtype=dtype).contiguous()\n    yb = yb.to(device=device, dtype=dtype).contiguous()\n\n    # Prepare output\n    if out is not None:\n        if out.device != device:\n            raise RuntimeError(\"out tensor must be on the same CUDA device as inputs\")\n        if out.shape != xb.shape:\n            raise RuntimeError(f\"out tensor has shape {out.shape}, but expected {xb.shape}\")\n        if out.dtype != dtype:\n            raise RuntimeError(f\"out tensor has dtype {out.dtype}, but expected {dtype}\")\n        out_contig = out.contiguous()\n    else:\n        out_contig = torch.empty_like(xb, device=device, dtype=dtype)\n\n    n_elements = out_contig.numel()\n    if n_elements == 0:\n        # Handle empty tensors early\n        if out is not None and out.data_ptr() != out_contig.data_ptr():\n            out.copy_(out_contig)\n        return out if out is not None else out_contig\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _maximum_kernel[grid](\n        xb, yb, out_contig, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    if out is not None:\n        if out.data_ptr() != out_contig.data_ptr():\n            out.copy_(out_contig)\n        return out\n    return out_contig\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.006408815236691,
        "meets_threshold": false
      },
      "zeros": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.013156314807764,
            "flaggems_speedup_vs_cuda": 1.00899066666667,
            "speedup_vs_flaggems": 1.0041285299048959
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef zeros(output_ptr,  # *Pointer* to output tensor.\n          n_elements,  # Number of elements to write.\n          BLOCK_SIZE: tl.constexpr  # Number of elements per program.\n          ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    tl.store(output_ptr + offsets, 0, mask=mask)\n\n\n# Preserve kernel reference before defining the wrapper with the same name.\nzeros_kernel = zeros\n\n\ndef zeros(*args, **kwargs):\n    # Determine the size\n    size_kw = kwargs.get('size', None)\n    if size_kw is not None:\n        size = tuple(size_kw) if isinstance(size_kw, (list, tuple, torch.Size)) else (size_kw,)\n    elif len(args) == 1 and isinstance(args[0], (list, tuple, torch.Size)):\n        size = tuple(args[0])\n    elif len(args) >= 1:\n        size = tuple(args)\n    else:\n        raise TypeError(\"zeros() missing required argument 'size'\")\n\n    # Determine dtype and device\n    dtype = kwargs.get('dtype', None)\n    if dtype is None:\n        dtype = torch.get_default_dtype()\n    device = kwargs.get('device', None)\n    pin_memory = kwargs.get('pin_memory', False)\n    layout = kwargs.get('layout', torch.strided)\n    memory_format = kwargs.get('memory_format', torch.contiguous_format)\n    requires_grad = kwargs.get('requires_grad', False)\n\n    # Normalize device\n    if isinstance(device, str):\n        device = torch.device(device)\n    elif device is not None and not isinstance(device, torch.device):\n        device = torch.device(device)\n\n    # Allocate output tensor\n    out = torch.empty(size, dtype=dtype, device=device, layout=layout, pin_memory=pin_memory, memory_format=memory_format)\n    out.requires_grad_(requires_grad)\n\n    # Launch Triton kernel on CUDA tensors; otherwise, fall back to zero_()\n    n_elements = out.numel()\n    if out.device.type == 'cuda' and n_elements > 0:\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        zeros_kernel[grid](out, n_elements, BLOCK_SIZE=1024)\n    else:\n        out.zero_()\n\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0041285299048959,
        "meets_threshold": false
      },
      "rand": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.3297204721090023,
            "flaggems_speedup_vs_cuda": 1.325573,
            "speedup_vs_flaggems": 1.0031288145647221
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef rand(output_ptr, n_elements, seed, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Prepare 64-bit state per element using SplitMix64\n    state = offsets.to(tl.uint64) + tl.full(offsets.shape, seed, tl.uint64)\n\n    z = state + tl.full(state.shape, 0x9E3779B97F4A7C15, tl.uint64)\n    z ^= z >> 30\n    z = z * tl.full(state.shape, 0xBF58476D1CE4E5B9, tl.uint64)\n    z ^= z >> 27\n    z = z * tl.full(state.shape, 0x94D049BB133111EB, tl.uint64)\n    z ^= z >> 31\n\n    # Convert to uniform float32 in [0, 1)\n    u32 = (z >> 32).to(tl.uint32)\n    randf = u32.to(tl.float32) * tl.full(state.shape, 1.0 / 4294967296.0, tl.float32)\n\n    tl.store(output_ptr + offsets, randf, mask=mask)\n\n\n_triton_rand_kernel = rand\n\n\ndef rand(*args, **kwargs):\n    out = kwargs.get(\"out\", None)\n    requires_grad = kwargs.get(\"requires_grad\", False)\n    dtype = kwargs.get(\"dtype\", torch.float32)\n    layout = kwargs.get(\"layout\", torch.strided)\n    device = kwargs.get(\"device\", torch.device(\"cuda\"))\n    seed = kwargs.get(\"seed\", 0)\n\n    if out is None:\n        if len(args) == 1 and isinstance(args[0], (tuple, list)):\n            shape = tuple(args[0])\n        else:\n            shape = tuple(args)\n        if not shape:\n            shape = (1,)\n        out = torch.empty(shape, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)\n    else:\n        if not out.is_cuda:\n            raise ValueError(\"Output tensor must be on CUDA device for Triton kernel execution.\")\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _triton_rand_kernel[grid](out, n_elements, int(seed), BLOCK_SIZE=1024)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0031288145647221,
        "meets_threshold": false
      },
      "aten::abs_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.0040229705064405,
            "flaggems_speedup_vs_cuda": 1.00268733333333,
            "speedup_vs_flaggems": 1.0013320574906142
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef abs_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.abs(x)\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n\n# Alias the kernel before defining the Python wrapper with the same name\nabs__kernel = abs_\n\n\ndef abs_(*args, **kwargs):\n    # Extract input tensor\n    x = args[0] if len(args) > 0 else kwargs.get('input', None)\n    if x is None:\n        raise ValueError(\"abs_ expects a tensor as the first positional argument or 'input' keyword argument.\")\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"abs_ expects a torch.Tensor as input.\")\n\n    # Handle trivial/unsupported cases\n    if x.numel() == 0:\n        return x\n    if x.dtype == torch.bool:\n        # abs on boolean is identity; nothing to do\n        return x\n    if x.is_complex():\n        raise TypeError(\"abs_ does not support complex tensors in-place.\")\n\n    # Ensure tensor is on CUDA and contiguous\n    assert x.is_cuda, \"abs_ expects a CUDA tensor.\"\n    assert x.is_contiguous(), \"abs_ expects a contiguous tensor.\"\n\n    # Launch kernel\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    abs__kernel[grid](x, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.0013320574906142,
        "meets_threshold": false
      },
      "neg": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0055861053682538,
            "flaggems_speedup_vs_cuda": 1.00481333333333,
            "speedup_vs_flaggems": 1.000769070243485
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef neg(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    out = -x\n    tl.store(output_ptr + offsets, out, mask=mask)\n\n\nneg_kernel = neg\n\n\ndef neg(*args, **kwargs):\n    # Extract input tensor\n    if len(args) > 0:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    if x is None:\n        raise ValueError(\"neg expects a 'input' tensor as positional or keyword argument.\")\n    out = kwargs.get('out', None)\n\n    # CPU fallback\n    if not isinstance(x, torch.Tensor) or x.device.type != 'cuda':\n        return torch.neg(*args, **kwargs)\n\n    # Prepare contiguous tensors\n    x_contig = x.contiguous()\n    if out is None:\n        out_contig = torch.empty_like(x_contig)\n        final_out = out_contig\n    else:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor if provided.\")\n        if out.device != x.device:\n            raise ValueError(\"out must be on the same device as input.\")\n        if out.shape != x.shape:\n            raise ValueError(\"out must have the same shape as input.\")\n        if out.dtype != x.dtype:\n            raise ValueError(\"out must have the same dtype as input.\")\n        if out.is_contiguous():\n            out_contig = out\n            final_out = out\n        else:\n            out_contig = torch.empty_like(x_contig)\n            final_out = out  # will copy back after kernel\n\n    n_elements = x_contig.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    neg_kernel[grid](x_contig, out_contig, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    if out is not None and not out.is_contiguous():\n        out.copy_(out_contig)\n        return out\n    return final_out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 1.000769070243485,
        "meets_threshold": false
      },
      "gt": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.001732627498651,
            "flaggems_speedup_vs_cuda": 1.002599,
            "speedup_vs_flaggems": 0.9991358733637786
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gt_tensor_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    res = x > y\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\n@triton.jit\ndef gt_scalar_kernel(x_ptr, y_scalar, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Create a vector of the scalar with the same dtype as x for a well-typed comparison\n    y = tl.full([BLOCK_SIZE], y_scalar, x.dtype)\n    res = x > y\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\ndef _common_checks_and_grid(n_elements):\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    return BLOCK_SIZE, grid\n\n\ndef gt_Tensor(*args, **kwargs):\n    # Expected signature: (input: Tensor, other: Tensor, *, out=None)\n    if \"input\" in kwargs:\n        x = kwargs[\"input\"]\n        y = kwargs[\"other\"]\n    else:\n        x, y = args[0], args[1]\n\n    assert isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor), \"gt_Tensor expects Tensor, Tensor\"\n    # Ensure both on CUDA\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    if y.device != x.device:\n        y = y.to(device=x.device)\n\n    # Broadcast\n    x_b, y_b = torch.broadcast_tensors(x, y)\n\n    # Type promotion to match PyTorch semantics\n    promoted_dtype = torch.result_type(x_b, y_b)\n    x_c = x_b.to(dtype=promoted_dtype).contiguous()\n    y_c = y_b.to(dtype=promoted_dtype).contiguous()\n\n    # Output allocation\n    out = torch.empty(x_c.shape, dtype=torch.bool, device=x_c.device)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    BLOCK_SIZE, grid = _common_checks_and_grid(n_elements)\n    gt_tensor_kernel[grid](x_c, y_c, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    return out\n\n\ndef gt_Scalar(*args, **kwargs):\n    # Expected signature: (input: Tensor, other: Scalar, *, out=None)\n    if \"input\" in kwargs:\n        x = kwargs[\"input\"]\n        scalar = kwargs[\"other\"]\n    else:\n        x, scalar = args[0], args[1]\n\n    assert isinstance(x, torch.Tensor), \"gt_Scalar expects (Tensor, Scalar)\"\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n\n    # Type promotion with scalar to match PyTorch semantics\n    # Create a 0-dim tensor on CPU just for dtype inference, no need to move to CUDA\n    scalar_tensor = torch.tensor(scalar)\n    promoted_dtype = torch.result_type(x, scalar_tensor)\n\n    x_c = x.to(dtype=promoted_dtype).contiguous()\n\n    out = torch.empty(x_c.shape, dtype=torch.bool, device=x_c.device)\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    BLOCK_SIZE, grid = _common_checks_and_grid(n_elements)\n    # Pass scalar directly; Triton will broadcast, we explicitly cast inside kernel\n    gt_scalar_kernel[grid](x_c, scalar, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9991358733637786,
        "meets_threshold": false
      },
      "relu": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.013502898949638,
            "flaggems_speedup_vs_cuda": 1.01545333333333,
            "speedup_vs_flaggems": 0.9980792476427356
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef relu(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    zero = tl.zeros_like(x)\n    y = tl.maximum(x, zero)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\n# Keep a handle to the Triton kernel before defining the Python wrapper with the same name.\nrelu_kernel = relu\n\n\ndef relu(*args, **kwargs):\n    # Extract input tensor and inplace flag to mimic torch.relu(*args, **kwargs)\n    x = None\n    if len(args) > 0:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    else:\n        raise ValueError(\"relu expects an input tensor as the first argument or as 'input' keyword.\")\n\n    inplace = kwargs.get('inplace', False)\n\n    if not torch.is_tensor(x):\n        raise TypeError(\"relu expects a torch.Tensor as input.\")\n\n    # Fallback for unsupported cases\n    supported_dtypes = {torch.float16, torch.float32, torch.bfloat16}\n    if (not x.is_cuda) or (not x.is_contiguous()) or (x.dtype not in supported_dtypes):\n        return torch.relu(x, inplace=inplace)\n\n    n_elements = x.numel()\n    if inplace:\n        out = x\n    else:\n        out = torch.empty_like(x, device=x.device)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, out, n_elements, BLOCK_SIZE=1024)\n\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9980792476427356,
        "meets_threshold": false
      },
      "ge": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.987427175249555,
            "flaggems_speedup_vs_cuda": 0.989791333333333,
            "speedup_vs_flaggems": 0.9976114580880233
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ge_kernel(\n    x_ptr,           # *Pointer* to first input vector.\n    y_ptr,           # *Pointer* to second input vector (unused if y_is_scalar=True).\n    out_ptr,         # *Pointer* to output vector (torch.bool).\n    n_elements,      # Number of elements to process.\n    y_is_scalar: tl.constexpr,  # Compile-time flag: compare against scalar if True.\n    y_scalar,        # Scalar value (used if y_is_scalar=True).\n    BLOCK_SIZE: tl.constexpr,   # Number of elements per program.\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if y_is_scalar:\n        # Broadcast scalar to x's dtype/shape to avoid dtype mismatch issues\n        y_val = y_scalar + tl.zeros_like(x)\n        comp = x >= y_val\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask)\n        comp = x >= y\n\n    tl.store(out_ptr + offsets, comp, mask=mask)\n\n\ndef ge_Tensor(*args, **kwargs):\n    # Parse inputs: torch.ge(input, other, *, out=None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get('input', None)\n        y = kwargs.get('other', kwargs.get('y', None))\n    out = kwargs.get('out', None)\n\n    assert isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor), \"Inputs must be torch.Tensors for ge_Tensor\"\n    assert x.device == y.device, \"Input tensors must be on the same device\"\n    assert x.numel() == y.numel(), \"ge_Tensor requires tensors with the same number of elements\"\n    # Ensure contiguous\n    x = x.contiguous()\n    y = y.contiguous()\n\n    if out is None:\n        out = torch.empty_like(x, dtype=torch.bool)\n    else:\n        assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n        assert out.dtype == torch.bool, \"Output tensor dtype must be torch.bool\"\n        assert out.numel() == x.numel(), \"Output tensor must have the same number of elements as input\"\n        assert out.device == x.device, \"Output tensor must be on the same device as input\"\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ge_kernel[grid](\n        x, y, out, n_elements,\n        y_is_scalar=False,\n        y_scalar=0,  # unused in tensor-tensor path\n        BLOCK_SIZE=1024,\n    )\n    return out\n\n\ndef ge_Scalar(*args, **kwargs):\n    # Parse inputs: torch.ge(input, other, *, out=None) where other is a Python scalar\n    if len(args) >= 2:\n        x, other = args[0], args[1]\n    else:\n        x = kwargs.get('input', None)\n        other = kwargs.get('other', kwargs.get('y', None))\n    out = kwargs.get('out', None)\n\n    assert isinstance(x, torch.Tensor), \"Input must be a torch.Tensor for ge_Scalar\"\n    assert isinstance(other, (int, float, bool)), \"Other must be a scalar (int/float/bool) for ge_Scalar\"\n\n    # Ensure contiguous\n    x = x.contiguous()\n\n    if out is None:\n        out = torch.empty_like(x, dtype=torch.bool)\n    else:\n        assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n        assert out.dtype == torch.bool, \"Output tensor dtype must be torch.bool\"\n        assert out.numel() == x.numel(), \"Output tensor must have the same number of elements as input\"\n        assert out.device == x.device, \"Output tensor must be on the same device as input\"\n        out = out.contiguous()\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    # y_ptr is not used when y_is_scalar=True, but must be passed; we pass x to satisfy signature\n    ge_kernel[grid](\n        x, x, out, n_elements,\n        y_is_scalar=True,\n        y_scalar=other,\n        BLOCK_SIZE=1024,\n    )\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9976114580880233,
        "meets_threshold": false
      },
      "aten::abs": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9940155070438491,
            "flaggems_speedup_vs_cuda": 0.997223,
            "speedup_vs_flaggems": 0.9967835750317122
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _abs_kernel_real(in_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    # For both integer and floating types: abs = x if x >= 0 else -x\n    y = tl.where(x >= 0, x, -x)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\n@triton.jit\ndef _abs_kernel_complex(rr_ptr, out_ptr, n_complex, BLOCK_SIZE: tl.constexpr):\n    # rr_ptr points to interleaved real/imag scalars: [re0, im0, re1, im1, ...]\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # complex element indices\n    mask = offsets < n_complex\n    base = offsets * 2\n    re = tl.load(rr_ptr + base, mask=mask)\n    im = tl.load(rr_ptr + base + 1, mask=mask)\n    mag = tl.sqrt(re * re + im * im)\n    tl.store(out_ptr + offsets, mag, mask=mask)\n\n\ndef _ensure_cuda_tensor(x: torch.Tensor):\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"Input must be a torch.Tensor\")\n    if x.device.type != \"cuda\":\n        raise ValueError(\"Tensor must be on CUDA device\")\n    return x\n\n\ndef _complex_abs_out_dtype(dtype: torch.dtype) -> torch.dtype:\n    if dtype == torch.complex64:\n        return torch.float32\n    if dtype == torch.complex128:\n        return torch.float64\n    # Optional support if complex32 exists\n    if hasattr(torch, \"complex32\") and dtype == getattr(torch, \"complex32\"):\n        return torch.float16\n    raise NotImplementedError(f\"Unsupported complex dtype for abs: {dtype}\")\n\n\ndef _launch_abs_real(inp: torch.Tensor, out: torch.Tensor):\n    n_elements = out.numel()\n    if n_elements == 0:\n        return\n    BLOCK = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _abs_kernel_real[grid](inp, out, n_elements, BLOCK_SIZE=BLOCK)\n\n\ndef _launch_abs_complex(inp: torch.Tensor, out: torch.Tensor):\n    # inp is complex contiguous tensor, out is real contiguous with matching shape\n    n_complex = inp.numel()\n    if n_complex == 0:\n        return\n    # Create a real view of the interleaved storage\n    if inp.dtype == torch.complex64:\n        rr = inp.view(torch.float32)\n    elif inp.dtype == torch.complex128:\n        rr = inp.view(torch.float64)\n    elif hasattr(torch, \"complex32\") and inp.dtype == getattr(torch, \"complex32\"):\n        rr = inp.view(torch.float16)\n    else:\n        raise NotImplementedError(f\"Unsupported complex dtype for abs: {inp.dtype}\")\n    BLOCK = 1024\n    grid = lambda meta: (triton.cdiv(n_complex, meta[\"BLOCK_SIZE\"]),)\n    _abs_kernel_complex[grid](rr, out, n_complex, BLOCK_SIZE=BLOCK)\n\n\ndef abs(x: torch.Tensor):\n    x = _ensure_cuda_tensor(x)\n    if x.is_complex():\n        out_dtype = _complex_abs_out_dtype(x.dtype)\n        out = torch.empty(x.shape, dtype=out_dtype, device=x.device)\n        x_c = x.contiguous()\n        out_c = out  # already contiguous\n        _launch_abs_complex(x_c, out_c)\n        return out\n    else:\n        out = torch.empty_like(x)\n        x_c = x.contiguous()\n        out_c = out  # contiguous\n        _launch_abs_real(x_c, out_c)\n        return out\n\n\ndef abs_out(x: torch.Tensor, out: torch.Tensor):\n    x = _ensure_cuda_tensor(x)\n    out = _ensure_cuda_tensor(out)\n    if x.is_complex():\n        expected_dtype = _complex_abs_out_dtype(x.dtype)\n        if out.dtype != expected_dtype:\n            raise TypeError(f\"abs_out: expected out.dtype={expected_dtype}, got {out.dtype}\")\n        if out.shape != x.shape:\n            raise ValueError(f\"abs_out: expected out.shape={x.shape}, got {out.shape}\")\n        x_c = x.contiguous()\n        if out.is_contiguous():\n            out_c = out\n            _launch_abs_complex(x_c, out_c)\n        else:\n            tmp = torch.empty_like(out, memory_format=torch.contiguous_format)\n            _launch_abs_complex(x_c, tmp)\n            out.copy_(tmp)\n        return out\n    else:\n        if out.dtype != x.dtype:\n            raise TypeError(f\"abs_out: expected out.dtype={x.dtype}, got {out.dtype}\")\n        if out.shape != x.shape:\n            raise ValueError(f\"abs_out: expected out.shape={x.shape}, got {out.shape}\")\n        x_c = x.contiguous()\n        if out.is_contiguous():\n            out_c = out\n            _launch_abs_real(x_c, out_c)\n        else:\n            tmp = torch.empty_like(out, memory_format=torch.contiguous_format)\n            _launch_abs_real(x_c, tmp)\n            out.copy_(tmp)\n        return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9967835750317122,
        "meets_threshold": false
      },
      "aten::lerp_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9811081549903279,
            "flaggems_speedup_vs_cuda": 0.985344,
            "speedup_vs_flaggems": 0.9957011510602671
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _lerp_inplace_scalar_kernel(x_ptr, end_ptr, n_elements, weight, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x_raw = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y_raw = tl.load(end_ptr + offsets, mask=mask, other=0)\n\n    x32 = x_raw.to(tl.float32)\n    y32 = y_raw.to(tl.float32)\n    out32 = x32 + weight * (y32 - x32)\n\n    out = out32.to(x_raw.dtype)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\n@triton.jit\ndef _lerp_inplace_tensor_kernel(x_ptr, end_ptr, w_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x_raw = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y_raw = tl.load(end_ptr + offsets, mask=mask, other=0)\n    w_raw = tl.load(w_ptr + offsets, mask=mask, other=0)\n\n    x32 = x_raw.to(tl.float32)\n    y32 = y_raw.to(tl.float32)\n    w32 = w_raw.to(tl.float32)\n\n    out32 = x32 + w32 * (y32 - x32)\n    out = out32.to(x_raw.dtype)\n    tl.store(x_ptr + offsets, out, mask=mask)\n\n\ndef _ensure_contig_same_dtype_device(t: torch.Tensor, ref: torch.Tensor):\n    t = t.to(device=ref.device, dtype=ref.dtype)\n    if t.shape != ref.shape or not t.is_contiguous():\n        t = t.expand_as(ref).contiguous()\n    return t\n\n\ndef _parse_args_scalar(args, kwargs):\n    # Accept either (self, end, weight) or args wrapped in a single tuple/list\n    if len(args) == 1 and isinstance(args[0], (tuple, list)):\n        args = tuple(args[0])\n    self_t = kwargs.get('self', args[0] if len(args) > 0 else None)\n    end = kwargs.get('end', args[1] if len(args) > 1 else None)\n    weight = kwargs.get('weight', args[2] if len(args) > 2 else None)\n    return self_t, end, weight\n\n\ndef _parse_args_tensor(args, kwargs):\n    # Accept either (self, end, weight) or args wrapped in a single tuple/list\n    if len(args) == 1 and isinstance(args[0], (tuple, list)):\n        args = tuple(args[0])\n    self_t = kwargs.get('self', args[0] if len(args) > 0 else None)\n    end = kwargs.get('end', args[1] if len(args) > 1 else None)\n    weight = kwargs.get('weight', args[2] if len(args) > 2 else None)\n    return self_t, end, weight\n\n\ndef lerp__Scalar(*args, **kwargs):\n    self_t, end, weight = _parse_args_scalar(args, kwargs)\n    if not isinstance(self_t, torch.Tensor) or not isinstance(end, torch.Tensor):\n        raise TypeError(\"lerp__Scalar expects tensors for 'self' and 'end'.\")\n    if not torch.is_floating_point(self_t):\n        raise TypeError(\"lerp__Scalar only supports floating point tensors for in-place operation.\")\n    if not self_t.is_cuda or not end.is_cuda:\n        raise ValueError(\"lerp__Scalar requires CUDA tensors.\")\n    if not self_t.is_contiguous():\n        raise ValueError(\"lerp__Scalar requires 'self' to be contiguous for in-place operation.\")\n\n    # Handle scalar weight (number or 0-dim tensor)\n    if isinstance(weight, torch.Tensor):\n        if weight.numel() != 1:\n            raise ValueError(\"Scalar overload received a non-scalar tensor for 'weight'.\")\n        weight_val = float(weight.item())\n    else:\n        weight_val = float(weight)\n\n    end_c = _ensure_contig_same_dtype_device(end, self_t)\n\n    n_elements = self_t.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _lerp_inplace_scalar_kernel[grid](\n        self_t, end_c, n_elements, float(weight_val), BLOCK_SIZE=1024\n    )\n    return self_t\n\n\ndef lerp__Tensor(*args, **kwargs):\n    self_t, end, weight = _parse_args_tensor(args, kwargs)\n    if not isinstance(self_t, torch.Tensor) or not isinstance(end, torch.Tensor) or not isinstance(weight, torch.Tensor):\n        raise TypeError(\"lerp__Tensor expects tensors for 'self', 'end', and 'weight'.\")\n    if not torch.is_floating_point(self_t):\n        raise TypeError(\"lerp__Tensor only supports floating point tensors for in-place operation.\")\n    if not self_t.is_cuda or not end.is_cuda or not weight.is_cuda:\n        raise ValueError(\"lerp__Tensor requires CUDA tensors.\")\n    if not self_t.is_contiguous():\n        raise ValueError(\"lerp__Tensor requires 'self' to be contiguous for in-place operation.\")\n\n    end_c = _ensure_contig_same_dtype_device(end, self_t)\n    weight_c = _ensure_contig_same_dtype_device(weight, self_t)\n\n    n_elements = self_t.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _lerp_inplace_tensor_kernel[grid](\n        self_t, end_c, weight_c, n_elements, BLOCK_SIZE=1024\n    )\n    return self_t",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9957011510602671,
        "meets_threshold": false
      },
      "erf": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9910858895044264,
            "flaggems_speedup_vs_cuda": 0.995399,
            "speedup_vs_flaggems": 0.9956669531558967
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef erf(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    x_f32 = x.to(tl.float32)\n    ax = tl.abs(x_f32)\n\n    # Abramowitz & Stegun 7.1.26 approximation\n    p = 0.3275911\n    a1 = 0.254829592\n    a2 = -0.284496736\n    a3 = 1.421413741\n    a4 = -1.453152027\n    a5 = 1.061405429\n\n    t = 1.0 / (1.0 + p * ax)\n    poly = (((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t)\n    y = 1.0 - poly * tl.exp(-ax * ax)\n    y = tl.where(x_f32 >= 0, y, -y)\n\n    y_cast = y.to(x.dtype)\n    tl.store(out_ptr + offsets, y_cast, mask=mask)\n\n\n_erf_kernel = erf\n\n\ndef erf(*args, **kwargs):\n    # Parse input tensor\n    if len(args) >= 1:\n        x = args[0]\n    elif \"input\" in kwargs:\n        x = kwargs[\"input\"]\n    else:\n        raise TypeError(\"erf() missing required argument 'input' (pos 1)\")\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"erf(): expected input to be a torch.Tensor\")\n\n    # Fallback to PyTorch if not on CUDA device\n    if not x.is_cuda:\n        return torch.erf(x, out=out)\n\n    # Validate dtype\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise TypeError(f\"erf(): expected floating point tensor, got {x.dtype}\")\n\n    # Prepare output\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"erf(): 'out' must be a torch.Tensor\")\n        if out.shape != x.shape:\n            raise RuntimeError(f\"erf(): output tensor has wrong shape {out.shape}, expected {x.shape}\")\n        if out.dtype != x.dtype:\n            raise RuntimeError(f\"erf(): output dtype {out.dtype} does not match input dtype {x.dtype}\")\n        if out.device != x.device:\n            raise RuntimeError(\"erf(): output tensor must be on the same device as input\")\n    x_contig = x.contiguous()\n    need_copy_back = False\n    if out is None:\n        out_buf = torch.empty_like(x_contig)\n    else:\n        if out.is_contiguous():\n            out_buf = out\n        else:\n            out_buf = torch.empty_like(x_contig)\n            need_copy_back = True\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        if out is None:\n            return torch.empty_like(x)\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _erf_kernel[grid](x_contig, out_buf, n_elements, BLOCK_SIZE=1024)\n\n    if need_copy_back:\n        out.copy_(out_buf)\n        return out\n    return out_buf if out is None else out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9956669531558967,
        "meets_threshold": false
      },
      "reciprocal": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0001973890266735,
            "flaggems_speedup_vs_cuda": 1.00518566666667,
            "speedup_vs_flaggems": 0.9950374564566382
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef reciprocal(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = 1 / x\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\n# Keep a reference to the kernel before defining the Python wrapper with the same name\n_reciprocal_kernel = reciprocal\n\n\ndef reciprocal(*args, **kwargs):\n    x = None\n    if len(args) > 0:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    if x is None:\n        raise TypeError(\"reciprocal() missing 1 required positional argument: 'input'\")\n\n    out = kwargs.get('out', None)\n\n    if not torch.is_tensor(x):\n        x = torch.tensor(x)\n\n    if x.is_complex():\n        raise NotImplementedError(\"reciprocal() Triton kernel currently supports only real dtypes, got complex.\")\n\n    if not x.is_cuda:\n        raise ValueError(\"reciprocal(): input must be a CUDA tensor.\")\n\n    x_contig = x.contiguous()\n\n    if out is None:\n        output = torch.empty_like(x_contig)\n    else:\n        if not torch.is_tensor(out):\n            raise TypeError(\"reciprocal(): 'out' must be a torch.Tensor.\")\n        if not out.is_cuda:\n            raise ValueError(\"reciprocal(): 'out' must be a CUDA tensor.\")\n        if out.numel() != x_contig.numel():\n            raise ValueError(\"reciprocal(): 'out' must have the same number of elements as 'input'.\")\n        # Match shape if needed\n        if out.shape != x_contig.shape:\n            out = out.reshape(x_contig.shape)\n        output = out.contiguous()\n\n    n_elements = x_contig.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _reciprocal_kernel[grid](x_contig, output, n_elements, BLOCK_SIZE=1024)\n\n    if out is not None and output.data_ptr() != out.data_ptr():\n        out.copy_(output)\n        return out\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9950374564566382,
        "meets_threshold": false
      },
      "ones": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0087439251321262,
            "flaggems_speedup_vs_cuda": 1.01377733333333,
            "speedup_vs_flaggems": 0.9950349963096395
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ones(output_ptr, n_elements, DTYPE: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val = tl.full([BLOCK_SIZE], 1, DTYPE)\n    tl.store(output_ptr + offsets, val, mask=mask)\n\n\n_ones_triton_kernel = ones  # preserve Triton kernel before defining the Python wrapper with the same name\n\n\ndef ones(*args, **kwargs):\n    # Parse kwargs\n    out = kwargs.pop(\"out\", None)\n    dtype = kwargs.pop(\"dtype\", None)\n    device = kwargs.pop(\"device\", None)\n    requires_grad = kwargs.pop(\"requires_grad\", False)\n    # Optional/ignored kwargs for this implementation\n    layout = kwargs.pop(\"layout\", torch.strided)\n    pin_memory = kwargs.pop(\"pin_memory\", False)\n    memory_format = kwargs.pop(\"memory_format\", torch.contiguous_format)\n    # Any remaining unexpected kwargs will raise\n    if kwargs:\n        raise TypeError(f\"ones() got unexpected keyword arguments: {', '.join(kwargs.keys())}\")\n\n    # Determine size\n    size = None\n    if out is not None and len(args) == 0:\n        size = tuple(out.shape)\n    else:\n        if len(args) == 0:\n            size = tuple()\n        elif len(args) == 1 and isinstance(args[0], (tuple, list, torch.Size)):\n            size = tuple(int(x) for x in args[0])\n        else:\n            try:\n                size = tuple(int(x) for x in args)\n            except Exception as e:\n                raise TypeError(\"Size must be a sequence of integers or multiple integer arguments\") from e\n\n    # Prepare output tensor\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if size is not None and tuple(out.shape) != tuple(size):\n            raise ValueError(f\"Provided size {tuple(size)} does not match out tensor shape {tuple(out.shape)}\")\n        if dtype is not None and out.dtype != dtype:\n            raise TypeError(f\"dtype {dtype} does not match out tensor dtype {out.dtype}\")\n        output = out\n    else:\n        # Infer device\n        if device is None:\n            if torch.cuda.is_available():\n                device = torch.device(\"cuda\")\n            else:\n                device = torch.device(\"cpu\")\n        # Infer dtype\n        if dtype is None:\n            dtype = torch.get_default_dtype()\n        output = torch.empty(size, dtype=dtype, device=device)\n\n    # Set requires_grad if requested\n    if requires_grad:\n        output.requires_grad_(True)\n\n    # If not on CUDA, fall back to PyTorch implementation\n    if output.device.type != \"cuda\":\n        # Respect out parameter when provided\n        if out is not None:\n            torch.ones(output.shape, dtype=output.dtype, device=output.device, out=output)\n            return output\n        else:\n            return torch.ones(size, dtype=output.dtype, device=output.device)\n\n    # Ensure contiguous for simple linear fill\n    if memory_format is not torch.contiguous_format or not output.is_contiguous():\n        raise ValueError(\"This Triton implementation currently supports only contiguous tensors.\")\n\n    # Map torch dtype to triton dtype\n    def _torch_dtype_to_tl_dtype(td: torch.dtype):\n        mapping = {\n            torch.float16: tl.float16,\n            torch.bfloat16: tl.bfloat16,\n            torch.float32: tl.float32,\n            torch.float64: tl.float64,\n            torch.int8: tl.int8,\n            torch.uint8: tl.uint8,\n            torch.int16: tl.int16,\n            torch.int32: tl.int32,\n            torch.int64: tl.int64,\n            torch.bool: tl.int8,  # store 1 as uint8 for bool tensors\n        }\n        if td not in mapping:\n            raise TypeError(f\"Unsupported dtype for Triton ones kernel: {td}\")\n        return mapping[td]\n\n    tl_dtype = _torch_dtype_to_tl_dtype(output.dtype)\n    n_elements = output.numel()\n    if n_elements == 0:\n        return output\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _ones_triton_kernel[grid](output, n_elements, DTYPE=tl_dtype, BLOCK_SIZE=BLOCK_SIZE)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9950349963096395,
        "meets_threshold": false
      },
      "aten::gelu_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.995893523295114,
            "flaggems_speedup_vs_cuda": 1.00548333333333,
            "speedup_vs_flaggems": 0.9904624873229632
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gelu_(x_ptr,  # *Pointer* to the input/output tensor (in-place).\n          n_elements,  # Number of elements.\n          USE_TANH: tl.constexpr,  # Whether to use tanh approximation.\n          BLOCK_SIZE: tl.constexpr  # Elements per program.\n          ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    x_f32 = x.to(tl.float32)\n\n    # Compute GELU either exact (via erf approximation) or tanh approximation\n    if USE_TANH:\n        # tanh approximation:\n        # gelu(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 x^3)))\n        c0 = 0.7978845608028654  # sqrt(2/pi)\n        c1 = 0.044715\n        x3 = x_f32 * x_f32 * x_f32\n        z = c0 * (x_f32 + c1 * x3)\n        # tanh(z) = (1 - e^{-2z}) / (1 + e^{-2z})\n        t = tl.exp(-2.0 * z)\n        tanh_z = (1.0 - t) / (1.0 + t)\n        y = 0.5 * x_f32 * (1.0 + tanh_z)\n    else:\n        # exact (erf-based) GELU:\n        # gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))\n        inv_sqrt2 = 0.7071067811865476\n        z = x_f32 * inv_sqrt2\n\n        # Abramowitz and Stegun formula 7.1.26 for erf approximation\n        # erf(x) â‰ˆ sign(x) * (1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * e^{-x^2})\n        # where t = 1 / (1 + p*|x|)\n        p = 0.3275911\n        a1 = 0.254829592\n        a2 = -0.284496736\n        a3 = 1.421413741\n        a4 = -1.453152027\n        a5 = 1.061405429\n\n        az = tl.abs(z)\n        t = 1.0 / (1.0 + p * az)\n        poly = a5\n        poly = poly * t + a4\n        poly = poly * t + a3\n        poly = poly * t + a2\n        poly = poly * t + a1\n        poly = poly * t\n        erf_abs = 1.0 - poly * tl.exp(-az * az)\n        erf_z = tl.where(z >= 0, erf_abs, -erf_abs)\n\n        y = 0.5 * x_f32 * (1.0 + erf_z)\n\n    y_cast = y.to(x.dtype)\n    tl.store(x_ptr + offsets, y_cast, mask=mask)\n\n\n# Preserve a handle to the kernel before defining the Python wrapper of the same name\ngelu__kernel = gelu_\n\n\ndef gelu_(*args, **kwargs):\n    # Resolve input tensor\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        # Try common names\n        x = kwargs.get('input', None)\n        if x is None:\n            x = kwargs.get('self', None)\n        if x is None:\n            x = kwargs.get('x', None)\n    if x is None:\n        raise ValueError(\"gelu_ expects a tensor as the first argument.\")\n\n    # Determine approximation mode\n    approx = kwargs.get('approximate', 'none')\n    if isinstance(approx, bool):\n        use_tanh = bool(approx)\n    else:\n        approx_str = str(approx).lower()\n        if approx_str in ('tanh', 'true'):\n            use_tanh = True\n        elif approx_str in ('none', 'false'):\n            use_tanh = False\n        else:\n            raise ValueError(f\"Unsupported approximate mode: {approx}. Use 'none' or 'tanh'.\")\n\n    if not x.is_cuda:\n        raise AssertionError(\"Input tensor must be on CUDA device for Triton kernel.\")\n    if not x.is_contiguous():\n        raise AssertionError(\"Input tensor must be contiguous.\")\n    if not x.is_floating_point():\n        raise AssertionError(\"gelu_ expects a floating point tensor.\")\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    gelu__kernel[grid](x, n_elements, USE_TANH=use_tanh, BLOCK_SIZE=BLOCK_SIZE)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9904624873229632,
        "meets_threshold": false
      },
      "zeros_like": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9904176180868353,
            "flaggems_speedup_vs_cuda": 1.00877966666667,
            "speedup_vs_flaggems": 0.9817977610110751
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef zeros_like(out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    tl.store(out_ptr + offsets, 0, mask=mask)\n\n\n# Preserve a handle to the kernel before defining the wrapper with the same name\nzeros_like_kernel = zeros_like\n\n\ndef zeros_like(*args, **kwargs):\n    # Extract input tensor\n    if len(args) > 0:\n        inp = args[0]\n    else:\n        inp = kwargs.get('input', None)\n    if inp is None:\n        raise ValueError(\"zeros_like expects an input tensor as the first positional argument or as 'input' kwarg.\")\n\n    # Parse options with defaults following torch.zeros_like semantics\n    dtype = kwargs.get('dtype', None)\n    layout = kwargs.get('layout', None)\n    device = kwargs.get('device', None)\n    memory_format = kwargs.get('memory_format', torch.preserve_format)\n    requires_grad = kwargs.get('requires_grad', inp.requires_grad)\n\n    # Allocate output tensor preserving requested properties\n    out = torch.empty_like(\n        inp,\n        dtype=dtype if dtype is not None else inp.dtype,\n        layout=layout if layout is not None else inp.layout,\n        device=device if device is not None else inp.device,\n        memory_format=memory_format,\n        requires_grad=requires_grad,\n    )\n\n    # Ensure CUDA device for Triton execution\n    if not out.is_cuda:\n        raise ValueError(\"zeros_like Triton kernel requires CUDA tensors. Please move the input to a CUDA device.\")\n\n    # Compute the size of the underlying storage in elements and zero it\n    storage_nbytes = out.untyped_storage().nbytes()\n    elem_nbytes = out.element_size()\n    n_elements = storage_nbytes // elem_nbytes\n\n    if n_elements == 0:\n        return out\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    zeros_like_kernel[grid](out, n_elements, BLOCK_SIZE=1024)\n\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9817977610110751,
        "meets_threshold": false
      },
      "aten::erf_": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9855473842296526,
            "flaggems_speedup_vs_cuda": 1.00445933333333,
            "speedup_vs_flaggems": 0.9811720111744917
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef erf_(x_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x32 = x.to(tl.float32)\n\n    ax = tl.abs(x32)\n    t = 1.0 / (1.0 + 0.5 * ax)\n\n    p = 1.00002368 + t * (\n        0.37409196 + t * (\n            0.09678418 + t * (\n                -0.18628806 + t * (\n                    0.27886807 + t * (\n                        -1.13520398 + t * (\n                            1.48851587 + t * (\n                                -0.82215223 + t * 0.17087277\n                            )\n                        )\n                    )\n                )\n            )\n        )\n    )\n    s = -x32 * x32 - 1.26551223 + t * p\n    tau = t * tl.exp(s)\n    y32 = tl.where(x32 >= 0, 1.0 - tau, tau - 1.0)\n\n    y = y32.to(x.dtype)\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n# keep a reference to the kernel before defining the wrapper with the same name\nerf__kernel = erf_\n\ndef erf_(*args, **kwargs):\n    # Extract the input tensor\n    x = None\n    if len(args) >= 1 and isinstance(args[0], torch.Tensor):\n        x = args[0]\n    elif 'input' in kwargs and isinstance(kwargs['input'], torch.Tensor):\n        x = kwargs['input']\n    elif 'self' in kwargs and isinstance(kwargs['self'], torch.Tensor):\n        x = kwargs['self']\n    elif 'args' in kwargs and isinstance(kwargs['args'], (list, tuple)) and kwargs['args']:\n        x = kwargs['args'][0]\n    if x is None:\n        raise TypeError(\"erf_ expects a tensor as its first argument\")\n\n    # Fallback for unsupported devices/dtypes\n    if not x.is_cuda:\n        return x.erf_()\n\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32):\n        return x.erf_()\n\n    n_elements = x.numel()\n    if n_elements == 0:\n        return x\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    erf__kernel[grid](x, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    return x",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9811720111744917,
        "meets_threshold": false
      },
      "triu": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9742413434359468,
            "flaggems_speedup_vs_cuda": 0.997625,
            "speedup_vs_flaggems": 0.9765606750391649
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef triu(x_ptr, out_ptr, M, N, B, diagonal, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    pid_b = tl.program_id(axis=0)\n    pid_m = tl.program_id(axis=1)\n    pid_n = tl.program_id(axis=2)\n\n    rows = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    cols = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    in_bounds = (rows[:, None] < M) & (cols[None, :] < N) & (pid_b < B)\n\n    base = pid_b * (M * N)\n    offsets = base + rows[:, None] * N + cols[None, :]\n\n    x = tl.load(x_ptr + offsets, mask=in_bounds, other=0)\n    keep = (cols[None, :] - rows[:, None]) >= diagonal\n    out = tl.where(keep, x, 0)\n\n    tl.store(out_ptr + offsets, out, mask=in_bounds)\n\n\n# Preserve kernel reference before defining Python wrapper with the same name.\ntriu_kernel = triu\n\n\ndef triu(*args, **kwargs):\n    # Parse arguments like torch.triu(input, diagonal=0, *, out=None)\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    if x is None:\n        raise TypeError(\"triu() missing required argument 'input' (pos 1)\")\n\n    if len(args) >= 2:\n        diagonal = args[1]\n    else:\n        diagonal = kwargs.get('diagonal', 0)\n\n    out_arg = kwargs.get('out', None)\n\n    if x.ndim < 2:\n        raise RuntimeError(\"triu: input tensor must be at least 2-D\")\n\n    # Ensure contiguous for Triton processing\n    x_contig = x.contiguous()\n    M, N = x_contig.shape[-2], x_contig.shape[-1]\n    B = x_contig.numel() // (M * N)\n\n    # Prepare output\n    if out_arg is not None:\n        if out_arg.shape != x.shape or out_arg.device != x.device or out_arg.dtype != x.dtype:\n            raise RuntimeError(\"triu: provided 'out' has incompatible shape, device or dtype\")\n        out_contig = out_arg.contiguous()\n    else:\n        out_contig = torch.empty_like(x_contig)\n\n    # Launch kernel\n    BLOCK_M = 32\n    BLOCK_N = 32\n    grid = (B, triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n    triu_kernel[grid](x_contig, out_contig, M, N, B, int(diagonal), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n\n    # If out was provided and non-contiguous, copy back into it\n    if out_arg is not None:\n        if out_arg.is_contiguous():\n            return out_arg\n        else:\n            out_arg.copy_(out_contig.view_as(out_arg))\n            return out_arg\n\n    # Return result with original shape\n    return out_contig.view_as(x)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9765606750391649,
        "meets_threshold": false
      },
      "aten::lerp": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.9755256973571663,
            "flaggems_speedup_vs_cuda": 1.00350733333333,
            "speedup_vs_flaggems": 0.9721161619385306
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _lerp_kernel(a_ptr, b_ptr, w_ptr, out_ptr, n_elements, w_scalar, HAS_W_TENSOR: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    a = tl.load(a_ptr + offsets, mask=mask)\n    b = tl.load(b_ptr + offsets, mask=mask)\n\n    # Compute in fp32 for numerical stability\n    a32 = a.to(tl.float32)\n    b32 = b.to(tl.float32)\n\n    if HAS_W_TENSOR:\n        w = tl.load(w_ptr + offsets, mask=mask)\n        w32 = w.to(tl.float32)\n        res32 = a32 + w32 * (b32 - a32)\n    else:\n        w32 = w_scalar  # assumed float32 scalar\n        res32 = a32 + w32 * (b32 - a32)\n\n    # Cast back to input dtype (assuming a/b/out share dtype)\n    res = res32.to(a.dtype)\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\ndef _check_device_and_dtype(*tensors):\n    devices = [t.device for t in tensors if isinstance(t, torch.Tensor)]\n    dtypes = [t.dtype for t in tensors if isinstance(t, torch.Tensor)]\n    if len(devices) > 0:\n        dev = devices[0]\n        assert all(d == dev for d in devices), \"All tensors must be on the same device\"\n        assert dev.type == \"cuda\", \"Triton kernels require CUDA tensors\"\n    if len(dtypes) > 0:\n        base_dtype = dtypes[0]\n        assert all(dt == base_dtype for dt in dtypes), \"All tensors must have the same dtype\"\n\n\ndef _broadcast_and_prepare(a: torch.Tensor, b: torch.Tensor, out_dtype: torch.dtype, weight_tensor: torch.Tensor = None):\n    # Compute broadcasted shape\n    if weight_tensor is None:\n        shape = torch.broadcast_shapes(a.shape, b.shape)\n    else:\n        shape = torch.broadcast_shapes(a.shape, b.shape, weight_tensor.shape)\n    # Expand and convert dtype\n    a_exp = a.expand(shape).to(out_dtype).contiguous()\n    b_exp = b.expand(shape).to(out_dtype).contiguous()\n    w_exp = None\n    if weight_tensor is not None:\n        w_exp = weight_tensor.expand(shape).to(out_dtype).contiguous()\n    return a_exp, b_exp, w_exp, shape\n\n\ndef _result_float_dtype(a: torch.Tensor, b: torch.Tensor, weight_scalar=None, weight_tensor: torch.Tensor = None):\n    # Choose a reasonable floating dtype for computation/output\n    # Prefer existing floating dtype if present, else default to float32\n    def _preferred_float(dt):\n        if dt in (torch.float32, torch.bfloat16, torch.float16, torch.float64):\n            return dt\n        return None\n\n    dt_candidates = [_preferred_float(a.dtype), _preferred_float(b.dtype)]\n    if weight_tensor is not None:\n        dt_candidates.append(_preferred_float(weight_tensor.dtype))\n    chosen = next((dt for dt in dt_candidates if dt is not None), None)\n    if chosen is None:\n        chosen = torch.float32\n    # Avoid float64 in Triton kernel; compute in fp32 then cast back\n    if chosen == torch.float64:\n        chosen = torch.float32\n    return chosen\n\n\ndef _launch_lerp(a_exp, b_exp, out, w_exp=None, w_scalar=None):\n    n_elements = out.numel()\n    BLOCK = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    if w_exp is not None:\n        _lerp_kernel[grid](a_exp, b_exp, w_exp, out, n_elements, 0.0, HAS_W_TENSOR=True, BLOCK_SIZE=BLOCK)\n    else:\n        # ensure scalar is float32\n        w_scalar_f32 = float(w_scalar)\n        _lerp_kernel[grid](a_exp, b_exp, out, out, n_elements, w_scalar_f32, HAS_W_TENSOR=False, BLOCK_SIZE=BLOCK)\n\n\ndef lerp_Scalar(self: torch.Tensor, end: torch.Tensor, weight):\n    assert isinstance(self, torch.Tensor) and isinstance(end, torch.Tensor)\n    out_dtype = _result_float_dtype(self, end, weight_scalar=weight)\n    _check_device_and_dtype(self.to(out_dtype), end.to(out_dtype))\n    a_exp, b_exp, _, shape = _broadcast_and_prepare(self, end, out_dtype, weight_tensor=None)\n    out = torch.empty(shape, device=self.device, dtype=out_dtype)\n    _launch_lerp(a_exp, b_exp, out, w_exp=None, w_scalar=weight)\n    return out\n\n\ndef lerp_Tensor(self: torch.Tensor, end: torch.Tensor, weight: torch.Tensor):\n    assert isinstance(self, torch.Tensor) and isinstance(end, torch.Tensor) and isinstance(weight, torch.Tensor)\n    out_dtype = _result_float_dtype(self, end, weight_tensor=weight)\n    _check_device_and_dtype(self.to(out_dtype), end.to(out_dtype), weight.to(out_dtype))\n    a_exp, b_exp, w_exp, shape = _broadcast_and_prepare(self, end, out_dtype, weight_tensor=weight)\n    out = torch.empty(shape, device=self.device, dtype=out_dtype)\n    _launch_lerp(a_exp, b_exp, out, w_exp=w_exp, w_scalar=None)\n    return out\n\n\ndef lerp_Scalar_out(self: torch.Tensor, end: torch.Tensor, weight, out: torch.Tensor):\n    assert isinstance(self, torch.Tensor) and isinstance(end, torch.Tensor) and isinstance(out, torch.Tensor)\n    out_dtype = out.dtype\n    # Prepare inputs to match out dtype\n    _check_device_and_dtype(self.to(out_dtype), end.to(out_dtype), out)\n    a_exp, b_exp, _, shape = _broadcast_and_prepare(self, end, out_dtype, weight_tensor=None)\n    if out.numel() != int(torch.Size(shape).numel()) or out.shape != shape:\n        # Resize if shape mismatch\n        out.resize_(shape)\n    if not out.is_contiguous():\n        out_contig = torch.empty_like(out, memory_format=torch.contiguous_format)\n        _launch_lerp(a_exp, b_exp, out_contig, w_exp=None, w_scalar=weight)\n        out.copy_(out_contig)\n    else:\n        _launch_lerp(a_exp, b_exp, out, w_exp=None, w_scalar=weight)\n    return out\n\n\ndef lerp_Tensor_out(self: torch.Tensor, end: torch.Tensor, weight: torch.Tensor, out: torch.Tensor):\n    assert isinstance(self, torch.Tensor) and isinstance(end, torch.Tensor) and isinstance(weight, torch.Tensor) and isinstance(out, torch.Tensor)\n    out_dtype = out.dtype\n    _check_device_and_dtype(self.to(out_dtype), end.to(out_dtype), weight.to(out_dtype), out)\n    a_exp, b_exp, w_exp, shape = _broadcast_and_prepare(self, end, out_dtype, weight_tensor=weight)\n    if out.numel() != int(torch.Size(shape).numel()) or out.shape != shape:\n        out.resize_(shape)\n    if not out.is_contiguous():\n        out_contig = torch.empty_like(out, memory_format=torch.contiguous_format)\n        _launch_lerp(a_exp, b_exp, out_contig, w_exp=w_exp, w_scalar=None)\n        out.copy_(out_contig)\n    else:\n        _launch_lerp(a_exp, b_exp, out, w_exp=w_exp, w_scalar=None)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9721161619385306,
        "meets_threshold": false
      },
      "any": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.8226649895028534,
            "flaggems_speedup_vs_cuda": 0.857870666666667,
            "speedup_vs_flaggems": 0.958961556174186
          }
        },
        "code": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef any_reduce_dim_kernel(\n    x_ptr,\n    out_ptr,\n    K,\n    inner,\n    n_out,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    mask_pid = pid < n_out\n\n    # Compute i and j indices corresponding to pid\n    # i: index over \"outer\" dimension, j: index over \"inner\" dimension\n    # Note: outer is implicitly n_out // inner\n    i = pid // inner\n    j = pid % inner\n\n    # Base offset for this output position\n    base = i * K * inner + j\n\n    acc = tl.zeros((), dtype=tl.int32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        offs_k = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = offs_k < K\n        x_offsets = base + offs_k * inner\n        # Load with masking and other=0 to treat masked elements as zero (False)\n        x_vals = tl.load(x_ptr + x_offsets, mask=(mask_pid & mask_k), other=0)\n        # Convert to boolean (non-zero -> True)\n        is_true = x_vals != 0\n        # Convert booleans to int32 (True->1, False->0) and accumulate\n        ints = tl.where(is_true, 1, 0)\n        partial = tl.sum(ints, axis=0)\n        acc += partial\n\n    # Any is True if acc > 0\n    res = acc > 0\n    tl.store(out_ptr + pid, res, mask=mask_pid)\n\n\ndef _normalize_dim(dim, ndim):\n    if dim < 0:\n        dim += ndim\n    return dim\n\n\ndef _reduce_single_dim(x: torch.Tensor, dim: int, keepdim: bool):\n    x = x.contiguous()\n    sizes = list(x.shape)\n    ndim = len(sizes)\n    dim = _normalize_dim(dim, ndim)\n\n    outer = 1 if dim == 0 else math.prod(sizes[:dim])\n    K = sizes[dim]\n    inner = 1 if dim == ndim - 1 else math.prod(sizes[dim + 1:])\n\n    # Output shape with keepdim=True: replace reduced dim by 1\n    out_shape_keepdim = sizes.copy()\n    out_shape_keepdim[dim] = 1\n    out = torch.empty(out_shape_keepdim, dtype=torch.bool, device=x.device)\n    n_out = out.numel()\n\n    if n_out > 0:\n        grid = lambda meta: (n_out,)\n        any_reduce_dim_kernel[grid](\n            x,\n            out,\n            K,\n            inner,\n            n_out,\n            BLOCK_SIZE_K=1024,\n        )\n\n    if keepdim:\n        return out\n    else:\n        # Remove the reduced dimension\n        return out.squeeze(dim=dim)\n\n\ndef any(*args, **kwargs):\n    # Parse input tensor\n    if len(args) >= 1:\n        x = args[0]\n        remaining = args[1:]\n    else:\n        x = kwargs.get(\"input\", None)\n        remaining = ()\n    if x is None:\n        raise ValueError(\"Input tensor is required.\")\n\n    keepdim = kwargs.get(\"keepdim\", False)\n\n    # Determine reduction specs\n    dim = kwargs.get(\"dim\", None)\n    dims = kwargs.get(\"dims\", None)\n\n    # If dim provided positionally\n    if dim is None and len(remaining) >= 1:\n        dim = remaining[0]\n        remaining = remaining[1:]\n\n    if dims is None and len(remaining) >= 1 and isinstance(remaining[0], (list, tuple)):\n        dims = remaining[0]\n\n    if dim is not None and dims is not None:\n        raise ValueError(\"Provide either 'dim' or 'dims', not both.\")\n\n    if dim is None and dims is None:\n        # Global reduction across all elements\n        x = x.contiguous()\n        total = x.numel()\n        out_shape_keepdim = [1] * x.dim()\n        out_keep = torch.empty(out_shape_keepdim, dtype=torch.bool, device=x.device)\n        n_out = 1\n        if n_out > 0:\n            grid = lambda meta: (n_out,)\n            any_reduce_dim_kernel[grid](\n                x,\n                out_keep,\n                total,\n                1,\n                n_out,\n                BLOCK_SIZE_K=1024,\n            )\n        if keepdim:\n            return out_keep\n        else:\n            # Return scalar boolean\n            return out_keep.reshape(())\n    elif dim is not None:\n        return _reduce_single_dim(x, int(dim), keepdim)\n    else:\n        # dims provided: reduce sequentially with keepdim=True, then squeeze if needed\n        dims_list = list(dims)\n        ndim = x.dim()\n        # Normalize dims to positive and unique\n        dims_norm = []\n        for d in dims_list:\n            dn = _normalize_dim(int(d), ndim)\n            if dn not in dims_norm:\n                dims_norm.append(dn)\n        dims_norm.sort()\n\n        out = x\n        for d in dims_norm:\n            out = _reduce_single_dim(out, d, keepdim=True)\n\n        if keepdim:\n            return out\n        else:\n            # Squeeze reduced dimensions, accounting for shifting indices\n            for idx, d in enumerate(dims_norm):\n                out = out.squeeze(dim=d - idx)\n            return out\n\n\ndef any_dim(input: torch.Tensor, dim: int, keepdim: bool = False):\n    return _reduce_single_dim(input, int(dim), keepdim)\n\n\ndef any_dims(input: torch.Tensor, dims, keepdim: bool = False):\n    dims_list = list(dims)\n    ndim = input.dim()\n    # Normalize dims to positive and unique\n    dims_norm = []\n    for d in dims_list:\n        dn = _normalize_dim(int(d), ndim)\n        if dn not in dims_norm:\n            dims_norm.append(dn)\n    dims_norm.sort()\n\n    out = input\n    for d in dims_norm:\n        out = _reduce_single_dim(out, d, keepdim=True)\n\n    if keepdim:\n        return out\n    else:\n        for idx, d in enumerate(dims_norm):\n            out = out.squeeze(dim=d - idx)\n        return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.958961556174186,
        "meets_threshold": false
      },
      "mv": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.9688958175840144,
            "flaggems_speedup_vs_cuda": 1.02239766666667,
            "speedup_vs_flaggems": 0.9476702159766385
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef mv(A_ptr,\n       x_ptr,\n       y_ptr,\n       M, N,\n       stride_am, stride_an,\n       stride_x, stride_y,\n       BLOCK_SIZE_K: tl.constexpr,\n       ACC_TYPE: tl.constexpr):\n    row = tl.program_id(0)\n    mask_row = row < M\n\n    acc = tl.zeros((), dtype=ACC_TYPE)\n\n    k = 0\n    while k < N:\n        offs_k = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = offs_k < N\n\n        a_ptrs = A_ptr + row * stride_am + offs_k * stride_an\n        x_ptrs = x_ptr + offs_k * stride_x\n\n        a = tl.load(a_ptrs, mask=mask_row & mask_k, other=0)\n        xv = tl.load(x_ptrs, mask=mask_k, other=0)\n\n        acc += tl.sum(a.to(ACC_TYPE) * xv.to(ACC_TYPE), axis=0)\n        k += BLOCK_SIZE_K\n\n    tl.store(y_ptr + row * stride_y, acc, mask=mask_row)\n\n\n_mv_kernel = mv\n\n\ndef mv(*args, **kwargs):\n    # Parse inputs akin to torch.mv(input, vec, *, out=None)\n    if len(args) >= 2:\n        A, x = args[0], args[1]\n    else:\n        A = kwargs.get('input', None)\n        x = kwargs.get('vec', None)\n        if A is None or x is None:\n            raise ValueError(\"mv expects arguments (input, vec) or keyword arguments input=, vec=.\")\n    out = kwargs.get('out', None)\n\n    if not isinstance(A, torch.Tensor) or not isinstance(x, torch.Tensor):\n        raise TypeError(\"mv expects torch.Tensor inputs\")\n\n    if A.dim() != 2 or x.dim() != 1:\n        raise ValueError(\"mv expects a 2D matrix and a 1D vector\")\n\n    M, N = A.shape\n    if x.shape[0] != N:\n        raise ValueError(f\"Incompatible shapes: A is {A.shape}, x is {x.shape}\")\n\n    if A.device.type != 'cuda' or x.device.type != 'cuda':\n        raise ValueError(\"Tensors must be on CUDA device\")\n\n    if A.device != x.device:\n        raise ValueError(\"Input tensors must be on the same device\")\n\n    # Supported dtypes\n    supported_dtypes = {torch.float16, torch.bfloat16, torch.float32, torch.float64}\n    if A.dtype not in supported_dtypes or x.dtype not in supported_dtypes:\n        raise TypeError(f\"Unsupported dtypes: A.dtype={A.dtype}, x.dtype={x.dtype}\")\n\n    # Determine result dtype following PyTorch promotion\n    result_dtype = torch.result_type(A, x)\n\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor if provided\")\n        if out.device != A.device:\n            raise ValueError(\"out tensor must be on the same CUDA device as inputs\")\n        if out.shape != (M,):\n            raise ValueError(f\"out tensor must have shape ({M},), got {out.shape}\")\n        if out.dtype != result_dtype:\n            raise TypeError(f\"out tensor must have dtype {result_dtype}, got {out.dtype}\")\n        y = out\n    else:\n        y = torch.empty((M,), device=A.device, dtype=result_dtype)\n\n    # Strides in elements\n    stride_am, stride_an = A.stride()\n    stride_x = x.stride(0)\n    stride_y = y.stride(0)\n\n    # Choose accumulation dtype for numerical stability/precision\n    # Use fp64 accumulation if result is float64, else fp32\n    ACC_TYPE = tl.float64 if result_dtype == torch.float64 else tl.float32\n\n    # Select a reasonable BLOCK_SIZE_K\n    BLOCK_SIZE_K = 1024\n    grid = (M,)\n\n    _mv_kernel[grid](\n        A, x, y,\n        M, N,\n        stride_am, stride_an,\n        stride_x, stride_y,\n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n        ACC_TYPE=ACC_TYPE,\n    )\n\n    return y\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.9476702159766385,
        "meets_threshold": false
      },
      "aten::maximum": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.8847966608091403,
            "flaggems_speedup_vs_cuda": 1.00460633333333,
            "speedup_vs_flaggems": 0.8807396802619633
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\nMAX_DIMS = 8\nBLOCK_SIZE = 1024\n\n\n@triton.jit\ndef maximum_kernel(\n    a_ptr, b_ptr, out_ptr,\n    n_elements,\n    s0, s1, s2, s3, s4, s5, s6, s7,         # shape dims\n    sa0, sa1, sa2, sa3, sa4, sa5, sa6, sa7, # a strides\n    sb0, sb1, sb2, sb3, sb4, sb5, sb6, sb7, # b strides\n    so0, so1, so2, so3, so4, so5, so6, so7, # out strides\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Use int64 for address calculations\n    li = offsets.to(tl.int64)\n\n    # Compute multi-dimensional indices from linear index (row-major: last dim fastest)\n    i7 = li % s7; li = li // s7\n    i6 = li % s6; li = li // s6\n    i5 = li % s5; li = li // s5\n    i4 = li % s4; li = li // s4\n    i3 = li % s3; li = li // s3\n    i2 = li % s2; li = li // s2\n    i1 = li % s1; li = li // s1\n    i0 = li % s0; li = li // s0\n\n    # Compute element offsets for each tensor using strides (in elements)\n    off_a = (i0 * sa0 + i1 * sa1 + i2 * sa2 + i3 * sa3 +\n             i4 * sa4 + i5 * sa5 + i6 * sa6 + i7 * sa7)\n    off_b = (i0 * sb0 + i1 * sb1 + i2 * sb2 + i3 * sb3 +\n             i4 * sb4 + i5 * sb5 + i6 * sb6 + i7 * sb7)\n    off_o = (i0 * so0 + i1 * so1 + i2 * so2 + i3 * so3 +\n             i4 * so4 + i5 * so5 + i6 * so6 + i7 * so7)\n\n    a_vals = tl.load(a_ptr + off_a, mask=mask, other=0)\n    b_vals = tl.load(b_ptr + off_b, mask=mask, other=0)\n    out_vals = tl.maximum(a_vals, b_vals)\n    tl.store(out_ptr + off_o, out_vals, mask=mask)\n\n\ndef _as_tensor_on_device(x, device, dtype=None):\n    if torch.is_tensor(x):\n        return x.to(device=device, dtype=dtype) if (dtype is not None and x.dtype != dtype) or (x.device != device) else x\n    return torch.tensor(x, device=device, dtype=dtype)\n\n\ndef _broadcast_to_common(a, b):\n    a_b, b_b = torch.broadcast_tensors(a, b)\n    return a_b, b_b\n\n\ndef _pad_shape_strides(shape, strides):\n    # Ensure shape dims are at least 1 to avoid div by zero\n    shape_list = list(shape)\n    strides_list = list(strides)\n    nd = len(shape_list)\n    assert nd <= MAX_DIMS\n    shape_list = shape_list + [1] * (MAX_DIMS - nd)\n    strides_list = strides_list + [0] * (MAX_DIMS - nd)\n    # Triton expects integers\n    shape_list = [int(s) for s in shape_list]\n    strides_list = [int(s) for s in strides_list]\n    return shape_list, strides_list\n\n\ndef _launch_maximum_kernel(a, b, out):\n    # Assumes a and b are broadcastable and already cast to out.dtype and on same device\n    a_b, b_b = _broadcast_to_common(a, b)\n    # Make inputs contiguous to avoid negative/irregular strides complications\n    # Broadcasting uses 0-stride for broadcasted dims; keeping 0-stride is fine\n    # but handle potential negative/non-standard strides by materializing.\n    if any(s < 0 for s in a_b.stride()):\n        a_b = a_b.contiguous()\n    if any(s < 0 for s in b_b.stride()):\n        b_b = b_b.contiguous()\n\n    out_shape = a_b.shape  # == b_b.shape\n    n_elements = int(a_b.numel())\n    if n_elements == 0:\n        return\n\n    # Prepare shape and strides for kernel\n    shp, sa = _pad_shape_strides(out_shape, a_b.stride())\n    _, sb = _pad_shape_strides(out_shape, b_b.stride())\n    _, so = _pad_shape_strides(out_shape, out.stride())\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    maximum_kernel[grid](\n        a_b, b_b, out,\n        n_elements,\n        shp[0], shp[1], shp[2], shp[3], shp[4], shp[5], shp[6], shp[7],\n        sa[0], sa[1], sa[2], sa[3], sa[4], sa[5], sa[6], sa[7],\n        sb[0], sb[1], sb[2], sb[3], sb[4], sb[5], sb[6], sb[7],\n        so[0], so[1], so[2], so[3], so[4], so[5], so[6], so[7],\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n\ndef maximum(a, b):\n    # Determine device\n    dev = None\n    if torch.is_tensor(a):\n        dev = a.device\n    if torch.is_tensor(b):\n        dev = b.device if dev is None else dev\n    if dev is None or dev.type != 'cuda':\n        raise ValueError(\"maximum expects at least one CUDA tensor as input\")\n\n    # Determine result dtype per PyTorch promotion rules\n    res_dtype = torch.result_type(a, b)\n    a_t = _as_tensor_on_device(a, dev, dtype=res_dtype)\n    b_t = _as_tensor_on_device(b, dev, dtype=res_dtype)\n\n    # Broadcast to determine output shape\n    a_b, b_b = _broadcast_to_common(a_t, b_t)\n    out = torch.empty(a_b.shape, device=dev, dtype=res_dtype)\n\n    # If out has negative strides or is non-contiguous, compute into a contiguous buffer then copy\n    if not out.is_contiguous() or any(s < 0 for s in out.stride()):\n        out_buf = torch.empty_like(out, memory_format=torch.contiguous_format)\n        _launch_maximum_kernel(a_t, b_t, out_buf)\n        out.copy_(out_buf)\n    else:\n        _launch_maximum_kernel(a_t, b_t, out)\n\n    return out\n\n\ndef maximum_out(a, b, out):\n    if not torch.is_tensor(out):\n        raise TypeError(\"out must be a torch.Tensor\")\n    if out.device.type != 'cuda':\n        raise ValueError(\"out tensor must be on CUDA device\")\n\n    dev = out.device\n\n    # Cast inputs to out dtype (following typical .out behavior)\n    a_t = _as_tensor_on_device(a, dev, dtype=out.dtype)\n    b_t = _as_tensor_on_device(b, dev, dtype=out.dtype)\n\n    # Validate/broadcast shape against out\n    a_b, b_b = _broadcast_to_common(a_t, b_t)\n    if tuple(a_b.shape) != tuple(out.shape):\n        raise ValueError(f\"out shape {tuple(out.shape)} is not broadcast-compatible with inputs shape {tuple(a_b.shape)}\")\n\n    # If out has negative strides or is non-contiguous, compute into a contiguous buffer then copy\n    if not out.is_contiguous() or any(s < 0 for s in out.stride()):\n        out_buf = torch.empty_like(out, memory_format=torch.contiguous_format)\n        _launch_maximum_kernel(a_t, b_t, out_buf)\n        out.copy_(out_buf)\n    else:\n        _launch_maximum_kernel(a_t, b_t, out)\n\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.8807396802619633,
        "meets_threshold": false
      },
      "isinf": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.7481864483922045,
            "flaggems_speedup_vs_cuda": 2.043888,
            "speedup_vs_flaggems": 0.8553239944616361
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef isinf(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    inv = 1 / x\n    eq_zero = inv == 0\n    ne_zero = x != 0\n    res = eq_zero & ne_zero\n\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\n_isinf_kernel = isinf\n\n\ndef isinf(*args, **kwargs):\n    if len(args) == 0:\n        raise TypeError(\"isinf() missing required argument 'input' (pos 1)\")\n    x = args[0]\n    out = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"isinf(): input must be a torch.Tensor\")\n\n    if not x.is_cuda:\n        raise AssertionError(\"isinf(): input tensor must be on CUDA device for Triton kernel\")\n\n    if x.is_complex():\n        real_view = torch.view_as_real(x).contiguous()\n        bool_view = torch.empty_like(real_view, dtype=torch.bool)\n        n_elements = real_view.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _isinf_kernel[grid](real_view, bool_view, n_elements, BLOCK_SIZE=1024)\n        result = bool_view.any(dim=-1)\n        if out is not None:\n            if not isinstance(out, torch.Tensor):\n                raise TypeError(\"isinf(): out must be a torch.Tensor\")\n            if out.dtype != torch.bool:\n                raise TypeError(\"isinf(): out tensor must have dtype torch.bool\")\n            if out.shape != x.shape:\n                raise ValueError(\"isinf(): out tensor must have the same shape as input\")\n            out.copy_(result)\n            return out\n        return result\n\n    if x.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        input_contig = x.contiguous()\n        output = out\n        if output is None:\n            output = torch.empty_like(input_contig, dtype=torch.bool)\n        else:\n            if not isinstance(output, torch.Tensor):\n                raise TypeError(\"isinf(): out must be a torch.Tensor\")\n            if output.dtype != torch.bool:\n                raise TypeError(\"isinf(): out tensor must have dtype torch.bool\")\n            if output.shape != x.shape:\n                raise ValueError(\"isinf(): out tensor must have the same shape as input\")\n        n_elements = input_contig.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _isinf_kernel[grid](input_contig, output, n_elements, BLOCK_SIZE=1024)\n        return output\n\n    # Non-floating types: per PyTorch, return all-False tensor of the same shape.\n    result = torch.zeros_like(x, dtype=torch.bool)\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"isinf(): out must be a torch.Tensor\")\n        if out.dtype != torch.bool:\n            raise TypeError(\"isinf(): out tensor must have dtype torch.bool\")\n        if out.shape != x.shape:\n            raise ValueError(\"isinf(): out tensor must have the same shape as input\")\n        out.copy_(result)\n        return out\n    return result\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.8553239944616361,
        "meets_threshold": false
      },
      "clamp": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.8561091887661121,
            "flaggems_speedup_vs_cuda": 1.01396866666667,
            "speedup_vs_flaggems": 0.8443152307462254
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef clamp_kernel(\n    x_ptr,\n    out_ptr,\n    n_elements,\n    min_ptr,  # pointer to min tensor (broadcasted contiguous) or scalar 0-dim tensor\n    max_ptr,  # pointer to max tensor (broadcasted contiguous) or scalar 0-dim tensor\n    min_stride,  # 1 if min_ptr is a contiguous tensor matching x, 0 if scalar, ignored if HAS_MIN=False\n    max_stride,  # 1 if max_ptr is a contiguous tensor matching x, 0 if scalar, ignored if HAS_MAX=False\n    HAS_MIN: tl.constexpr,\n    HAS_MAX: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    if HAS_MIN:\n        min_vals = tl.load(min_ptr + offsets * min_stride, mask=mask)\n        x = tl.maximum(x, min_vals)\n    if HAS_MAX:\n        max_vals = tl.load(max_ptr + offsets * max_stride, mask=mask)\n        x = tl.minimum(x, max_vals)\n    tl.store(out_ptr + offsets, x, mask=mask)\n\n\ndef _clamp_impl(*args, **kwargs):\n    # Parse arguments similar to torch.clamp(input, min=None, max=None, *, out=None)\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get(\"input\", None)\n    if x is None:\n        raise TypeError(\"clamp: missing required argument 'input'\")\n\n    min_arg = kwargs[\"min\"] if \"min\" in kwargs else (args[1] if len(args) >= 2 else None)\n    max_arg = kwargs[\"max\"] if \"max\" in kwargs else (args[2] if len(args) >= 3 else None)\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"clamp: 'input' must be a torch.Tensor\")\n    if not x.is_cuda:\n        raise ValueError(\"This Triton implementation requires CUDA tensors.\")\n\n    # Make input contiguous and flattened for linear indexing\n    x_c = x.contiguous()\n    x_flat = x_c.view(-1)\n    n_elements = x_flat.numel()\n\n    has_min = min_arg is not None\n    has_max = max_arg is not None\n\n    # Prepare min pointer and stride\n    min_ptr = x_flat  # placeholder\n    max_ptr = x_flat  # placeholder\n    min_stride = 0\n    max_stride = 0\n\n    # Keep references to prevent GC\n    min_buf = None\n    max_buf = None\n\n    if has_min:\n        if torch.is_tensor(min_arg):\n            min_buf = min_arg.to(device=x.device, dtype=x.dtype)\n            if min_buf.shape != x.shape:\n                min_buf = torch.broadcast_to(min_buf, x.shape).contiguous()\n            else:\n                min_buf = min_buf.contiguous()\n            min_ptr = min_buf.view(-1)\n            min_stride = 1\n        else:\n            # scalar path: use 0-dim tensor and stride 0 broadcasting within kernel\n            min_buf = torch.tensor(min_arg, device=x.device, dtype=x.dtype)\n            min_ptr = min_buf\n            min_stride = 0\n\n    if has_max:\n        if torch.is_tensor(max_arg):\n            max_buf = max_arg.to(device=x.device, dtype=x.dtype)\n            if max_buf.shape != x.shape:\n                max_buf = torch.broadcast_to(max_buf, x.shape).contiguous()\n            else:\n                max_buf = max_buf.contiguous()\n            max_ptr = max_buf.view(-1)\n            max_stride = 1\n        else:\n            # scalar path: use 0-dim tensor and stride 0 broadcasting within kernel\n            max_buf = torch.tensor(max_arg, device=x.device, dtype=x.dtype)\n            max_ptr = max_buf\n            max_stride = 0\n\n    # Prepare output\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.dtype != x.dtype:\n            raise TypeError(\"out must have the same dtype as input\")\n        if out.device != x.device:\n            raise TypeError(\"out must be on the same device as input\")\n        if out.shape != x.shape:\n            raise ValueError(\"out must have the same shape as input\")\n        out_c = out.contiguous()\n    else:\n        out_c = torch.empty_like(x_c)\n\n    out_flat = out_c.view(-1)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    clamp_kernel[grid](\n        x_flat,\n        out_flat,\n        n_elements,\n        min_ptr,\n        max_ptr,\n        int(min_stride),\n        int(max_stride),\n        HAS_MIN=bool(has_min),\n        HAS_MAX=bool(has_max),\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    # If out provided and non-contiguous, copy back\n    if out is not None:\n        if not out.is_contiguous():\n            out.copy_(out_c)\n        return out\n    return out_c.view_as(x)\n\n\ndef clamp(*args, **kwargs):\n    return _clamp_impl(*args, **kwargs)\n\n\ndef clamp_Tensor(*args, **kwargs):\n    return _clamp_impl(*args, **kwargs)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.8443152307462254,
        "meets_threshold": false
      },
      "aten::masked_select": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 1.2275522765056512,
            "flaggems_speedup_vs_cuda": 1.574177,
            "speedup_vs_flaggems": 0.7798057502464153
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _masked_select_count_kernel(\n    mask_ptr,         # int32* flattened mask (0/1)\n    n_elements,       # int32 number of elements\n    counts_ptr,       # int32* per-block counts\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    in_bounds = offsets < n_elements\n\n    flags = tl.load(mask_ptr + offsets, mask=in_bounds, other=0)  # int32 0/1\n    block_count = tl.sum(flags, axis=0)\n    tl.store(counts_ptr + pid, block_count)\n\n\n@triton.jit\ndef _masked_select_scatter_kernel(\n    input_ptr,         # * input data (flattened, contiguous)\n    mask_ptr,          # int32* flattened mask (0/1)\n    block_offsets_ptr, # int32* per-block exclusive offsets\n    output_ptr,        # * output data\n    n_elements,        # int32 number of elements\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    in_bounds = offsets < n_elements\n\n    flags = tl.load(mask_ptr + offsets, mask=in_bounds, other=0)  # int32\n    # Compute local exclusive positions for true elements\n    inclusive = tl.cumsum(flags, axis=0)\n    local_pos = inclusive - 1  # valid only where flags == 1\n\n    base = tl.load(block_offsets_ptr + pid)  # int32\n    write_idx = base + local_pos\n\n    mstore = in_bounds & (flags != 0)\n    vals = tl.load(input_ptr + offsets, mask=mstore, other=0)\n    tl.store(output_ptr + write_idx, vals, mask=mstore)\n\n\ndef _prepare_broadcast_flatten(input: torch.Tensor, mask: torch.Tensor):\n    # Broadcast input and mask to a common shape\n    bshape = torch.broadcast_shapes(tuple(input.shape), tuple(mask.shape))\n    inp_b = input.expand(bshape)\n    msk_b = mask.to(torch.bool).expand(bshape)\n\n    # Make contiguous flattened views\n    inp_flat = inp_b.contiguous().view(-1)\n    msk_flat_bool = msk_b.contiguous().view(-1)\n    # Convert mask to int32 (0/1) for kernels\n    msk_flat_i32 = msk_flat_bool.to(torch.int32)\n    return inp_flat, msk_flat_i32\n\n\ndef masked_select(input: torch.Tensor, mask: torch.Tensor):\n    inp_flat, msk_flat_i32 = _prepare_broadcast_flatten(input, mask)\n    device = inp_flat.device\n    assert msk_flat_i32.device == device, \"input and mask must be on the same device\"\n\n    n_elements = inp_flat.numel()\n    if n_elements == 0:\n        return torch.empty(0, dtype=input.dtype, device=device)\n\n    BLOCK_SIZE = 1024\n    num_blocks = triton.cdiv(n_elements, BLOCK_SIZE)\n\n    counts = torch.empty(num_blocks, dtype=torch.int32, device=device)\n    grid = (num_blocks,)\n    _masked_select_count_kernel[grid](msk_flat_i32, n_elements, counts, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Compute per-block exclusive offsets and total number of selected elements\n    if num_blocks == 1:\n        block_offsets = torch.zeros(1, dtype=torch.int32, device=device)\n        total_selected = int(counts[0].item())\n    else:\n        prefix = torch.cumsum(counts, dim=0)\n        block_offsets = torch.empty_like(counts)\n        block_offsets[0] = 0\n        block_offsets[1:] = prefix[:-1]\n        total_selected = int(prefix[-1].item())\n\n    output = torch.empty(total_selected, dtype=input.dtype, device=device)\n    _masked_select_scatter_kernel[grid](\n        inp_flat, msk_flat_i32, block_offsets, output, n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n    return output\n\n\ndef masked_select_out(input: torch.Tensor, mask: torch.Tensor, out: torch.Tensor):\n    inp_flat, msk_flat_i32 = _prepare_broadcast_flatten(input, mask)\n    device = inp_flat.device\n    assert msk_flat_i32.device == device, \"input and mask must be on the same device\"\n    if out.device != device:\n        raise RuntimeError(\"out tensor must be on the same device as input\")\n\n    n_elements = inp_flat.numel()\n    if n_elements == 0:\n        out.resize_(0)\n        return out\n\n    BLOCK_SIZE = 1024\n    num_blocks = triton.cdiv(n_elements, BLOCK_SIZE)\n\n    counts = torch.empty(num_blocks, dtype=torch.int32, device=device)\n    grid = (num_blocks,)\n    _masked_select_count_kernel[grid](msk_flat_i32, n_elements, counts, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Compute per-block exclusive offsets and total number of selected elements\n    if num_blocks == 1:\n        block_offsets = torch.zeros(1, dtype=torch.int32, device=device)\n        total_selected = int(counts[0].item())\n    else:\n        prefix = torch.cumsum(counts, dim=0)\n        block_offsets = torch.empty_like(counts)\n        block_offsets[0] = 0\n        block_offsets[1:] = prefix[:-1]\n        total_selected = int(prefix[-1].item())\n\n    if out.dtype != input.dtype:\n        raise RuntimeError(\"out tensor dtype must match input dtype\")\n    out.resize_(total_selected)\n\n    _masked_select_scatter_kernel[grid](\n        inp_flat, msk_flat_i32, block_offsets, out, n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.7798057502464153,
        "meets_threshold": false
      },
      "exponential_": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.600086892832044,
            "flaggems_speedup_vs_cuda": 0.814171,
            "speedup_vs_flaggems": 0.7370526496670159
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef exponential_(out_ptr,  # Pointer to output tensor (in-place)\n                 n_elements,  # Number of elements\n                 lambd,  # Rate parameter\n                 seed,  # RNG seed\n                 BLOCK_SIZE: tl.constexpr,  # Elements per program\n                 TO_DTYPE: tl.constexpr  # Target dtype\n                 ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Uniform random in [0, 1)\n    u = tl.rand(seed, offsets)\n    # Avoid u being exactly 1 to prevent inf from log(0)\n    u = tl.minimum(u, 1.0 - 1e-7)\n\n    # Inverse CDF: X = -log(1 - U) / lambd\n    vals = -tl.log(1.0 - u) / lambd\n    vals = tl.cast(vals, TO_DTYPE)\n\n    tl.store(out_ptr + offsets, vals, mask=mask)\n\n\n_exponential_kernel = exponential_\n\n\ndef exponential_(*args, **kwargs):\n    x = args[0] if len(args) >= 1 else kwargs.get('input', None)\n    if x is None or not isinstance(x, torch.Tensor):\n        raise ValueError(\"exponential_ requires a torch.Tensor as the first positional argument.\")\n    lambd = kwargs.get('lambd', None)\n    if lambd is None and len(args) >= 2:\n        lambd = args[1]\n    if lambd is None:\n        lambd = 1.0\n    generator = kwargs.get('generator', None)\n    if generator is None and len(args) >= 3:\n        generator = args[2]\n\n    if x.device.type != 'cuda':\n        raise ValueError(\"Triton exponential_ requires the input tensor to be on CUDA device.\")\n    allowed_dtypes = {torch.float16, torch.bfloat16, torch.float32, torch.float64}\n    if x.dtype not in allowed_dtypes:\n        raise ValueError(f\"Unsupported dtype {x.dtype}. Supported dtypes are: {allowed_dtypes}\")\n    if lambd <= 0:\n        raise ValueError(\"lambd must be a positive number.\")\n\n    dtype_map = {\n        torch.float16: tl.float16,\n        torch.bfloat16: tl.bfloat16,\n        torch.float32: tl.float32,\n        torch.float64: tl.float64,\n    }\n\n    # Seed selection\n    if isinstance(generator, torch.Generator):\n        seed = int(generator.initial_seed())\n    else:\n        seed = int(torch.randint(0, 2**31 - 1, (), device='cpu').item())\n\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _exponential_kernel[grid](\n        x, n_elements, float(lambd), seed,\n        BLOCK_SIZE=1024, TO_DTYPE=dtype_map[x.dtype]\n    )\n    return x\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.7370526496670159,
        "meets_threshold": false
      },
      "resolve_neg": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.7133122000950739,
            "flaggems_speedup_vs_cuda": 1.036356,
            "speedup_vs_flaggems": 0.6882887734476124
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef resolve_neg(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = -x\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\n# Preserve reference to the Triton kernel before defining the Python wrapper with the same name\nresolve_neg_kernel = resolve_neg\n\n\ndef resolve_neg(*args, **kwargs):\n    # Extract input tensor\n    if len(args) > 0:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    else:\n        raise ValueError(\"resolve_neg expects a single input tensor as the first positional arg or 'input' kwarg.\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"resolve_neg expects a torch.Tensor as input.\")\n\n    # If not on CUDA or unsupported dtype, fall back to torch.resolve_neg\n    supported_dtypes = {\n        torch.float16,\n        torch.bfloat16,\n        torch.float32,\n        torch.float64,\n        torch.int8,\n        torch.int16,\n        torch.int32,\n        torch.int64,\n    }\n    if (not x.is_cuda) or (x.dtype not in supported_dtypes):\n        return torch.resolve_neg(x)\n\n    x_contig = x.contiguous()\n    output = torch.empty_like(x_contig)\n\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    resolve_neg_kernel[grid](x_contig, output, n_elements, BLOCK_SIZE=1024)\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6882887734476124,
        "meets_threshold": false
      },
      "vstack": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.7467980647259109,
            "flaggems_speedup_vs_cuda": 1.097169,
            "speedup_vs_flaggems": 0.6806591005815065
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef vstack(out_ptr, in_ptr, start_idx, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr + offsets, mask=mask)\n    tl.store(out_ptr + start_idx + offsets, x, mask=mask)\n\n\n_vstack_kernel = vstack\n\n\ndef vstack(*args, **kwargs):\n    # Parse inputs: allow either a single sequence or multiple positional tensors\n    if len(args) == 1 and isinstance(args[0], (list, tuple)):\n        tensors = list(args[0])\n    else:\n        tensors = list(args)\n\n    out = kwargs.pop('out', None)\n    if kwargs:\n        # No other kwargs are supported in this implementation\n        raise TypeError(f\"Unsupported keyword arguments: {list(kwargs.keys())}\")\n\n    if len(tensors) == 0:\n        raise ValueError(\"vstack expects at least one tensor\")\n\n    # Validate inputs: dtype/device match, tensors type\n    device = tensors[0].device\n    dtype = tensors[0].dtype\n    if device.type != 'cuda':\n        raise ValueError(\"Triton vstack requires CUDA tensors (device.type == 'cuda').\")\n    for t in tensors:\n        if not isinstance(t, torch.Tensor):\n            raise TypeError(\"All inputs must be torch.Tensor\")\n        if t.device != device:\n            raise ValueError(\"All tensors must be on the same device\")\n        if t.dtype != dtype:\n            raise ValueError(\"All tensors must have the same dtype\")\n\n    # Normalize shapes: 1D tensors become (1, N), others unchanged\n    def normalized_shape(t: torch.Tensor):\n        if t.dim() == 1:\n            return (1, t.shape[0])\n        return tuple(t.shape)\n\n    base_rest = None\n    total_dim0 = 0\n    for t in tensors:\n        ns = normalized_shape(t)\n        if base_rest is None:\n            base_rest = ns[1:]\n        else:\n            if ns[1:] != base_rest:\n                raise ValueError(\"All tensors must have matching shape after the first dimension for vstack\")\n        total_dim0 += ns[0]\n\n    out_shape = (total_dim0,) + base_rest\n\n    # Allocate or validate output\n    if out is None:\n        output = torch.empty(out_shape, dtype=dtype, device=device)\n    else:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.device != device:\n            raise ValueError(\"out tensor must be on the same device as inputs\")\n        if out.dtype != dtype:\n            raise ValueError(\"out tensor dtype must match inputs\")\n        if tuple(out.shape) != out_shape:\n            raise ValueError(f\"out tensor has incorrect shape: expected {out_shape}, got {tuple(out.shape)}\")\n        if not out.is_contiguous():\n            raise ValueError(\"out tensor must be contiguous\")\n        output = out\n\n    # Ensure inputs are contiguous\n    contiguous_inputs = [t.contiguous() for t in tensors]\n\n    # Fast return for empty result\n    if output.numel() == 0:\n        return output\n\n    BLOCK_SIZE = 1024\n    offset_elems = 0\n    for t in contiguous_inputs:\n        n_elements = t.numel()\n        if n_elements == 0:\n            continue\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _vstack_kernel[grid](\n            output,\n            t,\n            offset_elems,\n            n_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        offset_elems += n_elements\n\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6806591005815065,
        "meets_threshold": false
      },
      "sum": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.7420485832050568,
            "flaggems_speedup_vs_cuda": 1.09361233333333,
            "speedup_vs_flaggems": 0.6785298232174221
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _sum_reduce_2d_kernel(\n    x_ptr,            # *Pointer* to input 2D tensor (M, N)\n    out_ptr,          # *Pointer* to output 1D tensor (M,)\n    M,                # number of rows\n    N,                # number of cols (reduction dimension)\n    stride_xm,        # row stride of x (in elements)\n    stride_xn,        # col stride of x (in elements)\n    stride_outm,      # stride of out (in elements)\n    ACC_TYPE: tl.constexpr,  # accumulation dtype (e.g., tl.float32, tl.int64)\n    BLOCK_N: tl.constexpr,   # tile size along reduction dimension\n):\n    m = tl.program_id(axis=0)\n    offs_n = tl.arange(0, BLOCK_N)\n\n    # Initialize vectorized accumulator; will be reduced across lanes at the end\n    acc = tl.zeros([BLOCK_N], dtype=ACC_TYPE)\n\n    # Guard for row index\n    row_valid = m < M\n\n    n = 0\n    while n < N:\n        cols = n + offs_n\n        mask = row_valid & (cols < N)\n        x = tl.load(x_ptr + m * stride_xm + cols * stride_xn, mask=mask, other=0)\n        x = x.to(ACC_TYPE)\n        # Accumulate, zero out masked lanes to avoid garbage\n        x = tl.where(mask, x, tl.zeros_like(x))\n        acc += x\n        n += BLOCK_N\n\n    res = tl.sum(acc, axis=0)\n    tl.store(out_ptr + m * stride_outm, res, mask=row_valid)\n\n\ndef _torch_dtype_to_triton(dtype: torch.dtype):\n    if dtype == torch.float16:\n        return tl.float16\n    if dtype == torch.bfloat16:\n        return tl.bfloat16\n    if dtype == torch.float32:\n        return tl.float32\n    if dtype == torch.int32:\n        return tl.int32\n    if dtype == torch.int64:\n        return tl.int64\n    # Fallback not supported\n    return None\n\n\ndef _acc_dtype_for(dtype: torch.dtype):\n    # Choose a safe accumulation dtype\n    if dtype in (torch.float16, torch.bfloat16, torch.float32):\n        return tl.float32\n    if dtype == torch.int64:\n        return tl.int64\n    if dtype == torch.int32:\n        return tl.int32\n    # Default: try to map and use same\n    td = _torch_dtype_to_triton(dtype)\n    return td\n\n\ndef _normalize_dims(dim, ndim):\n    if dim is None:\n        return list(range(ndim))\n    if isinstance(dim, int):\n        dim = [dim]\n    else:\n        dim = list(dim)\n    # Normalize negatives and remove duplicates preserving order\n    seen = set()\n    dims = []\n    for d in dim:\n        if d < 0:\n            d += ndim\n        if d < 0 or d >= ndim:\n            raise IndexError(\"Dimension out of range\")\n        if d not in seen:\n            seen.add(d)\n            dims.append(d)\n    return dims\n\n\ndef _parse_sum_args(*args, **kwargs):\n    # Expect patterns like:\n    # sum(input)\n    # sum(input, dtype=torch.float32)\n    # sum(input, dim=..., keepdim=False, dtype=...)\n    # sum(input, dim, keepdim=False)\n    if len(args) == 0 and 'input' not in kwargs:\n        raise TypeError(\"sum(): missing required argument 'input'\")\n\n    x = args[0] if len(args) > 0 else kwargs['input']\n    dtype = kwargs.get('dtype', None)\n    keepdim = kwargs.get('keepdim', False)\n    dim = kwargs.get('dim', None)\n\n    # Positional handling for dim / dtype / keepdim\n    # Cases:\n    #  - sum(x, dtype)\n    #  - sum(x, dim, keepdim)\n    if len(args) >= 2 and dim is None:\n        arg1 = args[1]\n        # If second arg is a dtype\n        if isinstance(arg1, torch.dtype):\n            dtype = arg1\n        else:\n            dim = arg1\n            if len(args) >= 3:\n                keepdim = args[2]\n\n    return x, dim, keepdim, dtype\n\n\ndef _sum_triton_impl(x: torch.Tensor, dim=None, keepdim: bool = False, dtype: torch.dtype = None) -> torch.Tensor:\n    if not x.is_cuda:\n        raise RuntimeError(\"This Triton sum implementation supports CUDA tensors only.\")\n    device = x.device\n\n    # Apply dtype cast if requested\n    if dtype is not None and dtype != x.dtype:\n        x = x.to(dtype)\n\n    in_dtype = x.dtype\n    out_dtype = x.dtype\n\n    # Handle empty tensor early\n    if x.numel() == 0:\n        ndim = x.ndim\n        reduce_dims = _normalize_dims(dim, ndim) if dim is not None else list(range(ndim))\n        if len(reduce_dims) == 0:\n            # No reduction, just return (possibly cast) x\n            return x.clone()\n        # Build output shape\n        if keepdim:\n            out_shape = [1 if i in reduce_dims else x.shape[i] for i in range(ndim)]\n        else:\n            out_shape = [x.shape[i] for i in range(ndim) if i not in reduce_dims]\n        if len(out_shape) == 0:\n            out = torch.zeros((), device=device, dtype=out_dtype)\n        else:\n            out = torch.zeros(out_shape, device=device, dtype=out_dtype)\n        return out\n\n    ndim = x.ndim\n    reduce_dims = _normalize_dims(dim, ndim) if dim is not None else list(range(ndim))\n\n    # If no reduction dims specified (e.g., dim=[]), return x (identity)\n    if len(reduce_dims) == 0:\n        return x.clone()\n\n    keep_dims = [i for i in range(ndim) if i not in reduce_dims]\n    perm = keep_dims + reduce_dims\n\n    # Permute to [keep..., reduce...] and make contiguous\n    xp = x.permute(perm).contiguous()\n    sizes_keep = [x.shape[i] for i in keep_dims]\n    sizes_reduce = [x.shape[i] for i in reduce_dims]\n\n    M = 1\n    for s in sizes_keep:\n        M *= s\n    N = 1\n    for s in sizes_reduce:\n        N *= s\n\n    # Create 2D view [M, N]\n    x2 = xp.view(M, N)\n\n    # Prepare output buffer [M]\n    out_tmp = torch.empty((M,), device=device, dtype=out_dtype)\n\n    # Choose accumulation dtype\n    ACC_TYPE = _acc_dtype_for(out_dtype)\n    if ACC_TYPE is None:\n        raise NotImplementedError(f\"Dtype {out_dtype} not supported by this Triton sum kernel.\")\n\n    # Launch kernel\n    def grid(meta):\n        return (M,)\n\n    _sum_reduce_2d_kernel[grid](\n        x2, out_tmp,\n        M, N,\n        N, 1, 1,  # strides in elements for contiguous [M, N] and contiguous output\n        ACC_TYPE=ACC_TYPE,\n        BLOCK_N=1024,\n        num_warps=4,\n    )\n\n    # Reshape back\n    if keepdim:\n        out_shape_perm = sizes_keep + [1] * len(sizes_reduce)\n        y_perm = out_tmp.view(out_shape_perm)\n        # Inverse permutation to restore original order with kept dims and singleton reduced dims\n        inv = [0] * len(perm)\n        for i, p in enumerate(perm):\n            inv[p] = i\n        y = y_perm.permute(inv)\n    else:\n        if len(sizes_keep) == 0:\n            y = out_tmp.view(())\n        else:\n            y = out_tmp.view(sizes_keep)\n\n    return y\n\n\ndef sum(*args, **kwargs):\n    x, dim, keepdim, dtype = _parse_sum_args(*args, **kwargs)\n    return _sum_triton_impl(x, dim=dim, keepdim=keepdim, dtype=dtype)\n\n\ndef sum_dim_IntList(*args, **kwargs):\n    x, dim, keepdim, dtype = _parse_sum_args(*args, **kwargs)\n    return _sum_triton_impl(x, dim=dim, keepdim=keepdim, dtype=dtype)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6785298232174221,
        "meets_threshold": false
      },
      "logical_or": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.6390965301812909,
            "flaggems_speedup_vs_cuda": 0.982935333333333,
            "speedup_vs_flaggems": 0.650191837151672
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logical_or(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0)\n\n    # Since inputs are boolean (0/1 byte), bitwise OR yields correct result (0/1 byte).\n    result = x | y\n\n    tl.store(out_ptr + offsets, result, mask=mask)\n\n\n# Preserve reference to the kernel before defining the Python wrapper with the same name\nlogical_or_kernel = logical_or\n\n\ndef logical_or(*args, **kwargs):\n    # Parse inputs\n    if len(args) < 2:\n        raise TypeError(\"logical_or expected at least 2 positional arguments (got {})\".format(len(args)))\n    a, b = args[0], args[1]\n    out = kwargs.pop('out', None)\n    if kwargs:\n        # torch.logical_or doesn't accept additional kwargs beyond 'out'\n        unexpected = ', '.join(kwargs.keys())\n        raise TypeError(f\"logical_or() got unexpected keyword arguments: {unexpected}\")\n\n    # Determine device\n    device = None\n    if out is not None:\n        if not torch.is_tensor(out):\n            raise TypeError(\"out must be a torch.Tensor if provided\")\n        device = out.device\n    elif torch.is_tensor(a) and a.is_cuda:\n        device = a.device\n    elif torch.is_tensor(b) and b.is_cuda:\n        device = b.device\n    elif torch.cuda.is_available():\n        device = torch.device(f\"cuda:{torch.cuda.current_device()}\")\n    else:\n        raise RuntimeError(\"Triton kernel requires CUDA tensors. No CUDA device available.\")\n\n    # Convert inputs to tensors on the correct device\n    if not torch.is_tensor(a):\n        a = torch.as_tensor(a, device=device)\n    else:\n        if not a.is_cuda:\n            a = a.to(device)\n    if not torch.is_tensor(b):\n        b = torch.as_tensor(b, device=device)\n    else:\n        if not b.is_cuda:\n            b = b.to(device)\n\n    # Convert to boolean dtype\n    a = a.to(torch.bool)\n    b = b.to(torch.bool)\n\n    # Broadcast to common shape\n    a_b, b_b = torch.broadcast_tensors(a, b)\n\n    # Ensure contiguous for simple 1D indexing in the kernel\n    a_c = a_b.contiguous()\n    b_c = b_b.contiguous()\n\n    # Prepare output tensor\n    needs_copy_back = False\n    if out is not None:\n        if out.device != device:\n            raise ValueError(\"out tensor must be on the same CUDA device as inputs\")\n        if out.dtype != torch.bool:\n            raise TypeError(f\"out tensor must have dtype torch.bool, got {out.dtype}\")\n        if tuple(out.shape) != tuple(a_c.shape):\n            raise ValueError(f\"out tensor has incorrect shape {tuple(out.shape)}; expected {tuple(a_c.shape)}\")\n        if out.is_contiguous():\n            out_buf = out\n        else:\n            out_buf = torch.empty_like(a_c, dtype=torch.bool, device=device)\n            needs_copy_back = True\n    else:\n        out_buf = torch.empty_like(a_c, dtype=torch.bool, device=device)\n\n    n_elements = out_buf.numel()\n    if n_elements == 0:\n        # Handle empty tensors\n        if out is not None:\n            return out\n        return out_buf\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    logical_or_kernel[grid](a_c, b_c, out_buf, n_elements, BLOCK_SIZE=1024)\n\n    if needs_copy_back:\n        out.copy_(out_buf)\n        return out\n    return out_buf\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.650191837151672,
        "meets_threshold": false
      },
      "normal": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.0011819975911693,
            "flaggems_speedup_vs_cuda": 1.55982133333333,
            "speedup_vs_flaggems": 0.6418568435986503
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef normal_kernel(\n    mean_ptr,         # pointer to mean tensor (or dummy)\n    std_ptr,          # pointer to std tensor (or dummy)\n    out_ptr,          # pointer to output tensor\n    n_elements,       # total number of elements\n    seed,             # RNG seed (uint32)\n    mean_scalar,      # scalar mean if MEAN_IS_TENSOR == False\n    std_scalar,       # scalar std if STD_IS_TENSOR == False\n    MEAN_IS_TENSOR: tl.constexpr,\n    STD_IS_TENSOR: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load/broadcast mean\n    if MEAN_IS_TENSOR:\n        mean_vals = tl.load(mean_ptr + offsets, mask=mask, other=0.0).to(tl.float32)\n    else:\n        mean_vals = tl.zeros((BLOCK_SIZE,), dtype=tl.float32) + mean_scalar\n\n    # Load/broadcast std\n    if STD_IS_TENSOR:\n        std_vals = tl.load(std_ptr + offsets, mask=mask, other=0.0).to(tl.float32)\n    else:\n        std_vals = tl.zeros((BLOCK_SIZE,), dtype=tl.float32) + std_scalar\n\n    # Generate two independent uniform random numbers in (0, 1)\n    # Use two different seeds via simple mixing to decorrelate\n    u0 = tl.rand(seed, offsets)\n    u1 = tl.rand(seed ^ tl.uint32(0x9E3779B9), offsets)\n\n    # Box-Muller transform to obtain standard normal\n    # Avoid log(0) by nudging u0 away from 0 and 1\n    eps = 1e-7\n    u0 = u0 + eps\n    u1 = tl.where(u1 >= 1.0, 1.0 - eps, u1)\n    r = tl.sqrt(-2.0 * tl.log(u0))\n    theta = 6.283185307179586 * u1  # 2*pi\n    z = r * tl.cos(theta)  # standard normal\n\n    out_vals = mean_vals + z * std_vals\n    tl.store(out_ptr + offsets, out_vals, mask=mask)\n\n\ndef _get_seed_from_kwargs(kwargs):\n    generator = kwargs.get('generator', None)\n    seed = kwargs.get('seed', None)\n    if seed is None:\n        if generator is not None:\n            try:\n                seed = int(generator.initial_seed())\n            except Exception:\n                seed = int(torch.initial_seed())\n        else:\n            seed = int(torch.initial_seed())\n    return int(seed) & 0xFFFFFFFF\n\n\ndef _launch_normal_kernel(mean_tensor, std_tensor, out, mean_is_tensor, std_is_tensor, mean_scalar, std_scalar, seed):\n    n_elements = out.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    # Pass valid pointers even if they're not used in the kernel branch\n    mean_ptr = mean_tensor if mean_is_tensor else out\n    std_ptr = std_tensor if std_is_tensor else out\n    normal_kernel[grid](\n        mean_ptr,\n        std_ptr,\n        out,\n        n_elements,\n        seed,\n        float(mean_scalar),\n        float(std_scalar),\n        MEAN_IS_TENSOR=mean_is_tensor,\n        STD_IS_TENSOR=std_is_tensor,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n\ndef normal_Tensor_float(*args, **kwargs):\n    # mean: Tensor, std: float\n    if len(args) >= 2:\n        mean, std = args[0], args[1]\n    else:\n        mean = kwargs.get('mean', None)\n        std = kwargs.get('std', None)\n    assert isinstance(mean, torch.Tensor), \"mean must be a torch.Tensor\"\n    assert isinstance(std, (float, int)), \"std must be a float\"\n    seed = _get_seed_from_kwargs(kwargs)\n    mean_c = mean.contiguous()\n    out = torch.empty_like(mean_c)\n    _launch_normal_kernel(mean_c, out, out, True, False, 0.0, float(std), seed)\n    return out\n\n\ndef normal_float_Tensor(*args, **kwargs):\n    # mean: float, std: Tensor\n    if len(args) >= 2:\n        mean, std = args[0], args[1]\n    else:\n        mean = kwargs.get('mean', None)\n        std = kwargs.get('std', None)\n    assert isinstance(mean, (float, int)), \"mean must be a float\"\n    assert isinstance(std, torch.Tensor), \"std must be a torch.Tensor\"\n    seed = _get_seed_from_kwargs(kwargs)\n    std_c = std.contiguous()\n    out = torch.empty_like(std_c)\n    _launch_normal_kernel(out, std_c, out, False, True, float(mean), 0.0, seed)\n    return out\n\n\ndef normal_Tensor_Tensor(*args, **kwargs):\n    # mean: Tensor, std: Tensor\n    if len(args) >= 2:\n        mean, std = args[0], args[1]\n    else:\n        mean = kwargs.get('mean', None)\n        std = kwargs.get('std', None)\n    assert isinstance(mean, torch.Tensor), \"mean must be a torch.Tensor\"\n    assert isinstance(std, torch.Tensor), \"std must be a torch.Tensor\"\n\n    # Promote dtypes to a common dtype\n    promoted_dtype = torch.result_type(mean, std)\n    mean = mean.to(promoted_dtype)\n    std = std.to(promoted_dtype)\n\n    # Broadcast to common shape and materialize contiguous buffers\n    mean_b, std_b = torch.broadcast_tensors(mean, std)\n    mean_c = mean_b.contiguous()\n    std_c = std_b.contiguous()\n\n    seed = _get_seed_from_kwargs(kwargs)\n    out = torch.empty_like(mean_c)\n    _launch_normal_kernel(mean_c, std_c, out, True, True, 0.0, 0.0, seed)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6418568435986503,
        "meets_threshold": false
      },
      "logical_and": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.6337451468657319,
            "flaggems_speedup_vs_cuda": 0.999584333333333,
            "speedup_vs_flaggems": 0.6340086831416913
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logical_and(a_ptr, b_ptr, out_ptr, n_elements,\n                a_s0, a_s1, a_s2, a_s3, a_s4, a_s5, a_s6, a_s7,\n                b_s0, b_s1, b_s2, b_s3, b_s4, b_s5, b_s6, b_s7,\n                o_s0, o_s1, o_s2, o_s3, o_s4, o_s5, o_s6, o_s7,\n                sh0, sh1, sh2, sh3, sh4, sh5, sh6, sh7,\n                BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    rem = offsets\n    i7 = rem % sh7\n    rem = rem // sh7\n    i6 = rem % sh6\n    rem = rem // sh6\n    i5 = rem % sh5\n    rem = rem // sh5\n    i4 = rem % sh4\n    rem = rem // sh4\n    i3 = rem % sh3\n    rem = rem // sh3\n    i2 = rem % sh2\n    rem = rem // sh2\n    i1 = rem % sh1\n    rem = rem // sh1\n    i0 = rem % sh0\n\n    a_offsets = (i0 * a_s0 + i1 * a_s1 + i2 * a_s2 + i3 * a_s3 +\n                 i4 * a_s4 + i5 * a_s5 + i6 * a_s6 + i7 * a_s7)\n    b_offsets = (i0 * b_s0 + i1 * b_s1 + i2 * b_s2 + i3 * b_s3 +\n                 i4 * b_s4 + i5 * b_s5 + i6 * b_s6 + i7 * b_s7)\n    o_offsets = (i0 * o_s0 + i1 * o_s1 + i2 * o_s2 + i3 * o_s3 +\n                 i4 * o_s4 + i5 * o_s5 + i6 * o_s6 + i7 * o_s7)\n\n    a = tl.load(a_ptr + a_offsets, mask=mask, other=0)\n    b = tl.load(b_ptr + b_offsets, mask=mask, other=0)\n\n    out = (a != 0) & (b != 0)\n    tl.store(out_ptr + o_offsets, out, mask=mask)\n\n\nlogical_and_kernel = logical_and\n\n\ndef logical_and(*args, **kwargs):\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get('input', None)\n        y = kwargs.get('other', None)\n        if x is None or y is None:\n            raise ValueError(\"logical_and requires two inputs\")\n    out_kw = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor):\n        device = y.device if isinstance(y, torch.Tensor) else 'cuda'\n        x = torch.tensor(x, device=device)\n    if not isinstance(y, torch.Tensor):\n        device = x.device if isinstance(x, torch.Tensor) else 'cuda'\n        y = torch.tensor(y, device=device)\n\n    if x.device != y.device:\n        raise ValueError(\"Inputs must be on the same device\")\n    if not x.is_cuda:\n        raise ValueError(\"Triton kernels require CUDA tensors\")\n\n    xb, yb = torch.broadcast_tensors(x.bool(), y.bool())\n\n    if out_kw is None:\n        out = torch.empty_like(xb, dtype=torch.bool, device=xb.device)\n    else:\n        out = out_kw\n        if not isinstance(out, torch.Tensor):\n            raise ValueError(\"out must be a torch.Tensor\")\n        if out.device != xb.device:\n            raise ValueError(\"out tensor must be on the same device as inputs\")\n        if out.dtype != torch.bool:\n            raise ValueError(\"out tensor must have dtype torch.bool\")\n        if out.shape != xb.shape:\n            raise ValueError(\"out tensor has incorrect shape\")\n\n    MAX_DIMS = 8\n    shape = list(out.shape)\n    dims = len(shape)\n    if dims > MAX_DIMS:\n        raise ValueError(f\"logical_and Triton kernel supports up to {MAX_DIMS} dimensions\")\n    pad = MAX_DIMS - dims\n\n    def pad_list(lst, fill, front=True):\n        lst = list(lst)\n        if front:\n            return [fill] * pad + lst\n        return lst + [fill] * pad\n\n    shape_padded = pad_list(shape, 1, front=True)\n\n    a_strides = pad_list(xb.stride(), 0, front=True)\n    b_strides = pad_list(yb.stride(), 0, front=True)\n    o_strides = pad_list(out.stride(), 0, front=True)\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    logical_and_kernel[grid](\n        xb, yb, out,\n        n_elements,\n        a_strides[0], a_strides[1], a_strides[2], a_strides[3], a_strides[4], a_strides[5], a_strides[6], a_strides[7],\n        b_strides[0], b_strides[1], b_strides[2], b_strides[3], b_strides[4], b_strides[5], b_strides[6], b_strides[7],\n        o_strides[0], o_strides[1], o_strides[2], o_strides[3], o_strides[4], o_strides[5], o_strides[6], o_strides[7],\n        shape_padded[0], shape_padded[1], shape_padded[2], shape_padded[3],\n        shape_padded[4], shape_padded[5], shape_padded[6], shape_padded[7],\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6340086831416913,
        "meets_threshold": false
      },
      "where": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.6331244979984949,
            "flaggems_speedup_vs_cuda": 1.00231033333333,
            "speedup_vs_flaggems": 0.6316651409678143
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef where_kernel(cond_ptr, x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    cond_val = tl.load(cond_ptr + offsets, mask=mask, other=0)\n    x_val = tl.load(x_ptr + offsets, mask=mask)\n    y_val = tl.load(y_ptr + offsets, mask=mask)\n    select_mask = cond_val != 0\n    out_val = tl.where(select_mask, x_val, y_val)\n    tl.store(out_ptr + offsets, out_val, mask=mask)\n\n\ndef _get_arg(name, args, kwargs, idx):\n    if name in kwargs:\n        return kwargs[name]\n    if len(args) > idx:\n        return args[idx]\n    return None\n\n\ndef _ensure_cuda_device(*tensors_or_scalars):\n    for obj in tensors_or_scalars:\n        if isinstance(obj, torch.Tensor):\n            if obj.device.type != \"cuda\":\n                raise AssertionError(\"All tensors must be on CUDA device for Triton kernels.\")\n            return obj.device\n    raise AssertionError(\"At least one tensor (condition/self/other) must be a CUDA tensor.\")\n\n\ndef _broadcast_shape(*shapes):\n    # torch.broadcast_shapes is available in recent versions\n    return torch.broadcast_shapes(*shapes)\n\n\ndef _to_tensor(value, device, dtype):\n    if isinstance(value, torch.Tensor):\n        return value.to(device=device, dtype=dtype)\n    else:\n        return torch.as_tensor(value, device=device, dtype=dtype)\n\n\ndef _where_impl(condition, self_val, other_val, out=None):\n    device = _ensure_cuda_device(condition, self_val, other_val)\n\n    # Prepare condition as boolean and then uint8 for Triton\n    cond_tensor = condition\n    if not isinstance(cond_tensor, torch.Tensor):\n        cond_tensor = torch.as_tensor(cond_tensor, device=device, dtype=torch.bool)\n    else:\n        cond_tensor = cond_tensor.to(device=device, dtype=torch.bool)\n\n    # Determine result dtype based on self and other values\n    dtype_out = torch.result_type(\n        torch.as_tensor(self_val),\n        torch.as_tensor(other_val)\n    )\n\n    x_tensor = _to_tensor(self_val, device, dtype_out)\n    y_tensor = _to_tensor(other_val, device, dtype_out)\n\n    # Compute broadcasted output shape\n    out_shape = _broadcast_shape(cond_tensor.shape, x_tensor.shape, y_tensor.shape)\n\n    # Broadcast inputs to the same shape\n    bc_cond = torch.broadcast_to(cond_tensor, out_shape)\n    bc_x = torch.broadcast_to(x_tensor, out_shape)\n    bc_y = torch.broadcast_to(y_tensor, out_shape)\n\n    # Convert condition to uint8 for kernel consumption\n    bc_cond_u8 = bc_cond.to(torch.uint8)\n\n    # Ensure contiguous memory\n    bc_cond_u8 = bc_cond_u8.contiguous()\n    bc_x = bc_x.contiguous()\n    bc_y = bc_y.contiguous()\n\n    n_elements = bc_x.numel()\n\n    # Prepare output\n    if out is not None:\n        if out.device.type != \"cuda\":\n            raise AssertionError(\"Output tensor must be on CUDA device.\")\n        if out.dtype != dtype_out:\n            raise AssertionError(\"Output dtype must match result dtype.\")\n        if tuple(out.shape) != tuple(out_shape):\n            raise AssertionError(\"Output shape must match broadcasted shape.\")\n        out_contig = out if out.is_contiguous() else torch.empty_like(out, memory_format=torch.contiguous_format)\n    else:\n        out_contig = torch.empty(out_shape, device=device, dtype=dtype_out)\n\n    # Launch Triton kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    where_kernel[grid](\n        bc_cond_u8, bc_x, bc_y, out_contig, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    if out is not None and not out.is_contiguous():\n        out.copy_(out_contig)\n        return out\n    return out_contig\n\n\ndef where_self_out(*args, **kwargs):\n    condition = _get_arg('condition', args, kwargs, 0)\n    self_val = _get_arg('self', args, kwargs, 1)\n    other_val = _get_arg('other', args, kwargs, 2)\n    out = _get_arg('out', args, kwargs, 3)\n    return _where_impl(condition, self_val, other_val, out=out)\n\n\ndef where_self(*args, **kwargs):\n    condition = _get_arg('condition', args, kwargs, 0)\n    self_val = _get_arg('self', args, kwargs, 1)\n    other_val = _get_arg('other', args, kwargs, 2)\n    return _where_impl(condition, self_val, other_val, out=None)\n\n\ndef where_ScalarSelf(*args, **kwargs):\n    condition = _get_arg('condition', args, kwargs, 0)\n    self_scalar = _get_arg('self', args, kwargs, 1)\n    other_tensor = _get_arg('other', args, kwargs, 2)\n    return _where_impl(condition, self_scalar, other_tensor, out=None)\n\n\ndef where_ScalarOther(*args, **kwargs):\n    condition = _get_arg('condition', args, kwargs, 0)\n    self_tensor = _get_arg('self', args, kwargs, 1)\n    other_scalar = _get_arg('other', args, kwargs, 2)\n    return _where_impl(condition, self_tensor, other_scalar, out=None)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6316651409678143,
        "meets_threshold": false
      },
      "logical_xor": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.6253176491249763,
            "flaggems_speedup_vs_cuda": 0.991963,
            "speedup_vs_flaggems": 0.6303840457002694
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef logical_xor(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0)\n    res = x != y\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\n# Keep a reference to the Triton kernel before defining the Python wrapper with the same name.\nlogical_xor_kernel = logical_xor\n\n\ndef logical_xor(*args, **kwargs):\n    # Parse inputs similar to torch.logical_xor(input, other, *, out=None)\n    out = kwargs.pop('out', None)\n\n    if len(args) >= 2:\n        a, b = args[0], args[1]\n    elif len(args) == 1 and 'other' in kwargs:\n        a = args[0]\n        b = kwargs.pop('other')\n    else:\n        raise TypeError(\"logical_xor() expected 2 positional arguments (input, other)\")\n\n    # Convert inputs to tensors and determine device\n    if isinstance(a, torch.Tensor):\n        device = a.device\n    elif isinstance(b, torch.Tensor):\n        device = b.device\n    else:\n        raise TypeError(\"At least one of the inputs must be a torch.Tensor\")\n\n    if device.type != 'cuda':\n        raise AssertionError(\"Triton kernels require CUDA tensors (device='cuda').\")\n\n    a = torch.as_tensor(a, device=device)\n    b = torch.as_tensor(b, device=device)\n\n    # Broadcast to common shape\n    a_b, b_b = torch.broadcast_tensors(a, b)\n    # Ensure boolean dtype and contiguous memory\n    a_b = a_b.to(torch.bool).contiguous()\n    b_b = b_b.to(torch.bool).contiguous()\n\n    out_shape = a_b.shape\n\n    # Prepare output tensor\n    used_temp_out = False\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor if provided\")\n        if out.device != device:\n            raise AssertionError(\"out tensor must be on the same CUDA device as inputs\")\n        if out.dtype != torch.bool:\n            raise TypeError(\"out tensor must have dtype torch.bool\")\n        if out.shape != out_shape:\n            raise ValueError(f\"out tensor has shape {out.shape}, expected {out_shape}\")\n        if out.is_contiguous():\n            output = out\n        else:\n            output = torch.empty(out_shape, dtype=torch.bool, device=device)\n            used_temp_out = True\n    else:\n        output = torch.empty(out_shape, dtype=torch.bool, device=device)\n\n    # Launch Triton kernel\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    logical_xor_kernel[grid](a_b, b_b, output, n_elements, BLOCK_SIZE=1024)\n\n    # If using a temporary output for a non-contiguous out tensor, copy back\n    if used_temp_out:\n        out.copy_(output)\n        return out\n\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6303840457002694,
        "meets_threshold": false
      },
      "ne": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5964883059091649,
            "flaggems_speedup_vs_cuda": 0.99122,
            "speedup_vs_flaggems": 0.6017718628651206
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef ne_kernel_tensor(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    neq = x != y\n    out_vals = tl.where(neq, 1, 0)\n    tl.store(out_ptr + offsets, out_vals, mask=mask)\n\n\n@triton.jit\ndef ne_kernel_scalar(x_ptr, scalar_val, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    neq = x != scalar_val\n    out_vals = tl.where(neq, 1, 0)\n    tl.store(out_ptr + offsets, out_vals, mask=mask)\n\n\ndef _parse_args_tensor(*args, **kwargs):\n    x = None\n    y = None\n    out = kwargs.get('out', None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n        if len(args) >= 3 and out is None:\n            out = args[2]\n    else:\n        x = kwargs.get('input', kwargs.get('self', None))\n        y = kwargs.get('other', None)\n    return x, y, out\n\n\ndef _parse_args_scalar(*args, **kwargs):\n    x = None\n    scalar = None\n    out = kwargs.get('out', None)\n    if len(args) >= 2:\n        x, scalar = args[0], args[1]\n        if len(args) >= 3 and out is None:\n            out = args[2]\n    else:\n        x = kwargs.get('input', kwargs.get('self', None))\n        scalar = kwargs.get('other', None)\n    return x, scalar, out\n\n\ndef ne_Tensor(*args, **kwargs):\n    x, y, out = _parse_args_tensor(*args, **kwargs)\n    assert isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor), \"ne_Tensor expects two tensors\"\n    assert x.device.type == 'cuda' and y.device.type == 'cuda', \"Inputs must be CUDA tensors\"\n    x_flat = x.contiguous().view(-1)\n    y_flat = y.contiguous().view(-1)\n    assert x_flat.numel() == y_flat.numel(), \"Input tensors must have the same number of elements\"\n    n_elements = x_flat.numel()\n    out_u8 = torch.empty(n_elements, dtype=torch.uint8, device=x.device)\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ne_kernel_tensor[grid](x_flat, y_flat, out_u8, n_elements, BLOCK_SIZE=1024)\n    result_bool = out_u8.view(x.shape).bool()\n    if out is not None:\n        assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n        assert out.device == x.device, \"out must be on the same device as input\"\n        assert out.dtype == torch.bool, \"out must have dtype torch.bool\"\n        assert out.shape == x.shape, \"out must have the same shape as input\"\n        out.copy_(result_bool)\n        return out\n    return result_bool\n\n\ndef ne_Scalar(*args, **kwargs):\n    x, scalar, out = _parse_args_scalar(*args, **kwargs)\n    assert isinstance(x, torch.Tensor), \"ne_Scalar expects a tensor as the first argument\"\n    assert x.device.type == 'cuda', \"Input must be a CUDA tensor\"\n    assert scalar is not None, \"Scalar value must be provided\"\n    x_flat = x.contiguous().view(-1)\n    n_elements = x_flat.numel()\n    out_u8 = torch.empty(n_elements, dtype=torch.uint8, device=x.device)\n\n    # Choose scalar type based on tensor dtype category\n    if x.dtype.is_floating_point or x.dtype in (torch.float16, torch.bfloat16):\n        scalar_val = float(scalar)\n    else:\n        scalar_val = int(scalar)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    ne_kernel_scalar[grid](x_flat, scalar_val, out_u8, n_elements, BLOCK_SIZE=1024)\n    result_bool = out_u8.view(x.shape).bool()\n    if out is not None:\n        assert isinstance(out, torch.Tensor), \"out must be a torch.Tensor\"\n        assert out.device == x.device, \"out must be on the same device as input\"\n        assert out.dtype == torch.bool, \"out must have dtype torch.bool\"\n        assert out.shape == x.shape, \"out must have the same shape as input\"\n        out.copy_(result_bool)\n        return out\n    return result_bool\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.6017718628651206,
        "meets_threshold": false
      },
      "lt": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5958156616934435,
            "flaggems_speedup_vs_cuda": 0.997432,
            "speedup_vs_flaggems": 0.5973496556090475
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef lt_kernel(x_ptr, y_ptr, out_ptr, n_elements, y_is_scalar: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    if y_is_scalar:\n        y = tl.load(y_ptr)\n    else:\n        y = tl.load(y_ptr + offsets, mask=mask)\n\n    cmp = x < y\n    out_vals = tl.where(cmp, 1, 0)\n    tl.store(out_ptr + offsets, out_vals, mask=mask)\n\n\ndef lt_Tensor(*args, **kwargs):\n    # torch.lt(input, other, *, out=None)\n    if len(args) >= 2:\n        x = args[0]\n        y = args[1]\n    else:\n        x = kwargs.get('input', None)\n        y = kwargs.get('other', None)\n    out = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n        raise TypeError(\"lt_Tensor expects two torch.Tensor inputs\")\n\n    if x.device.type != 'cuda' or y.device.type != 'cuda':\n        raise RuntimeError(\"lt_Tensor expects CUDA tensors\")\n\n    if x.is_complex() or y.is_complex():\n        raise NotImplementedError(\"Complex dtype comparison is not supported\")\n\n    promoted_dtype = torch.promote_types(x.dtype, y.dtype)\n    x_t, y_t = torch.broadcast_tensors(x.to(promoted_dtype), y.to(promoted_dtype))\n    x_t = x_t.contiguous()\n    y_t = y_t.contiguous()\n\n    n_elements = x_t.numel()\n    out_i8 = torch.empty_like(x_t, dtype=torch.int8)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    lt_kernel[grid](x_t, y_t, out_i8, n_elements, y_is_scalar=False, BLOCK_SIZE=1024)\n\n    result = out_i8.bool()\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.device != x_t.device:\n            raise RuntimeError(\"out tensor must be on the same device as input\")\n        if out.dtype != torch.bool:\n            raise RuntimeError(\"out tensor must have dtype torch.bool\")\n        if out.shape != result.shape:\n            raise RuntimeError(\"out tensor has incorrect shape\")\n        out.copy_(result)\n        return out\n    return result\n\n\ndef lt_Scalar(*args, **kwargs):\n    # torch.lt(input, other, *, out=None) where other is a scalar\n    if len(args) >= 2:\n        x = args[0]\n        scalar = args[1]\n    else:\n        x = kwargs.get('input', None)\n        scalar = kwargs.get('other', None)\n    out = kwargs.get('out', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"lt_Scalar expects a torch.Tensor as input\")\n    if x.device.type != 'cuda':\n        raise RuntimeError(\"lt_Scalar expects CUDA tensors\")\n\n    if x.is_complex():\n        raise NotImplementedError(\"Complex dtype comparison is not supported\")\n\n    scalar_tensor_cpu = torch.tensor(scalar)\n    if scalar_tensor_cpu.is_complex():\n        raise NotImplementedError(\"Complex dtype comparison is not supported\")\n\n    promoted_dtype = torch.promote_types(x.dtype, scalar_tensor_cpu.dtype)\n    x_t = x.to(promoted_dtype).contiguous()\n    scalar_dev = torch.tensor(scalar, dtype=promoted_dtype, device=x_t.device)\n\n    n_elements = x_t.numel()\n    out_i8 = torch.empty_like(x_t, dtype=torch.int8)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    lt_kernel[grid](x_t, scalar_dev, out_i8, n_elements, y_is_scalar=True, BLOCK_SIZE=1024)\n\n    result = out_i8.bool()\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out.device != x_t.device:\n            raise RuntimeError(\"out tensor must be on the same device as input\")\n        if out.dtype != torch.bool:\n            raise RuntimeError(\"out tensor must have dtype torch.bool\")\n        if out.shape != result.shape:\n            raise RuntimeError(\"out tensor has incorrect shape\")\n        out.copy_(result)\n        return out\n    return result\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5973496556090475,
        "meets_threshold": false
      },
      "cos": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5610585830769668,
            "flaggems_speedup_vs_cuda": 0.971215,
            "speedup_vs_flaggems": 0.5776873123633457
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef cos(x_ptr,  # Pointer to input tensor (float32).\n        out_ptr,  # Pointer to output tensor (float32).\n        n_elements,  # Number of elements.\n        BLOCK_SIZE: tl.constexpr  # Elements per program.\n        ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.cos(x)\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\n# Preserve reference to the Triton kernel before defining the Python wrapper with the same name.\ncos_kernel = cos\n\n\ndef cos(*args, **kwargs):\n    # Extract input tensor\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input', kwargs.get('x', None))\n    out = kwargs.get('out', None)\n\n    if x is None:\n        raise TypeError(\"cos() missing required argument 'input'\")\n\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n\n    # Handle complex dtype by falling back to torch.cos\n    if x.is_complex():\n        return torch.cos(x) if out is None else torch.cos(x, out=out)\n\n    # Decide output dtype\n    if out is not None:\n        dst_dtype = out.dtype\n    else:\n        if x.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n            dst_dtype = x.dtype\n        elif x.dtype in (\n            torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64, torch.bool\n        ):\n            dst_dtype = torch.float32\n        else:\n            # Fallback for unsupported dtypes\n            return torch.cos(x) if out is None else torch.cos(x, out=out)\n\n    # Move input to CUDA and cast to float32 for computation\n    x_cuda = x.to('cuda').contiguous()\n    x32 = x_cuda.to(torch.float32).contiguous()\n\n    # Allocate temporary float32 output on CUDA\n    y32 = torch.empty_like(x32, dtype=torch.float32, device=x32.device)\n\n    # Launch Triton kernel\n    n_elements = x32.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    cos_kernel[grid](x32, y32, n_elements, BLOCK_SIZE=1024)\n\n    # Cast result to desired dtype\n    y_res = y32 if dst_dtype == torch.float32 else y32.to(dst_dtype)\n\n    # If an out tensor is provided, copy result into it\n    if out is not None:\n        # Ensure shape compatibility\n        if out.numel() != y_res.numel():\n            raise RuntimeError(\"The 'out' tensor must have the same number of elements as the input.\")\n        # Move result to out's device and dtype if needed, then copy\n        if out.device.type != 'cuda':\n            temp = y_res.to(out.device)\n            out.copy_(temp.view(out.shape))\n        else:\n            out.copy_(y_res.view(out.shape))\n        return out\n\n    return y_res\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5776873123633457,
        "meets_threshold": false
      },
      "cat": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.6429069403581904,
            "flaggems_speedup_vs_cuda": 1.126369,
            "speedup_vs_flaggems": 0.570778262148719
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef cat(src_ptr, dst_ptr,\n        n_elems,  # number of elements in src tensor\n        chunk_len,  # src.shape[dim] * inner\n        inner,  # product of sizes after dim\n        out_block_len,  # out.shape[dim] * inner\n        cum_before_mul_inner,  # cumulative size before this tensor along dim times inner\n        BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elems\n\n    # Compute destination indices\n    offsets64 = offsets.to(tl.int64)\n    outer = offsets64 // chunk_len\n    rem = offsets64 % chunk_len\n    pos_dim = rem // inner\n    inner_off = rem % inner\n    dst_index = outer * out_block_len + pos_dim * inner + inner_off + cum_before_mul_inner\n\n    x = tl.load(src_ptr + offsets, mask=mask)\n    tl.store(dst_ptr + dst_index, x, mask=mask)\n\n\n# Keep a reference to the Triton kernel before redefining the Python wrapper with the same name\ncat_kernel = cat\n\n\ndef cat(*args, **kwargs):\n    # Parse inputs similar to torch.cat\n    if len(args) == 0:\n        raise ValueError(\"cat expected at least one argument, got 0\")\n\n    # Determine tensors and dim\n    if isinstance(args[0], (list, tuple)):\n        tensors = list(args[0])\n        if len(args) >= 2 and isinstance(args[1], int):\n            dim = args[1]\n        else:\n            dim = kwargs.get('dim', 0)\n    else:\n        tensors = list(args)\n        dim = kwargs.get('dim', 0)\n\n    out_kw = kwargs.get('out', None)\n\n    if len(tensors) == 0:\n        raise ValueError(\"cat expected a non-empty list of Tensors\")\n\n    # Ensure all tensors are on CUDA and contiguous\n    device = tensors[0].device\n    dtype = tensors[0].dtype\n    ndims = tensors[0].dim()\n    if any(t.device != device for t in tensors):\n        raise AssertionError(\"All input tensors must be on the same device\")\n    if device.type != \"cuda\":\n        raise AssertionError(\"Triton kernel requires CUDA tensors\")\n    if any(t.dtype != dtype for t in tensors):\n        raise AssertionError(\"All input tensors must have the same dtype\")\n    if any(t.dim() != ndims for t in tensors):\n        raise AssertionError(\"All input tensors must have the same number of dimensions\")\n\n    # Normalize dim\n    if dim < 0:\n        dim += ndims\n    if not (0 <= dim < ndims):\n        raise IndexError(f\"Dimension out of range (expected to be in range of [{-ndims}, {ndims-1}], but got {dim - ndims if dim >= ndims else dim})\")\n\n    # Make all inputs contiguous\n    tensors = [t.contiguous() for t in tensors]\n\n    # Validate shapes and compute output shape\n    base_shape = list(tensors[0].shape)\n    for t in tensors[1:]:\n        shp = list(t.shape)\n        if any((i != dim) and (base_shape[i] != shp[i]) for i in range(ndims)):\n            raise AssertionError(\"Sizes of tensors must match except in dimension being concatenated\")\n\n    out_shape = base_shape[:]\n    out_shape[dim] = sum(t.shape[dim] for t in tensors)\n\n    # Prepare output\n    if out_kw is not None:\n        out = out_kw\n        if list(out.shape) != out_shape or out.dtype != dtype or out.device != device:\n            raise AssertionError(\"Provided 'out' tensor has incorrect shape/dtype/device\")\n    else:\n        out = torch.empty(out_shape, dtype=dtype, device=device)\n\n    # Precompute inner (product of sizes after dim)\n    sizes_after = tensors[0].shape[dim+1:]\n    inner = 1\n    for s in sizes_after:\n        inner *= int(s)\n\n    # out block length along flattened view for each outer position\n    out_block_len = int(out_shape[dim]) * inner\n\n    # Copy each tensor into output at the right offset along dim\n    cum_before = 0\n    BLOCK_SIZE = 1024\n    for t in tensors:\n        n = t.numel()\n        if n == 0:\n            # Nothing to copy\n            cum_before += int(t.shape[dim])\n            continue\n        chunk_len = int(t.shape[dim]) * inner\n        cum_before_mul_inner = cum_before * inner\n        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n        cat_kernel[grid](\n            t, out,\n            n,\n            chunk_len,\n            inner,\n            out_block_len,\n            cum_before_mul_inner,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        cum_before += int(t.shape[dim])\n\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.570778262148719,
        "meets_threshold": false
      },
      "sin": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5580186511171821,
            "flaggems_speedup_vs_cuda": 0.989787666666667,
            "speedup_vs_flaggems": 0.563776120788043
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef sin(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.sin(x)\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\n# Keep a reference to the kernel before defining the Python wrapper with the same name\nsin_kernel = sin\n\n\ndef sin(*args, **kwargs):\n    # Extract the input tensor\n    if len(args) > 0:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    elif 'x' in kwargs:\n        x = kwargs['x']\n    else:\n        raise TypeError(\"sin() missing 1 required positional argument: 'input tensor'\")\n\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n\n    if x.is_complex():\n        raise NotImplementedError(\"Complex dtypes are not supported in this Triton sin implementation.\")\n    if not x.is_cuda:\n        raise AssertionError(\"Input tensor must be on a CUDA device.\")\n\n    x_contig = x.contiguous()\n\n    # Compute in float32 for numerical stability and Triton support\n    x_work = x_contig.to(torch.float32)\n    out_work = torch.empty_like(x_work)\n\n    n_elements = x_work.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    sin_kernel[grid](x_work, out_work, n_elements, BLOCK_SIZE=1024)\n\n    # Cast back to original dtype, following torch.sin behavior:\n    # - Floating types: preserve original dtype\n    # - Integer/bool: return float32\n    if x.dtype in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        out = out_work.to(dtype=x.dtype)\n    else:\n        out = out_work\n\n    return out.view(x.shape)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.563776120788043,
        "meets_threshold": false
      },
      "uniform_": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.7134310164491949,
            "flaggems_speedup_vs_cuda": 1.27137433333333,
            "speedup_vs_flaggems": 0.5611494567290017
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef uniform_(out_ptr,  # Pointer to output tensor\n             n_elements,  # Number of elements to process\n             low,  # Lower bound of the uniform distribution\n             high,  # Upper bound of the uniform distribution\n             seed,  # RNG seed\n             BLOCK_SIZE: tl.constexpr  # Elements per program\n             ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    r = tl.rand(seed, offsets)  # uniform in [0, 1)\n    out = low + (high - low) * r\n    tl.store(out_ptr + offsets, out, mask=mask)\n\n\n_uniform_kernel = uniform_\n\n\ndef uniform_(*args, **kwargs):\n    if len(args) == 0:\n        raise ValueError(\"uniform_ expects at least a tensor input as the first positional argument.\")\n    x = args[0]\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"uniform_ expects a torch.Tensor as the first positional argument.\")\n\n    # Parse bounds: PyTorch uses 'from' and 'to'\n    low = kwargs.get('from', None)\n    high = kwargs.get('to', None)\n    if low is None and len(args) >= 2:\n        low = args[1]\n    if high is None and len(args) >= 3:\n        high = args[2]\n    if low is None:\n        low = 0.0\n    if high is None:\n        high = 1.0\n    low = float(low)\n    high = float(high)\n    if not (high > low):\n        raise ValueError(\"uniform_ expects 'to' > 'from'.\")\n\n    # Handle CPU tensors by falling back to PyTorch\n    if not x.is_cuda:\n        x.uniform_(from_=low, to=high)\n        return x\n\n    # Only floating dtypes supported\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise TypeError(\"uniform_ only supports floating point dtypes (float16, bfloat16, float32, float64).\")\n\n    # If non-contiguous, operate on a contiguous copy and copy back\n    need_copy_back = False\n    out = x\n    if not x.is_contiguous():\n        out = x.contiguous()\n        need_copy_back = True\n\n    n_elements = out.numel()\n    if n_elements == 0:\n        return x\n\n    # Seed: allow custom seed via kwargs; otherwise use a random CPU seed\n    seed = kwargs.get('seed', None)\n    if seed is None:\n        seed = torch.randint(0, 1 << 31, (1,), device='cpu', dtype=torch.int64).item()\n    else:\n        seed = int(seed)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    _uniform_kernel[grid](\n        out, n_elements, low, high, seed,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    if need_copy_back:\n        x.copy_(out)\n    return x\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5611494567290017,
        "meets_threshold": false
      },
      "nonzero": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.23341919538334602,
            "flaggems_speedup_vs_cuda": 0.417961333333333,
            "speedup_vs_flaggems": 0.558470788486047
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef count_nnz_kernel(x_ptr, n_elements, counts_ptr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    is_nz = x != 0\n    flags = is_nz.to(tl.int32)\n    nnz = tl.sum(flags, axis=0)\n    tl.store(counts_ptr + pid, nnz)\n\n\n@triton.jit\ndef nonzero(x_ptr, n_elements, block_offsets_ptr, out_idx_ptr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    is_nz = x != 0\n    flags = is_nz.to(tl.int32)\n    # exclusive scan of flags\n    prefix = tl.cumsum(flags, axis=0) - flags\n    base = tl.load(block_offsets_ptr + pid, mask=True, other=0).to(tl.int32)\n    pos = base + prefix  # positions within the global out array\n    store_mask = mask & is_nz\n    tl.store(out_idx_ptr + pos, offsets.to(tl.int64), mask=store_mask)\n\n\n# keep a handle to the Triton kernel before defining the Python wrapper with the same name\nnonzero_kernel = nonzero\n\n\ndef nonzero(*args, **kwargs):\n    # Extract input tensor\n    if len(args) > 0:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    else:\n        raise ValueError(\"nonzero expects an input tensor as the first positional argument or keyword 'input'.\")\n\n    as_tuple = kwargs.get('as_tuple', False)\n    if as_tuple:\n        raise NotImplementedError(\"as_tuple=True is not supported in this Triton implementation.\")\n\n    # Ensure tensor on CUDA for Triton and contiguous\n    if not x.is_cuda:\n        raise ValueError(\"Input tensor must be on a CUDA device.\")\n    x_contig = x.contiguous()\n    x_flat = x_contig.reshape(-1)\n    n_elements = x_flat.numel()\n\n    # Handle empty input early\n    if n_elements == 0:\n        return torch.empty((0, x.dim()), dtype=torch.long, device=x.device)\n\n    # Kernel launch configuration\n    BLOCK_SIZE = 1024\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    grid = lambda meta: (num_blocks,)\n\n    # First pass: count non-zeros per block\n    counts = torch.empty(num_blocks, dtype=torch.int32, device=x.device)\n    count_nnz_kernel[grid](x_flat, n_elements, counts, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Exclusive prefix sum of counts to get block offsets, and total nnz\n    counts64 = counts.to(torch.int64)\n    csum = torch.cumsum(counts64, dim=0)\n    block_offsets = torch.empty_like(csum)\n    if num_blocks > 0:\n        block_offsets[0] = 0\n        if num_blocks > 1:\n            block_offsets[1:] = csum[:-1]\n        total_nnz = int(csum[-1].item())\n    else:\n        total_nnz = 0\n\n    # If no non-zeros, return empty tensor with appropriate shape\n    if total_nnz == 0:\n        return torch.empty((0, x.dim()), dtype=torch.long, device=x.device)\n\n    # Allocate flat index buffer for non-zero elements\n    out_idx = torch.empty(total_nnz, dtype=torch.int64, device=x.device)\n\n    # Second pass: write flat indices of non-zero elements\n    nonzero_kernel[grid](x_flat, n_elements, block_offsets.to(torch.int32), out_idx, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Convert flat indices to multi-dimensional indices\n    if x.dim() == 0:\n        # Scalar tensor: shape is (nnz, 0)\n        return torch.empty((out_idx.numel(), 0), dtype=torch.long, device=x.device)\n    elif x.dim() == 1:\n        return out_idx.unsqueeze(1)\n    else:\n        coords = torch.unravel_index(out_idx, x_contig.shape)\n        return torch.stack(coords, dim=1)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.558470788486047,
        "meets_threshold": false
      },
      "rsqrt": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5591280497097925,
            "flaggems_speedup_vs_cuda": 1.00362833333333,
            "speedup_vs_flaggems": 0.5571066809690117
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef rsqrt(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = 1.0 / tl.sqrt(x)\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\n_rsqrt_kernel = rsqrt\n\n\ndef rsqrt(*args, **kwargs):\n    # Parse input tensor\n    if len(args) > 0:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    out = kwargs.get('out', None)\n\n    if x is None:\n        raise TypeError(\"rsqrt expected an input tensor as the first positional argument or keyword 'input'.\")\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"rsqrt: input must be a torch.Tensor.\")\n\n    if x.device.type != 'cuda':\n        raise ValueError(\"rsqrt: input tensor must be on a CUDA device.\")\n\n    if x.is_complex():\n        raise TypeError(\"rsqrt: complex dtypes are not supported.\")\n\n    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise TypeError(\"rsqrt: only floating point dtypes are supported (float16, bfloat16, float32, float64).\")\n\n    n_elements = x.numel()\n\n    if out is not None:\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"rsqrt: 'out' must be a torch.Tensor.\")\n        if out.device != x.device:\n            raise ValueError(\"rsqrt: 'out' must be on the same device as input.\")\n        if out.numel() != n_elements:\n            raise ValueError(\"rsqrt: 'out' must have the same number of elements as input.\")\n        if out.dtype != x.dtype:\n            raise TypeError(\"rsqrt: 'out' must have the same dtype as input.\")\n\n    # Early return for empty tensors\n    if n_elements == 0:\n        if out is not None:\n            return out\n        else:\n            return x.clone()\n\n    # Use float32 for computation for better precision and broader Triton support\n    compute_dtype = torch.float32\n    x_contig = x.contiguous()\n    x_comp = x_contig.to(compute_dtype) if x_contig.dtype != compute_dtype else x_contig\n    tmp_out = torch.empty_like(x_comp)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _rsqrt_kernel[grid](x_comp, tmp_out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    result = tmp_out.to(x_contig.dtype).view_as(x_contig)\n\n    if out is not None:\n        out.copy_(result)\n        return out\n    return result\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5571066809690117,
        "meets_threshold": false
      },
      "randn": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.4782184499049616,
            "flaggems_speedup_vs_cuda": 0.905391333333333,
            "speedup_vs_flaggems": 0.5281897808148098
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef randn_kernel(\n    out_ptr,\n    n_elements,\n    seed,  # integer seed for RNG\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Generate two independent uniform random numbers in (0, 1)\n    u1 = tl.rand(seed, offsets)\n    u2 = tl.rand(seed + 1337, offsets)\n\n    # Clamp u1 to avoid log(0)\n    eps = 1e-7\n    u1 = tl.maximum(u1, eps)\n\n    # Box-Muller transform to generate standard normal\n    two_pi = 6.283185307179586\n    r = tl.sqrt(-2.0 * tl.log(u1))\n    theta = two_pi * u2\n    z = r * tl.cos(theta)  # one of the two normals\n\n    tl.store(out_ptr + offsets, z, mask=mask)\n\n\ndef randn(*args, **kwargs):\n    # If device or out imply non-CUDA, fall back to torch.randn directly\n    out_kw = kwargs.get(\"out\", None)\n    if out_kw is not None:\n        if not isinstance(out_kw, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor\")\n        if out_kw.device.type != \"cuda\":\n            return torch.randn(*args, **kwargs)\n        device = out_kw.device\n    else:\n        device_kw = kwargs.get(\"device\", None)\n        if device_kw is None:\n            # Default PyTorch device is CPU; use fallback unless explicitly CUDA requested\n            return torch.randn(*args, **kwargs)\n        device = torch.device(device_kw)\n        if device.type != \"cuda\":\n            return torch.randn(*args, **kwargs)\n\n    # Determine output tensor\n    if out_kw is not None:\n        out = out_kw\n        # Torch.randn(out=...) ignores dtype/device arguments; we match by just filling\n        pass\n    else:\n        # Parse shape from args / kwargs\n        if len(args) == 0:\n            # randn() -> scalar\n            shape = ()\n        elif len(args) == 1 and isinstance(args[0], (tuple, list, torch.Size)):\n            shape = tuple(int(x) for x in args[0])\n        else:\n            shape = tuple(int(x) for x in args)\n\n        # Dtype and requires_grad\n        dtype = kwargs.get(\"dtype\", torch.get_default_dtype())\n        requires_grad = bool(kwargs.get(\"requires_grad\", False))\n        out = torch.empty(shape, dtype=dtype, device=device, requires_grad=requires_grad)\n\n    # Ensure floating dtype\n    if out.dtype not in (torch.float16, torch.bfloat16, torch.float32, torch.float64):\n        raise TypeError(\"randn only supports floating point dtypes (float16, bfloat16, float32, float64).\")\n\n    # Ensure contiguous for simple linear indexing; otherwise, use a temp buffer and copy back\n    needs_copy_back = False\n    if not out.is_contiguous():\n        tmp = torch.empty_like(out, memory_format=torch.contiguous_format)\n        needs_copy_back = True\n        target = tmp\n    else:\n        target = out\n\n    n_elements = target.numel()\n    if n_elements > 0:\n        # Determine seed\n        gen = kwargs.get(\"generator\", None)\n        if gen is not None:\n            try:\n                seed_val = int(gen.initial_seed())\n            except Exception:\n                seed_val = int(torch.randint(0, 2**31 - 1, (1,), device=\"cpu\").item())\n        else:\n            seed_val = int(torch.randint(0, 2**31 - 1, (1,), device=\"cpu\").item())\n\n        # Launch kernel\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        randn_kernel[grid](target, n_elements, seed_val, BLOCK_SIZE=1024)\n\n    if needs_copy_back:\n        out.copy_(target)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5281897808148098,
        "meets_threshold": false
      },
      "full_like": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5102229492898767,
            "flaggems_speedup_vs_cuda": 0.975713333333333,
            "speedup_vs_flaggems": 0.5229230060296503
          }
        },
        "code": "import triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef full_like(output_ptr, value_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val = tl.load(value_ptr)\n    vec = val + tl.zeros([BLOCK_SIZE], dtype=val.dtype)\n    tl.store(output_ptr + offsets, vec, mask=mask)\n\n\n_full_like_kernel = full_like\n\n\ndef full_like(*args, **kwargs):\n    # Parse arguments similar to torch.full_like(input, fill_value, ...)\n    if len(args) >= 1:\n        input_tensor = args[0]\n    else:\n        input_tensor = kwargs.get('input', None)\n    if input_tensor is None:\n        raise ValueError(\"full_like requires an 'input' tensor as the first argument or 'input' keyword.\")\n\n    if len(args) >= 2:\n        fill_value = args[1]\n    else:\n        if 'fill_value' in kwargs:\n            fill_value = kwargs['fill_value']\n        else:\n            raise ValueError(\"full_like requires a 'fill_value' as the second argument or 'fill_value' keyword.\")\n\n    dtype = kwargs.get('dtype', None)\n    device = kwargs.get('device', None)\n    requires_grad = kwargs.get('requires_grad', False)\n\n    out_dtype = dtype if dtype is not None else input_tensor.dtype\n    out_device = device if device is not None else input_tensor.device\n    out_shape = tuple(input_tensor.shape)\n\n    # Allocate a contiguous output tensor\n    output = torch.empty(out_shape, dtype=out_dtype, device=out_device)\n\n    # Handle empty tensors early\n    if output.numel() == 0:\n        if requires_grad:\n            output.requires_grad_(True)\n        return output\n\n    # Prepare value tensor on the correct device/dtype (single scalar)\n    if isinstance(fill_value, torch.Tensor):\n        if fill_value.numel() != 1:\n            fill_value = fill_value.item()\n        value_tensor = torch.tensor(fill_value, dtype=out_dtype, device=out_device)\n    else:\n        value_tensor = torch.tensor(fill_value, dtype=out_dtype, device=out_device)\n\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    if output.is_cuda:\n        _full_like_kernel[grid](output, value_tensor, n_elements, BLOCK_SIZE=1024)\n    else:\n        # CPU fallback\n        output.fill_(value_tensor.item())\n\n    if requires_grad:\n        output.requires_grad_(True)\n    return output",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5229230060296503,
        "meets_threshold": false
      },
      "randn_like": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.4688140449829401,
            "flaggems_speedup_vs_cuda": 0.906711333333333,
            "speedup_vs_flaggems": 0.5170488420602886
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef randn_like(out_ptr,  # *Pointer* to output tensor.\n               n_elements,  # Number of elements to generate.\n               seed,  # RNG seed (int32).\n               BLOCK_SIZE: tl.constexpr  # Number of elements each program processes.\n               ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    key = tl.full((), seed, tl.int32)\n\n    # Generate two independent uniform random numbers in (0, 1)\n    u1 = tl.rand(key, offsets)\n    u2 = tl.rand(key, offsets + n_elements)\n\n    # Avoid log(0)\n    u1 = tl.maximum(u1, 1e-7)\n\n    # Box-Muller transform to get standard normal\n    r = tl.sqrt(-2.0 * tl.log(u1))\n    theta = 6.283185307179586 * u2  # 2 * pi\n    z = r * tl.cos(theta)\n\n    tl.store(out_ptr + offsets, z, mask=mask)\n\n\n# Preserve a handle to the Triton kernel before defining the Python wrapper with the same name.\nrandn_like_kernel = randn_like\n\n\ndef randn_like(*args, **kwargs):\n    x = args[0]\n    dtype = kwargs.get('dtype', x.dtype)\n    device = kwargs.get('device', x.device)\n    memory_format = kwargs.get('memory_format', torch.preserve_format)\n\n    # Supported dtypes for this Triton implementation\n    supported_dtypes = (torch.float16, torch.float32, torch.bfloat16)\n\n    # Fallback to torch if not on CUDA or dtype unsupported\n    if device.type != 'cuda' or dtype not in supported_dtypes:\n        return torch.randn_like(*args, **kwargs)\n\n    out = torch.empty_like(x, dtype=dtype, device=device, memory_format=memory_format)\n    n_elements = out.numel()\n    if n_elements == 0:\n        return out\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    # Generate a random seed\n    seed = torch.randint(0, 2**31, (1,), device='cpu', dtype=torch.int32).item()\n\n    randn_like_kernel[grid](out, n_elements, seed, BLOCK_SIZE=BLOCK_SIZE)\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5170488420602886,
        "meets_threshold": false
      },
      "div": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5152146877386244,
            "flaggems_speedup_vs_cuda": 1.00521633333333,
            "speedup_vs_flaggems": 0.5125411025009471
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef div_kernel(\n    x_ptr,           # pointer to input x (compute dtype: float32/float64)\n    y_ptr,           # pointer to input y (ignored if is_y_scalar=1)\n    out_ptr,         # pointer to output (compute dtype: float32/float64)\n    n_elements,      # number of elements\n    is_y_scalar,     # 0 or 1\n    y_scalar,        # scalar value of y if is_y_scalar=1\n    rounding_mode,   # 0=None, 1=trunc, 2=floor\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y_val = tl.load(y_ptr + offsets, mask=mask, other=1.0)\n    y = tl.where(is_y_scalar, y_scalar, y_val)\n\n    res = x / y\n\n    is_trunc = rounding_mode == 1\n    is_floor = rounding_mode == 2\n\n    # truncation toward zero using floor:\n    # trunc(a) = floor(a) if a >= 0 else -floor(-a)\n    trunc_res = tl.where(res >= 0, tl.floor(res), -tl.floor(-res))\n    res = tl.where(is_trunc, trunc_res, res)\n    res = tl.where(is_floor, tl.floor(res), res)\n\n    tl.store(out_ptr + offsets, res, mask=mask)\n\n\ndef _div_common(x, y, rounding_mode=None):\n    assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n    is_y_tensor = isinstance(y, torch.Tensor)\n    device = x.device\n    assert x.is_cuda, \"Input must be on CUDA device\"\n    if is_y_tensor:\n        assert y.is_cuda and y.device == device, \"Both tensors must be on the same CUDA device\"\n\n    # Validate rounding_mode\n    if rounding_mode is not None:\n        assert rounding_mode in (\"trunc\", \"floor\"), \"rounding_mode must be None, 'trunc' or 'floor'\"\n\n    # Determine compute dtype\n    def _is_float_dtype(dt):\n        return dt in (torch.float16, torch.float32, torch.float64, torch.bfloat16)\n\n    x_dtype = x.dtype\n    y_dtype = y.dtype if is_y_tensor else (torch.get_default_dtype() if isinstance(y, float) else torch.int64 if isinstance(y, int) else torch.get_default_dtype())\n\n    # Select compute dtype\n    if x_dtype == torch.float64 or (is_y_tensor and y_dtype == torch.float64):\n        compute_dtype = torch.float64\n    elif (rounding_mode is None) and (x_dtype == torch.int64 or (is_y_tensor and y_dtype == torch.int64)):\n        compute_dtype = torch.float64\n    else:\n        compute_dtype = torch.float32\n\n    # Prepare inputs in compute dtype (flattened)\n    x_c = x.to(compute_dtype).contiguous().view(-1)\n    n_elements = x_c.numel()\n\n    if is_y_tensor:\n        y_c = y.to(compute_dtype).contiguous().view(-1)\n        assert y_c.numel() == n_elements, \"Only same-sized tensors are supported\"\n    else:\n        # scalar y\n        y_val = (float(y) if compute_dtype == torch.float32 else float(y))\n        y_c = None\n\n    # Decide output dtype\n    if rounding_mode is None:\n        if _is_float_dtype(x_dtype) or (_is_float_dtype(y_dtype)):\n            # floating division: promote float types\n            out_dtype = torch.promote_types(x_dtype, y_dtype)\n        else:\n            # integer true division -> float\n            out_dtype = torch.float64 if (x_dtype == torch.int64 or (is_y_tensor and y_dtype == torch.int64)) else torch.float32\n    else:\n        # rounding division\n        if _is_float_dtype(x_dtype) or (_is_float_dtype(y_dtype)):\n            out_dtype = torch.promote_types(x_dtype, y_dtype)\n        else:\n            out_dtype = x_dtype\n\n    out_c = torch.empty_like(x_c, dtype=compute_dtype)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    rounding_flag = 0 if rounding_mode is None else (1 if rounding_mode == \"trunc\" else 2)\n    is_y_scalar_flag = 0 if is_y_tensor else 1\n    y_scalar_value = 0.0 if is_y_tensor else (float(y))\n\n    div_kernel[grid](\n        x_c,\n        (y_c if is_y_tensor else x_c),  # dummy pointer if scalar\n        out_c,\n        n_elements,\n        is_y_scalar_flag,\n        y_scalar_value,\n        rounding_flag,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    out = out_c.to(out_dtype).view_as(x)\n    return out\n\n\ndef div_Tensor(*args, **kwargs):\n    x = args[0]\n    y = args[1]\n    rounding_mode = kwargs.get(\"rounding_mode\", None)\n    return _div_common(x, y, rounding_mode)\n\n\ndef div_Scalar(*args, **kwargs):\n    x = args[0]\n    y = args[1]\n    rounding_mode = kwargs.get(\"rounding_mode\", None)\n    assert not isinstance(y, torch.Tensor), \"Second argument must be a scalar for div_Scalar\"\n    return _div_common(x, y, rounding_mode)\n\n\ndef div_Tensor_mode(*args, **kwargs):\n    x = args[0]\n    y = args[1]\n    rounding_mode = kwargs.get(\"rounding_mode\", None)\n    return _div_common(x, y, rounding_mode)\n\n\ndef div_Scalar_mode(*args, **kwargs):\n    x = args[0]\n    y = args[1]\n    rounding_mode = kwargs.get(\"rounding_mode\", None)\n    assert not isinstance(y, torch.Tensor), \"Second argument must be a scalar for div_Scalar_mode\"\n    return _div_common(x, y, rounding_mode)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5125411025009471,
        "meets_threshold": false
      },
      "full": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.5066224646526981,
            "flaggems_speedup_vs_cuda": 0.994028,
            "speedup_vs_flaggems": 0.5096661911462234
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef full(output_ptr, value_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val = tl.load(value_ptr)  # scalar value with the correct dtype\n    vec = val + tl.zeros((BLOCK_SIZE,), dtype=val.dtype)  # broadcast to block size\n    tl.store(output_ptr + offsets, vec, mask=mask)\n\n_full_kernel = full\n\ndef full(*args, **kwargs):\n    # Parse size and fill_value\n    size = None\n    fill_value_provided = False\n    if len(args) >= 1:\n        size = args[0]\n    elif 'size' in kwargs:\n        size = kwargs['size']\n    if len(args) >= 2:\n        fill_value = args[1]\n        fill_value_provided = True\n    elif 'fill_value' in kwargs:\n        fill_value = kwargs['fill_value']\n        fill_value_provided = True\n    else:\n        fill_value = None\n\n    if size is None:\n        raise TypeError(\"missing required argument 'size'\")\n    if not fill_value_provided:\n        raise TypeError(\"missing required argument 'fill_value'\")\n\n    # Normalize size to tuple\n    if isinstance(size, int):\n        shape = (size,)\n    else:\n        shape = tuple(size)\n\n    # Handle dtype/device/out/other kwargs\n    dtype = kwargs.get('dtype', None)\n    device = kwargs.get('device', None)\n    requires_grad = kwargs.get('requires_grad', False)\n    out_tensor = kwargs.get('out', None)\n\n    # Infer dtype if not provided\n    if dtype is None:\n        if isinstance(fill_value, torch.Tensor):\n            dtype = fill_value.dtype\n            # If it's a CPU tensor, we'll still allocate on device below\n        elif isinstance(fill_value, bool):\n            dtype = torch.bool\n        elif isinstance(fill_value, int):\n            dtype = torch.int64\n        elif isinstance(fill_value, float):\n            dtype = torch.get_default_dtype()\n        elif isinstance(fill_value, complex):\n            dtype = torch.complex64\n        else:\n            # Fallback to default float dtype\n            dtype = torch.get_default_dtype()\n\n    # Determine device\n    if out_tensor is not None:\n        device = out_tensor.device\n        if dtype is None:\n            dtype = out_tensor.dtype\n    else:\n        if device is None:\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        elif isinstance(device, str):\n            device = torch.device(device)\n\n    # Allocate or use provided output tensor\n    if out_tensor is None:\n        output = torch.empty(shape, dtype=dtype, device=device)\n    else:\n        output = out_tensor\n        # Optionally check size compatibility if size was provided\n        if output.numel() != int(torch.tensor(shape).prod()):\n            raise RuntimeError(\"Provided 'out' tensor size does not match 'size' argument\")\n\n    # Triton kernels require CUDA tensors\n    if output.device.type != 'cuda':\n        raise RuntimeError(\"This Triton implementation supports only CUDA tensors\")\n\n    # Set requires_grad if requested\n    if requires_grad:\n        if not (torch.is_floating_point(output) or torch.is_complex(output)):\n            raise RuntimeError(\"Only tensors of floating point or complex dtype can require gradients\")\n        output.requires_grad_(True)\n\n    n_elements = output.numel()\n    if n_elements == 0:\n        return output\n\n    # Create a device scalar tensor for the fill value with the correct dtype/device\n    if isinstance(fill_value, torch.Tensor):\n        # Ensure it's a scalar tensor on the correct device and dtype\n        if fill_value.numel() != 1:\n            raise RuntimeError(\"fill_value tensor must be a scalar (numel == 1)\")\n        value_tensor = fill_value.to(device=device, dtype=dtype).reshape(())\n    else:\n        value_tensor = torch.tensor(fill_value, dtype=dtype, device=output.device)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    _full_kernel[grid](output, value_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.5096661911462234,
        "meets_threshold": false
      },
      "aten::nan_to_num": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.47523293982484477,
            "flaggems_speedup_vs_cuda": 1.00122866666667,
            "speedup_vs_flaggems": 0.47464975349438304
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef nan_to_num_kernel(\n    x_ptr,                # *Pointer* to input tensor\n    out_ptr,              # *Pointer* to output tensor\n    n_elements,           # Number of elements\n    max_finite_ptr,       # *Pointer* to scalar: dtype-specific max finite\n    min_finite_ptr,       # *Pointer* to scalar: dtype-specific min finite\n    nan_rep_ptr,          # *Pointer* to scalar: replacement for NaN\n    posinf_rep_ptr,       # *Pointer* to scalar: replacement for +Inf\n    neginf_rep_ptr,       # *Pointer* to scalar: replacement for -Inf\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    max_finite = tl.load(max_finite_ptr)\n    min_finite = tl.load(min_finite_ptr)\n    nan_rep = tl.load(nan_rep_ptr)\n    posinf_rep = tl.load(posinf_rep_ptr)\n    neginf_rep = tl.load(neginf_rep_ptr)\n\n    is_nan = x != x\n    is_posinf = x > max_finite\n    is_neginf = x < min_finite\n\n    y = x\n    y = tl.where(is_nan, nan_rep, y)\n    y = tl.where(is_posinf, posinf_rep, y)\n    y = tl.where(is_neginf, neginf_rep, y)\n\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n\ndef _launch_nan_to_num(input: torch.Tensor, nan, posinf, neginf, out: torch.Tensor = None):\n    if not input.is_cuda:\n        raise AssertionError(\"Input tensor must be on CUDA device for Triton kernel.\")\n    if input.dtype not in (torch.float16, torch.float32, torch.float64, torch.bfloat16):\n        # For non-floating types, just copy input to output\n        if out is None:\n            return input.clone()\n        else:\n            out.copy_(input)\n            return out\n\n    device = input.device\n    dtype = input.dtype\n\n    finfo = torch.finfo(dtype)\n    max_finite_val = finfo.max\n    min_finite_val = finfo.min\n\n    # Resolve replacements\n    nan_rep_val = 0.0 if nan is None else float(nan)\n    posinf_rep_val = max_finite_val if posinf is None else float(posinf)\n    neginf_rep_val = min_finite_val if neginf is None else float(neginf)\n\n    # Prepare output\n    need_allocate_out = out is None\n    if need_allocate_out:\n        out = torch.empty_like(input)\n    else:\n        if out.shape != input.shape:\n            raise AssertionError(\"out tensor must have the same shape as input\")\n        if out.dtype != input.dtype:\n            raise AssertionError(\"out tensor must have the same dtype as input\")\n        if out.device != input.device:\n            raise AssertionError(\"out tensor must be on the same device as input\")\n\n    # Make contiguous buffers\n    x_contig = input.contiguous()\n    out_contig = out if out.is_contiguous() else torch.empty_like(input, memory_format=torch.contiguous_format)\n\n    n_elements = x_contig.numel()\n    if n_elements == 0:\n        if out is not out_contig:\n            out.copy_(out_contig)\n        return out\n\n    # Device scalars as 1-element tensors (to ensure dtype matches)\n    max_finite_t = torch.tensor([max_finite_val], device=device, dtype=dtype)\n    min_finite_t = torch.tensor([min_finite_val], device=device, dtype=dtype)\n    nan_rep_t = torch.tensor([nan_rep_val], device=device, dtype=dtype)\n    posinf_rep_t = torch.tensor([posinf_rep_val], device=device, dtype=dtype)\n    neginf_rep_t = torch.tensor([neginf_rep_val], device=device, dtype=dtype)\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    nan_to_num_kernel[grid](\n        x_contig,\n        out_contig,\n        n_elements,\n        max_finite_t,\n        min_finite_t,\n        nan_rep_t,\n        posinf_rep_t,\n        neginf_rep_t,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    if out is not out_contig:\n        out.copy_(out_contig)\n    return out\n\n\n# Wrapper for: ('nan_to_num', <Autograd.disable: False>)\ndef nan_to_num(input, nan=None, posinf=None, neginf=None):\n    return _launch_nan_to_num(input, nan, posinf, neginf, out=None)\n\n\n# Wrapper for: ('nan_to_num.out', <Autograd.disable: False>)\ndef nan_to_num_out(input, nan=None, posinf=None, neginf=None, out=None):\n    if out is None:\n        # Match the ATen .out behavior: out must be provided\n        out = torch.empty_like(input)\n    return _launch_nan_to_num(input, nan, posinf, neginf, out=out)",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.47464975349438304,
        "meets_threshold": false
      },
      "prod": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.3556075024768352,
            "flaggems_speedup_vs_cuda": 0.791212333333333,
            "speedup_vs_flaggems": 0.4494463590812353
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef prod_reduce_kernel(x_ptr, out_ptr, N_OUTER, REDUCE, N_INNER):\n    pid = tl.program_id(axis=0)\n    outer_idx = pid // N_INNER\n    inner_idx = pid % N_INNER\n    base_offset = outer_idx * REDUCE * N_INNER + inner_idx\n\n    # initialize accumulator with the first element (REDUCE > 0 guaranteed by wrapper)\n    acc = tl.load(x_ptr + base_offset)\n\n    r = 1\n    while r < REDUCE:\n        val = tl.load(x_ptr + base_offset + r * N_INNER)\n        acc = acc * val\n        r += 1\n\n    tl.store(out_ptr + outer_idx * N_INNER + inner_idx, acc)\n\n\ndef _compute_outer_inner(shape, dim):\n    # shape: tuple of sizes\n    # dim: int, normalized to [0, len(shape)-1]\n    outer = 1\n    for s in shape[:dim]:\n        outer *= s\n    inner = 1\n    for s in shape[dim + 1:]:\n        inner *= s\n    return outer, inner\n\n\ndef prod(*args, **kwargs):\n    # Supports:\n    # - prod(input, dtype=None)\n    # If 'dim' is provided, dispatch to prod_dim_int for convenience.\n    if 'dim' in kwargs or (len(args) >= 2 and isinstance(args[1], int)):\n        return prod_dim_int(*args, **kwargs)\n\n    # parse input and dtype\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input')\n    dtype = kwargs.get('dtype', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"prod: expected a torch.Tensor as input\")\n\n    if x.numel() == 0:\n        # identity for product over empty tensor is 1 (in requested dtype or input dtype)\n        out_dtype = dtype if dtype is not None else x.dtype\n        return torch.ones((), dtype=out_dtype, device=x.device)\n\n    # handle dtype and bool\n    compute_dtype = dtype if dtype is not None else x.dtype\n    is_bool = compute_dtype == torch.bool\n    if is_bool:\n        # compute using int32 and cast back to bool\n        xc = x.to(torch.int32).contiguous()\n        out = torch.empty((), dtype=torch.int32, device=x.device)\n        REDUCE = xc.numel()\n        grid = (1,)\n        prod_reduce_kernel[grid](xc, out, 1, REDUCE, 1)\n        return (out != 0).to(torch.bool)\n    else:\n        xc = x.to(compute_dtype).contiguous()\n        out = torch.empty((), dtype=compute_dtype, device=x.device)\n        REDUCE = xc.numel()\n        grid = (1,)\n        prod_reduce_kernel[grid](xc, out, 1, REDUCE, 1)\n        return out\n\n\ndef prod_dim_int(*args, **kwargs):\n    # Supports:\n    # - prod(input, dim: int, keepdim: bool=False, dtype=None)\n    # parse input, dim, keepdim, dtype\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input')\n\n    if 'dim' in kwargs:\n        dim = kwargs['dim']\n    elif len(args) >= 2 and isinstance(args[1], int):\n        dim = args[1]\n    else:\n        raise TypeError(\"prod_dim_int: missing required argument 'dim'\")\n\n    keepdim = kwargs.get('keepdim', False)\n    dtype = kwargs.get('dtype', None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"prod_dim_int: expected a torch.Tensor as input\")\n\n    # normalize dim\n    ndim = x.ndim\n    if dim < 0:\n        dim += ndim\n    if not (0 <= dim < ndim):\n        raise IndexError(\"prod_dim_int: 'dim' out of range\")\n\n    reduce_size = x.shape[dim]\n    # identity for product over empty dimension is 1\n    out_dtype = dtype if dtype is not None else x.dtype\n\n    # compute output shape\n    out_shape = []\n    for i in range(ndim):\n        if i == dim:\n            if keepdim:\n                out_shape.append(1)\n        else:\n            out_shape.append(x.shape[i])\n\n    if reduce_size == 0:\n        # fill with ones\n        return torch.ones(out_shape, dtype=out_dtype, device=x.device)\n\n    # select compute dtype and handle bool\n    compute_dtype = out_dtype\n    is_bool = compute_dtype == torch.bool\n\n    if is_bool:\n        xc = x.to(torch.int32).contiguous()\n        compute_dtype = torch.int32\n    else:\n        xc = x.to(compute_dtype).contiguous()\n\n    # compute outer, inner and set up kernel\n    N_OUTER, N_INNER = _compute_outer_inner(xc.shape, dim)\n    REDUCE = reduce_size\n\n    # allocate output tensor\n    if is_bool:\n        out_tmp = torch.empty(out_shape, dtype=torch.int32, device=xc.device)\n    else:\n        out_tmp = torch.empty(out_shape, dtype=compute_dtype, device=xc.device)\n\n    grid = (N_OUTER * N_INNER,)\n    prod_reduce_kernel[grid](xc, out_tmp, N_OUTER, REDUCE, N_INNER)\n\n    if is_bool:\n        return (out_tmp != 0).to(torch.bool)\n    else:\n        return out_tmp\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.4494463590812353,
        "meets_threshold": false
      },
      "isfinite": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.4599845867710128,
            "flaggems_speedup_vs_cuda": 3.55111,
            "speedup_vs_flaggems": 0.41113471189881834
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef isfinite(x_ptr,  # pointer to input tensor (any numeric dtype)\n             out_ptr,  # pointer to output tensor (uint8)\n             n_elements,  # number of elements\n             BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    d = x - x\n    is_fin = d == 0  # True for finite numbers, False for inf/NaN on floats; always True for ints\n\n    one = tl.full([BLOCK_SIZE], 1, dtype=tl.uint8)\n    zero = tl.full([BLOCK_SIZE], 0, dtype=tl.uint8)\n    out_vals = tl.where(is_fin, one, zero)\n    tl.store(out_ptr + offsets, out_vals, mask=mask)\n\n\n# Keep a reference to the Triton kernel before defining the Python wrapper with the same name.\nisfinite_kernel = isfinite\n\n\ndef isfinite(*args, **kwargs):\n    # Resolve input tensor\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    elif 'input' in kwargs:\n        x = kwargs['input']\n    if x is None:\n        raise ValueError(\"isfinite expects a tensor as the first argument or as 'input' kwarg.\")\n\n    out = kwargs.get('out', None)\n\n    # CPU fallback\n    if x.device.type != 'cuda':\n        result = torch.isfinite(x)\n        if out is not None:\n            out.copy_(result)\n            return out\n        return result\n\n    # Complex dtypes: finite if both real and imag are finite\n    if x.is_complex():\n        real_fin = isfinite(x.real)\n        imag_fin = isfinite(x.imag)\n        result = torch.logical_and(real_fin, imag_fin)\n        if out is not None:\n            out.copy_(result)\n            return out\n        return result\n\n    # Bool dtype: all values are finite (True)\n    if x.dtype == torch.bool:\n        result = torch.ones_like(x, dtype=torch.bool, device=x.device)\n        if out is not None:\n            out.copy_(result)\n            return out\n        return result\n\n    # General numeric types (integers and floats)\n    x_contig = x.contiguous()\n    n_elements = x_contig.numel()\n\n    # Temporary uint8 output buffer\n    tmp_u8 = torch.empty(n_elements, dtype=torch.uint8, device=x.device)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    isfinite_kernel[grid](x_contig, tmp_u8, n_elements, BLOCK_SIZE=1024)\n\n    result_bool = tmp_u8.view(x_contig.shape).to(torch.bool)\n\n    if out is not None:\n        out.copy_(result_bool)\n        return out\n    return result_bool\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.41113471189881834,
        "meets_threshold": false
      },
      "count_nonzero": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 1.138703733191004,
            "flaggems_speedup_vs_cuda": 3.00450666666667,
            "speedup_vs_flaggems": 0.3789985709881387
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef count_nonzero(x_ptr, out_ptr, M, K, BLOCK_M: tl.constexpr, BLOCK_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    rows = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    mask_rows = rows < M\n    acc = tl.zeros([BLOCK_M], dtype=tl.int64)\n\n    ks = tl.arange(0, BLOCK_K)\n    k = 0\n    while k < K:\n        k_idx = k + ks\n        mask_k = k_idx < K\n        offs = rows[:, None] * K + k_idx[None, :]\n        mask = mask_rows[:, None] & mask_k[None, :]\n        vals = tl.load(x_ptr + offs, mask=mask, other=0)\n        nz_mask = vals != 0\n        nz_i32 = tl.where(nz_mask, 1, 0)\n        acc += tl.sum(nz_i32, axis=1).to(tl.int64)\n        k += BLOCK_K\n\n    tl.store(out_ptr + rows, acc, mask=mask_rows)\n\n\n_count_nonzero_kernel = count_nonzero\n\n\ndef count_nonzero(*args, **kwargs):\n    # Parse input tensor\n    x = None\n    if len(args) >= 1:\n        x = args[0]\n    else:\n        x = kwargs.get('input', None)\n    if x is None:\n        raise ValueError(\"count_nonzero expects an input tensor as the first argument or via keyword 'input'.\")\n\n    # dtype casting if provided\n    dtype = kwargs.get('dtype', None)\n    if dtype is not None:\n        x = x.to(dtype)\n\n    # Parse dim argument\n    dim = kwargs.get('dim', None)\n    if len(args) >= 2 and dim is None:\n        dim = args[1]\n\n    # Normalize dims to reduce\n    if dim is None:\n        dims_to_reduce = list(range(x.dim()))\n    elif isinstance(dim, int):\n        dims_to_reduce = [dim]\n    else:\n        dims_to_reduce = list(dim)\n\n    # Normalize negative dims\n    nd = x.dim()\n    dims_to_reduce = [(d + nd) if d < 0 else d for d in dims_to_reduce]\n    # Validate dims\n    if any(d < 0 or d >= nd for d in dims_to_reduce):\n        raise ValueError(\"Invalid 'dim' specified.\")\n    # Remove duplicates while preserving order\n    seen = set()\n    dims_to_reduce = [d for d in dims_to_reduce if not (d in seen or seen.add(d))]\n\n    all_dims = list(range(nd))\n    dims_to_keep = [d for d in all_dims if d not in dims_to_reduce]\n\n    # Handle empty tensor\n    if x.numel() == 0:\n        # Output shape is product of kept dims\n        keep_sizes = [x.size(d) for d in dims_to_keep]\n        out = torch.zeros(keep_sizes, dtype=torch.int64, device=x.device)\n        return out\n\n    # Permute to make reduced dims contiguous at the end\n    permute_order = dims_to_keep + dims_to_reduce\n    x_perm = x.permute(permute_order).contiguous()\n\n    # Compute M (kept product) and K (reduced product)\n    keep_sizes = [x.size(d) for d in dims_to_keep]\n    reduce_sizes = [x.size(d) for d in dims_to_reduce] if len(dims_to_reduce) > 0 else [1]\n    M = int(torch.tensor(keep_sizes).prod().item()) if len(keep_sizes) > 0 else 1\n    K = int(torch.tensor(reduce_sizes).prod().item()) if len(reduce_sizes) > 0 else 1\n\n    x2 = x_perm.reshape(M, K)\n    out_flat = torch.empty((M,), dtype=torch.int64, device=x.device)\n\n    BLOCK_M = 128\n    BLOCK_K = 256\n    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']),)\n    _count_nonzero_kernel[grid](x2, out_flat, M, K, BLOCK_M=BLOCK_M, BLOCK_K=BLOCK_K)\n\n    # Reshape output to kept dims shape\n    if len(keep_sizes) == 0:\n        return out_flat.reshape(())\n    else:\n        return out_flat.reshape(keep_sizes)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.3789985709881387,
        "meets_threshold": false
      },
      "aten::masked_fill": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.6079054446389776,
            "flaggems_speedup_vs_cuda": 1.70214666666667,
            "speedup_vs_flaggems": 0.3571404606569213
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef masked_fill_kernel(\n    x_ptr,\n    mask_ptr,\n    fill_ptr,\n    out_ptr,\n    n_elements,\n    shape_ptr,\n    x_stride_ptr,\n    mask_stride_ptr,\n    fill_stride_ptr,\n    out_stride_ptr,\n    NDIMS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    valid = offsets < n_elements\n\n    # Use int64 for address computations\n    rem = offsets.to(tl.int64)\n    out_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    x_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    mask_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    fill_off = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Compute per-dimension indices and offsets\n    for d in range(NDIMS - 1, -1, -1):\n        size_d = tl.load(shape_ptr + d)\n        idx_d = rem % size_d\n        rem = rem // size_d\n\n        sx_d = tl.load(x_stride_ptr + d)\n        sm_d = tl.load(mask_stride_ptr + d)\n        sf_d = tl.load(fill_stride_ptr + d)\n        so_d = tl.load(out_stride_ptr + d)\n\n        x_off += idx_d * sx_d\n        mask_off += idx_d * sm_d\n        fill_off += idx_d * sf_d\n        out_off += idx_d * so_d\n\n    # Load values\n    x_val = tl.load(x_ptr + x_off, mask=valid)\n    m_val = tl.load(mask_ptr + mask_off, mask=valid)\n    f_val = tl.load(fill_ptr + fill_off, mask=valid)\n\n    # Convert mask to boolean\n    m_bool = m_val != 0\n\n    # Compute output\n    out_val = tl.where(m_bool, f_val, x_val)\n\n    # Store\n    tl.store(out_ptr + out_off, out_val, mask=valid)\n\n\ndef _launch_masked_fill(x: torch.Tensor, mask: torch.Tensor, fill: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and mask.is_cuda and fill.is_cuda and out.is_cuda\n    assert x.device == mask.device == fill.device == out.device\n\n    # Output shape is the shape of x\n    shape = tuple(x.shape)\n    NDIMS = len(shape)\n\n    # Ensure dtype matches\n    fill = fill.to(dtype=x.dtype)\n\n    # Make sure mask is boolean then cast to int8 for robust kernel comparison\n    mask = mask.to(torch.bool).to(torch.int8)\n\n    # Expand mask and fill to match x's shape for proper broadcasting strides\n    mask_exp = mask.expand(shape)\n    fill_exp = fill.expand(shape)\n\n    # Prepare stride and shape tensors (int64) on device\n    shape_t = torch.tensor(shape, dtype=torch.int64, device=x.device)\n    x_stride_t = torch.tensor(x.stride(), dtype=torch.int64, device=x.device)\n    mask_stride_t = torch.tensor(mask_exp.stride(), dtype=torch.int64, device=x.device)\n    fill_stride_t = torch.tensor(fill_exp.stride(), dtype=torch.int64, device=x.device)\n    out_stride_t = torch.tensor(out.stride(), dtype=torch.int64, device=x.device)\n\n    n_elements = out.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    masked_fill_kernel[grid](\n        x,\n        mask_exp,\n        fill_exp,\n        out,\n        n_elements,\n        shape_t,\n        x_stride_t,\n        mask_stride_t,\n        fill_stride_t,\n        out_stride_t,\n        NDIMS=NDIMS,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n\ndef masked_fill_Scalar(self: torch.Tensor, mask: torch.Tensor, value):\n    # Create a 1-element tensor of the scalar value on device with matching dtype\n    fill = torch.tensor(value, dtype=self.dtype, device=self.device)\n    out = torch.empty_like(self)\n    _launch_masked_fill(self, mask, fill, out)\n    return out\n\n\ndef masked_fill_Tensor(self: torch.Tensor, mask: torch.Tensor, value: torch.Tensor):\n    fill = value.to(device=self.device, dtype=self.dtype)\n    out = torch.empty_like(self)\n    _launch_masked_fill(self, mask, fill, out)\n    return out\n\n\ndef masked_fill_Scalar_out(self: torch.Tensor, mask: torch.Tensor, value, out: torch.Tensor):\n    assert out.is_cuda and out.device == self.device\n    assert out.shape == self.shape\n    # Create a 1-element tensor of the scalar value on device with matching dtype\n    fill = torch.tensor(value, dtype=self.dtype, device=self.device)\n    _launch_masked_fill(self, mask, fill, out)\n    return out\n\n\ndef masked_fill_Tensor_out(self: torch.Tensor, mask: torch.Tensor, value: torch.Tensor, out: torch.Tensor):\n    assert out.is_cuda and out.device == self.device\n    assert out.shape == self.shape\n    fill = value.to(device=self.device, dtype=self.dtype)\n    _launch_masked_fill(self, mask, fill, out)\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.3571404606569213,
        "meets_threshold": false
      },
      "aten::flip": {
        "data_sources": {
          "batch2": {
            "gpt_speedup_vs_cuda": 0.261558487481919,
            "flaggems_speedup_vs_cuda": 1.095078,
            "speedup_vs_flaggems": 0.23884918469909816
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef flip_kernel(\n    in_ptr,             # *Pointer* to input tensor storage\n    out_ptr,            # *Pointer* to output tensor storage\n    sizes_ptr,          # *Pointer* to sizes array (length = NDIMS)\n    in_strides_ptr,     # *Pointer* to input strides array (length = NDIMS)\n    out_strides_ptr,    # *Pointer* to output strides array (length = NDIMS)\n    dims_mask_ptr,      # *Pointer* to mask array (1 if that dim is flipped else 0) (length = NDIMS)\n    n_elements,         # total number of elements\n    BLOCK_SIZE: tl.constexpr,\n    NDIMS: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    off = block_start + tl.arange(0, BLOCK_SIZE)\n    off = off.to(tl.int64)\n    mask = off < n_elements\n\n    # Initialize offsets for input and output\n    in_offset = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    out_offset = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Raveled index to multi-index decomposition and compute offsets\n    rem = off\n    # Iterate from last dimension to first\n    for d in range(NDIMS - 1, -1, -1):\n        size_d = tl.load(sizes_ptr + d).to(tl.int64)\n        stride_in_d = tl.load(in_strides_ptr + d).to(tl.int64)\n        stride_out_d = tl.load(out_strides_ptr + d).to(tl.int64)\n        dim_flip = tl.load(dims_mask_ptr + d)\n\n        coord = rem % size_d\n        rem = rem // size_d\n\n        flip_cond = dim_flip != 0\n        flipped_coord = tl.where(flip_cond, size_d - 1 - coord, coord)\n\n        in_offset += flipped_coord * stride_in_d\n        out_offset += coord * stride_out_d\n\n    val = tl.load(in_ptr + in_offset, mask=mask, other=0)\n    tl.store(out_ptr + out_offset, val, mask=mask)\n\n\ndef _normalize_dims(dims, rank):\n    if isinstance(dims, (int,)):\n        dims = [int(dims)]\n    elif isinstance(dims, (list, tuple)):\n        dims = [int(d) for d in dims]\n    elif isinstance(dims, torch.Tensor):\n        dims = [int(d.item()) for d in dims.flatten().tolist()]\n    else:\n        raise TypeError(\"dims must be int, list/tuple of int, or 1D Tensor of int\")\n\n    if rank == 0:\n        if len(dims) != 0:\n            raise RuntimeError(\"flip: dims must be empty for 0-dim tensors\")\n        return []\n\n    # Normalize negative dims and validate\n    normalized = []\n    for d in dims:\n        if d < 0:\n            d += rank\n        if d < 0 or d >= rank:\n            raise IndexError(f\"Dimension out of range (expected to be in range of [-{rank}, {rank - 1}], but got {d - (rank if d >= rank else 0)})\")\n        normalized.append(d)\n\n    # Ensure unique dims\n    if len(set(normalized)) != len(normalized):\n        raise RuntimeError(\"flip: dims must be unique\")\n    return normalized\n\n\ndef _flip_impl(x: torch.Tensor, dims, out: torch.Tensor = None):\n    if not x.is_cuda:\n        raise RuntimeError(\"This Triton flip implementation requires CUDA tensors.\")\n\n    rank = x.dim()\n    dims = _normalize_dims(dims, rank)\n\n    # Prepare output\n    if out is None:\n        out = torch.empty_like(x)\n    else:\n        if not out.is_cuda:\n            raise RuntimeError(\"out tensor must be a CUDA tensor.\")\n        if out.dtype != x.dtype:\n            raise RuntimeError(\"out tensor must have the same dtype as input.\")\n        if tuple(out.shape) != tuple(x.shape):\n            raise RuntimeError(f\"out tensor must have the same shape as input, got {tuple(out.shape)} vs {tuple(x.shape)}\")\n\n    # Early exit for empty tensors\n    n_elements = x.numel()\n    if n_elements == 0:\n        return out\n\n    device = x.device\n    # Build metadata arrays on device\n    sizes_t = torch.tensor(list(x.shape), device=device, dtype=torch.int64)\n    in_strides_t = torch.tensor(list(x.stride()), device=device, dtype=torch.int64)\n    out_strides_t = torch.tensor(list(out.stride()), device=device, dtype=torch.int64)\n    dims_mask = torch.zeros(rank, device=device, dtype=torch.int64)\n    if len(dims) > 0:\n        dims_mask[torch.tensor(dims, device=device, dtype=torch.long)] = 1\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda META: (triton.cdiv(n_elements, META[\"BLOCK_SIZE\"]),)\n    flip_kernel[grid](\n        x, out,\n        sizes_t, in_strides_t, out_strides_t, dims_mask,\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        NDIMS=rank,\n    )\n    return out\n\n\ndef flip(x: torch.Tensor, dims):\n    return _flip_impl(x, dims)\n\n\ndef flip_out(x: torch.Tensor, dims, out: torch.Tensor):\n    return _flip_impl(x, dims, out=out)",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.23884918469909816,
        "meets_threshold": false
      },
      "eq": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.15390458528247578,
            "flaggems_speedup_vs_cuda": 0.993317333333333,
            "speedup_vs_flaggems": 0.15493999764003832
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef eq_strided_kernel(\n    x_ptr,\n    y_ptr,\n    out_ptr,\n    shape_ptr,         # int64[n_dim]\n    x_strides_ptr,     # int64[n_dim]\n    y_strides_ptr,     # int64[n_dim]\n    out_strides_ptr,   # int64[n_dim]\n    n_dim,             # int32\n    n_elements,        # int64\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    linear_idx = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = linear_idx < n_elements\n\n    idx = linear_idx.to(tl.int64)\n    tmp = idx\n\n    offx = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    offy = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    offo = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    i = n_dim - 1\n    while i >= 0:\n        sd = tl.load(shape_ptr + i)          # size of dim i\n        rd = tmp % sd                        # coordinate in dim i\n        tmp = tmp // sd\n\n        sx = tl.load(x_strides_ptr + i)\n        sy = tl.load(y_strides_ptr + i)\n        so = tl.load(out_strides_ptr + i)\n\n        offx += rd * sx\n        offy += rd * sy\n        offo += rd * so\n\n        i -= 1\n\n    x = tl.load(x_ptr + offx, mask=mask, other=0)\n    y = tl.load(y_ptr + offy, mask=mask, other=0)\n    res = x == y\n    tl.store(out_ptr + offo, res, mask=mask)\n\n\n@triton.jit\ndef and_strided_kernel(\n    a_ptr,             # bool\n    b_ptr,             # bool\n    out_ptr,           # bool\n    shape_ptr,         # int64[n_dim]\n    a_strides_ptr,     # int64[n_dim]\n    b_strides_ptr,     # int64[n_dim]\n    out_strides_ptr,   # int64[n_dim]\n    n_dim,             # int32\n    n_elements,        # int64\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    linear_idx = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = linear_idx < n_elements\n\n    idx = linear_idx.to(tl.int64)\n    tmp = idx\n\n    offa = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    offb = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    offo = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    i = n_dim - 1\n    while i >= 0:\n        sd = tl.load(shape_ptr + i)\n        rd = tmp % sd\n        tmp = tmp // sd\n\n        sa = tl.load(a_strides_ptr + i)\n        sb = tl.load(b_strides_ptr + i)\n        so = tl.load(out_strides_ptr + i)\n\n        offa += rd * sa\n        offb += rd * sb\n        offo += rd * so\n\n        i -= 1\n\n    av = tl.load(a_ptr + offa, mask=mask, other=0)\n    bv = tl.load(b_ptr + offb, mask=mask, other=0)\n    res = av & bv\n    tl.store(out_ptr + offo, res, mask=mask)\n\n\ndef _broadcast_meta(a: torch.Tensor, b: torch.Tensor, out: torch.Tensor = None):\n    # Returns: (a_view, b_view, out_tensor, shape_meta, a_strides_meta, b_strides_meta, out_strides_meta, n_dim, n_elements, device)\n    device = a.device\n    a_view, b_view = torch.broadcast_tensors(a, b)\n    if out is not None:\n        if out.dtype != torch.bool:\n            raise TypeError(\"out must have dtype torch.bool\")\n        if out.device != device:\n            raise ValueError(\"out must be on the same device as inputs\")\n        if tuple(out.shape) != tuple(a_view.shape):\n            raise ValueError(\"out shape must match broadcasted shape\")\n        out_tensor = out\n    else:\n        out_tensor = torch.empty(a_view.shape, dtype=torch.bool, device=device)\n\n    shape_meta = torch.tensor(a_view.shape, dtype=torch.long, device=device)\n    a_strides_meta = torch.tensor(a_view.stride(), dtype=torch.long, device=device)\n    b_strides_meta = torch.tensor(b_view.stride(), dtype=torch.long, device=device)\n    out_strides_meta = torch.tensor(out_tensor.stride(), dtype=torch.long, device=device)\n\n    n_dim = len(a_view.shape)\n    n_elements = out_tensor.numel()\n    return (a_view, b_view, out_tensor, shape_meta, a_strides_meta, b_strides_meta, out_strides_meta, n_dim, n_elements, device)\n\n\ndef _launch_eq_tensors(a: torch.Tensor, b: torch.Tensor, out: torch.Tensor = None):\n    a_view, b_view, out_tensor, shape_meta, a_strides_meta, b_strides_meta, out_strides_meta, n_dim, n_elements, device = _broadcast_meta(a, b, out)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    eq_strided_kernel[grid](\n        a_view,\n        b_view,\n        out_tensor,\n        shape_meta,\n        a_strides_meta,\n        b_strides_meta,\n        out_strides_meta,\n        n_dim,\n        n_elements,\n        BLOCK_SIZE=1024,\n    )\n    return out_tensor\n\n\ndef _launch_and_tensors(a_bool: torch.Tensor, b_bool: torch.Tensor, out: torch.Tensor = None):\n    a_view, b_view, out_tensor, shape_meta, a_strides_meta, b_strides_meta, out_strides_meta, n_dim, n_elements, device = _broadcast_meta(a_bool, b_bool, out)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    and_strided_kernel[grid](\n        a_view,\n        b_view,\n        out_tensor,\n        shape_meta,\n        a_strides_meta,\n        b_strides_meta,\n        out_strides_meta,\n        n_dim,\n        n_elements,\n        BLOCK_SIZE=1024,\n    )\n    return out_tensor\n\n\ndef eq_Tensor(*args, **kwargs):\n    # Accepts: (self: Tensor, other: Tensor, out: Optional[Tensor]=None)\n    if len(args) >= 2:\n        a, b = args[0], args[1]\n    else:\n        a = kwargs.get(\"input\", kwargs.get(\"self\"))\n        b = kwargs.get(\"other\")\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(a, torch.Tensor) or not isinstance(b, torch.Tensor):\n        raise TypeError(\"eq_Tensor expects two torch.Tensor arguments\")\n\n    if a.device.type != \"cuda\" or b.device.type != \"cuda\":\n        raise ValueError(\"eq_Tensor requires CUDA tensors\")\n    if a.device != b.device:\n        raise ValueError(\"Input tensors must be on the same device\")\n\n    # Complex handling: compare real and imaginary parts\n    common_dtype = torch.result_type(a, b)\n    if common_dtype.is_complex:\n        ac = a.to(common_dtype)\n        bc = b.to(common_dtype)\n        ar = ac.real\n        ai = ac.imag\n        br = bc.real\n        bi = bc.imag\n        # Compare real and imag separately, then logical_and\n        real_eq = _launch_eq_tensors(ar, br)\n        imag_eq = _launch_eq_tensors(ai, bi)\n        return _launch_and_tensors(real_eq, imag_eq, out=out)\n\n    # Non-complex path\n    a_cast = a.to(common_dtype)\n    b_cast = b.to(common_dtype)\n    return _launch_eq_tensors(a_cast, b_cast, out=out)\n\n\ndef eq_Scalar(*args, **kwargs):\n    # Accepts: (self: Tensor, other: Scalar, out: Optional[Tensor]=None)\n    if len(args) >= 2:\n        a, other = args[0], args[1]\n    else:\n        a = kwargs.get(\"input\", kwargs.get(\"self\"))\n        other = kwargs.get(\"other\")\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(a, torch.Tensor):\n        raise TypeError(\"eq_Scalar expects a torch.Tensor as the first argument\")\n    if a.device.type != \"cuda\":\n        raise ValueError(\"eq_Scalar requires CUDA tensor\")\n\n    # Determine common dtype with scalar\n    if isinstance(other, torch.Tensor):\n        raise TypeError(\"eq_Scalar expects a Python scalar as 'other'\")\n    # Handle complex scalar vs tensor\n    if isinstance(other, complex) or a.dtype.is_complex:\n        # Promote to complex dtype\n        if a.dtype.is_complex:\n            complex_dtype = a.dtype\n        else:\n            complex_dtype = torch.complex64 if a.dtype in (torch.float16, torch.bfloat16, torch.float32) else torch.complex128\n        ac = a.to(complex_dtype)\n        bc = torch.tensor(other, dtype=complex_dtype, device=a.device)\n        br = bc.real\n        bi = bc.imag\n        ar = ac.real\n        ai = ac.imag\n        real_eq = _launch_eq_tensors(ar, br)\n        imag_eq = _launch_eq_tensors(ai, bi)\n        return _launch_and_tensors(real_eq, imag_eq, out=out)\n\n    # Non-complex scalar\n    # Compute result/common dtype with scalar by creating a 0-d tensor for result_type\n    scalar_tensor = torch.tensor(other, device=a.device)\n    common_dtype = torch.result_type(a, scalar_tensor)\n    a_cast = a.to(common_dtype)\n    b_tensor = torch.tensor(other, dtype=common_dtype, device=a.device)\n    return _launch_eq_tensors(a_cast, b_tensor, out=out)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.15493999764003832,
        "meets_threshold": false
      },
      "select_scatter": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.1135903225019227,
            "flaggems_speedup_vs_cuda": 1.24622733333333,
            "speedup_vs_flaggems": 0.09114735286546677
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef select_scatter_kernel(\n    in_ptr,         # pointer to input tensor\n    src_ptr,        # pointer to source tensor\n    out_ptr,        # pointer to output tensor\n    shape_ptr,      # pointer to int32 array of shape (MAX_RANK,)\n    log_strides_ptr,# pointer to int32 array of logical (contiguous) strides (MAX_RANK,)\n    in_strides_ptr, # pointer to int32 array of input strides (MAX_RANK,)\n    out_strides_ptr,# pointer to int32 array of output strides (MAX_RANK,)\n    src_spad_ptr,   # pointer to int32 array of src strides padded to MAX_RANK with 0 at dim (MAX_RANK,)\n    n_elements,     # total number of elements in input/output\n    dim,            # dimension along which to select\n    index,          # index along `dim` to scatter from src\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Accumulate memory offsets for in, out, src\n    mem_in = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    mem_out = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    mem_src = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Compute coordinates and memory offsets\n    # Loop over MAX_RANK = 8\n    for d in range(8):\n        size_d = tl.load(shape_ptr + d)\n        logd = tl.load(log_strides_ptr + d)\n        # coord_d = (offsets // logd) % size_d\n        coord_d = (offsets // logd)\n        coord_d = coord_d % size_d\n\n        str_in_d = tl.load(in_strides_ptr + d).to(tl.int64)\n        str_out_d = tl.load(out_strides_ptr + d).to(tl.int64)\n        str_src_d = tl.load(src_spad_ptr + d).to(tl.int64)\n\n        mem_in += coord_d.to(tl.int64) * str_in_d\n        mem_out += coord_d.to(tl.int64) * str_out_d\n        mem_src += coord_d.to(tl.int64) * str_src_d\n\n    # Compute coord at the selected dim for condition\n    size_dim = tl.load(shape_ptr + dim)\n    log_dim = tl.load(log_strides_ptr + dim)\n    coord_dim = (offsets // log_dim) % size_dim\n    cond = coord_dim == index\n\n    # Load values\n    val_in = tl.load(in_ptr + mem_in, mask=mask)\n    val_src = tl.load(src_ptr + mem_src, mask=mask & cond)\n\n    # Select between input and src\n    out_val = tl.where(cond, val_src, val_in)\n\n    # Store result\n    tl.store(out_ptr + mem_out, out_val, mask=mask)\n\n\ndef select_scatter(*args, **kwargs):\n    # Parse arguments to match torch.select_scatter(input, src, dim, index)\n    if len(args) >= 4:\n        input_tensor, src_tensor, dim, index = args[:4]\n    else:\n        input_tensor = kwargs.get('input', kwargs.get('self', None))\n        src_tensor = kwargs.get('src', None)\n        dim = kwargs.get('dim', None)\n        index = kwargs.get('index', None)\n\n    assert input_tensor is not None and src_tensor is not None and dim is not None and index is not None, \\\n        \"select_scatter requires (input, src, dim, index)\"\n\n    device = input_tensor.device\n    output = torch.empty_like(input_tensor)\n\n    # Normalize dim\n    rank = input_tensor.dim()\n    if dim < 0:\n        dim = dim + rank\n    assert 0 <= dim < rank, \"dim out of range\"\n\n    # Normalize index\n    shape = list(input_tensor.shape)\n    if index < 0:\n        index = index + shape[dim]\n    assert 0 <= index < shape[dim], \"index out of range\"\n\n    # Validate src shape: input with dim removed equals src shape\n    expected_src_shape = tuple(shape[:dim] + shape[dim+1:])\n    assert tuple(src_tensor.shape) == expected_src_shape, \\\n        f\"src shape must match input.select(dim, index) shape: expected {expected_src_shape}, got {tuple(src_tensor.shape)}\"\n\n    # Prepare strides and shapes\n    MAX_RANK = 8\n\n    # Shapes padded\n    shape_pad = shape + [1] * (MAX_RANK - rank)\n\n    # Logical (contiguous) strides for coordinate recovery\n    log_strides = [1] * rank\n    prod = 1\n    for i in range(rank - 1, -1, -1):\n        log_strides[i] = prod\n        prod *= shape[i]\n    log_strides_pad = log_strides + [1] * (MAX_RANK - rank)\n\n    # Strides (in elements)\n    in_strides = list(input_tensor.stride())\n    out_strides = list(output.stride())\n    src_strides = list(src_tensor.stride())\n\n    in_strides_pad = in_strides + [0] * (MAX_RANK - rank)\n    out_strides_pad = out_strides + [0] * (MAX_RANK - rank)\n\n    # Src strides padded with 0 at selected dim; mapping dims < dim same index, dims > dim shift by -1\n    src_spad = [0] * MAX_RANK\n    for i in range(rank):\n        if i < dim:\n            src_spad[i] = src_strides[i]\n        elif i > dim:\n            src_spad[i] = src_strides[i - 1]\n        else:\n            src_spad[i] = 0  # selected dim is removed in src\n    # For dims beyond rank, keep 0\n\n    # Convert to torch tensors (int32) on device\n    shape_t = torch.tensor(shape_pad, dtype=torch.int32, device=device)\n    log_strides_t = torch.tensor(log_strides_pad, dtype=torch.int32, device=device)\n    in_strides_t = torch.tensor(in_strides_pad, dtype=torch.int32, device=device)\n    out_strides_t = torch.tensor(out_strides_pad, dtype=torch.int32, device=device)\n    src_spad_t = torch.tensor(src_spad, dtype=torch.int32, device=device)\n\n    n_elements = input_tensor.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    select_scatter_kernel[grid](\n        input_tensor, src_tensor, output,\n        shape_t, log_strides_t,\n        in_strides_t, out_strides_t, src_spad_t,\n        n_elements, dim, index,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return output\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.09114735286546677,
        "meets_threshold": false
      },
      "tile": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.10181148102915683,
            "flaggems_speedup_vs_cuda": 1.130476,
            "speedup_vs_flaggems": 0.09006071869651087
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef tile(in_ptr,\n         out_ptr,\n         n_elements,\n         out_shape_ptr,\n         out_strides_ptr,\n         in_shape_ptr,\n         in_strides_ptr,\n         BLOCK_SIZE: tl.constexpr,\n         MAX_DIMS: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    input_offsets = tl.zeros([BLOCK_SIZE], dtype=tl.int32)\n    # Unroll over dimensions\n    for k in range(MAX_DIMS):\n        out_stride_k = tl.load(out_strides_ptr + k)\n        out_shape_k = tl.load(out_shape_ptr + k)\n        in_shape_k = tl.load(in_shape_ptr + k)\n        in_stride_k = tl.load(in_strides_ptr + k)\n\n        idx_out_k = (offsets // out_stride_k) % out_shape_k\n        idx_in_k = idx_out_k % in_shape_k\n        input_offsets += idx_in_k * in_stride_k\n\n    x = tl.load(in_ptr + input_offsets, mask=mask)\n    tl.store(out_ptr + offsets, x, mask=mask)\n\n\n# Keep a reference to the kernel before defining the wrapper with the same name\n_tile_kernel = tile\n\n\ndef tile(*args, **kwargs):\n    # Parse inputs similar to torch.tile(input, dims)\n    if len(args) == 0:\n        raise TypeError(\"tile() missing required positional argument: 'input'\")\n    x = args[0]\n    if len(args) >= 2:\n        dims = args[1]\n    else:\n        dims = kwargs.get('dims', kwargs.get('reps', None))\n    if dims is None:\n        raise TypeError(\"tile() missing required argument 'dims' (pos 2)\")\n\n    if isinstance(dims, int):\n        reps = [dims]\n    else:\n        reps = list(dims)\n\n    x = x.contiguous()\n    device = x.device\n    dtype = x.dtype\n\n    in_shape = list(x.shape)\n    in_ndim = len(in_shape)\n    reps_ndim = len(reps)\n    out_ndim = max(in_ndim, reps_ndim)\n\n    # Left pad shapes to out_ndim\n    in_shape_pad = [1] * (out_ndim - in_ndim) + in_shape\n    reps_pad = [1] * (out_ndim - reps_ndim) + reps\n\n    # Compute output shape\n    out_shape = [in_shape_pad[i] * reps_pad[i] for i in range(out_ndim)]\n\n    # Compute contiguous strides\n    in_strides_pad = [0] * out_ndim\n    running = 1\n    for i in range(out_ndim - 1, -1, -1):\n        in_strides_pad[i] = running if in_shape_pad[i] > 0 else 0\n        running *= in_shape_pad[i]\n\n    out_strides = [0] * out_ndim\n    running = 1\n    for i in range(out_ndim - 1, -1, -1):\n        out_strides[i] = running if out_shape[i] > 0 else 0\n        running *= out_shape[i]\n\n    n_elements = running\n\n    # Set MAX_DIMS\n    MAX_DIMS = 8\n    pre_pad = MAX_DIMS - out_ndim\n    if pre_pad < 0:\n        raise ValueError(f\"tile: number of dimensions {out_ndim} exceeds MAX_DIMS={MAX_DIMS}\")\n\n    final_out_shape = [1] * pre_pad + out_shape\n    final_out_strides = [1] * pre_pad + out_strides\n    final_in_shape = [1] * pre_pad + in_shape_pad\n    final_in_strides = [0] * pre_pad + in_strides_pad\n\n    # Allocate output\n    out = torch.empty(out_shape, dtype=dtype, device=device)\n\n    # Prepare metadata tensors on device (int32)\n    out_shape_t = torch.tensor(final_out_shape, dtype=torch.int32, device=device)\n    out_strides_t = torch.tensor(final_out_strides, dtype=torch.int32, device=device)\n    in_shape_t = torch.tensor(final_in_shape, dtype=torch.int32, device=device)\n    in_strides_t = torch.tensor(final_in_strides, dtype=torch.int32, device=device)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _tile_kernel[grid](\n        x, out, n_elements,\n        out_shape_t, out_strides_t,\n        in_shape_t, in_strides_t,\n        BLOCK_SIZE=BLOCK_SIZE,\n        MAX_DIMS=MAX_DIMS\n    )\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.09006071869651087,
        "meets_threshold": false
      },
      "repeat": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.06905121626469231,
            "flaggems_speedup_vs_cuda": 1.131568,
            "speedup_vs_flaggems": 0.06102259542925596
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef repeat(x_ptr,\n           out_ptr,\n           out_numel,\n           ndims,\n           out_shape_ptr,\n           out_strides_ptr,\n           base_shape_ptr,\n           in_strides_ptr,\n           BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    offsets = offsets.to(tl.int64)\n    mask = offsets < out_numel\n\n    tmp = offsets\n    in_index = tl.zeros((BLOCK_SIZE,), dtype=tl.int64)\n    i = 0\n    while i < ndims:\n        out_stride_i = tl.load(out_strides_ptr + i)\n        out_shape_i = tl.load(out_shape_ptr + i)\n        base_shape_i = tl.load(base_shape_ptr + i)\n        in_stride_i = tl.load(in_strides_ptr + i)\n\n        o_i = tmp // out_stride_i\n        o_i = o_i % out_shape_i\n        in_i = o_i % base_shape_i\n\n        in_index += in_i * in_stride_i\n        tmp = tmp % out_stride_i\n        i += 1\n\n    x_val = tl.load(x_ptr + in_index, mask=mask)\n    tl.store(out_ptr + offsets, x_val, mask=mask)\n\n\n# Preserve reference to the Triton kernel before defining the Python wrapper with the same name.\nrepeat_kernel = repeat\n\n\ndef repeat(*args, **kwargs):\n    if len(args) == 0:\n        raise TypeError(\"repeat expects a tensor as the first argument\")\n    x = args[0]\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"First argument to repeat must be a torch.Tensor\")\n\n    # Parse repeats\n    repeats = None\n    if len(args) >= 2:\n        if len(args) == 2 and isinstance(args[1], (tuple, list)):\n            repeats = tuple(int(r) for r in args[1])\n        else:\n            repeats = tuple(int(r) for r in args[1:])\n    else:\n        rep = kwargs.get('repeats', kwargs.get('sizes', None))\n        if rep is None:\n            raise TypeError(\"repeat missing repeat sizes\")\n        if isinstance(rep, (tuple, list)):\n            repeats = tuple(int(r) for r in rep)\n        else:\n            repeats = (int(rep),)\n\n    if any(r < 0 for r in repeats):\n        raise ValueError(\"repeat sizes must be non-negative\")\n\n    x_dims = tuple(x.shape)\n    x_ndims = x.dim()\n    ndims = len(repeats)\n    if ndims < x_ndims:\n        raise RuntimeError(\"Number of repeat dimensions must be >= tensor dimensions\")\n\n    extra = ndims - x_ndims\n    base_shape = (1,) * extra + x_dims\n    out_shape = tuple(base_shape[i] * repeats[i] for i in range(ndims))\n\n    # Compute numel\n    out_numel = 1\n    for d in out_shape:\n        out_numel *= d\n\n    out = torch.empty(out_shape, dtype=x.dtype, device=x.device)\n\n    if out_numel == 0:\n        return out\n\n    if not x.is_cuda or not out.is_cuda:\n        raise RuntimeError(\"Triton kernels require CUDA tensors\")\n    if not x.is_contiguous():\n        x = x.contiguous()\n\n    def row_major_strides(shape):\n        strides = []\n        acc = 1\n        for s in reversed(shape):\n            strides.insert(0, acc)\n            acc *= s\n        return tuple(strides)\n\n    out_strides = row_major_strides(out_shape)\n    in_strides = row_major_strides(base_shape)\n\n    device = x.device\n    out_shape_t = torch.tensor(out_shape, dtype=torch.int64, device=device)\n    out_strides_t = torch.tensor(out_strides, dtype=torch.int64, device=device)\n    base_shape_t = torch.tensor(base_shape, dtype=torch.int64, device=device)\n    in_strides_t = torch.tensor(in_strides, dtype=torch.int64, device=device)\n\n    x_flat = x.reshape(-1)\n    out_flat = out.reshape(-1)\n\n    grid = lambda meta: (triton.cdiv(out_numel, meta['BLOCK_SIZE']),)\n    repeat_kernel[grid](\n        x_flat, out_flat,\n        out_numel, ndims,\n        out_shape_t, out_strides_t,\n        base_shape_t, in_strides_t,\n        BLOCK_SIZE=1024\n    )\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.06102259542925596,
        "meets_threshold": false
      },
      "flip": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.04280880802610522,
            "flaggems_speedup_vs_cuda": 1.095078,
            "speedup_vs_flaggems": 0.039092017213481794
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef flip(\n    x_ptr,                    # *Pointer* to input tensor storage (elements)\n    out_ptr,                  # *Pointer* to output tensor storage (elements, contiguous)\n    n_elements,               # Number of elements in the tensor\n    n_dims,                   # Actual number of dimensions\n    sizes_ptr,                # Pointer to sizes array (length MAX_DIMS, int32)\n    strides_ptr,              # Pointer to input strides array (length MAX_DIMS, int64)\n    flip_mask_ptr,            # Pointer to flip mask array (length MAX_DIMS, int32; 1 if flip)\n    x_storage_offset,         # Input storage offset (elements, int64)\n    BLOCK_SIZE: tl.constexpr,\n    MAX_DIMS: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offs = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < n_elements\n\n    # Work in 64-bit for index math\n    offs64 = offs.to(tl.int64)\n\n    # Convert linear index to multidimensional index and then to source linear index using input strides.\n    rem = offs64\n    src_index = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Process dimensions from last to first (row-major)\n    for di in range(MAX_DIMS):\n        d = MAX_DIMS - 1 - di\n        size_d_i32 = tl.load(sizes_ptr + d)\n        stride_d_i64 = tl.load(strides_ptr + d)\n        flip_d_i32 = tl.load(flip_mask_ptr + d)\n\n        size_d = size_d_i32.to(tl.int64)\n        # Avoid div-by-zero for padded dims: size=1 for padded dims\n        coord_d = rem % size_d\n        rem = rem // size_d\n\n        do_flip = flip_d_i32 != 0\n        src_coord_d = tl.where(do_flip, size_d - 1 - coord_d, coord_d)\n\n        src_index += src_coord_d * stride_d_i64\n\n    src_index += x_storage_offset.to(tl.int64)\n\n    vals = tl.load(x_ptr + src_index, mask=mask)\n    tl.store(out_ptr + offs64, vals, mask=mask)\n\n\n# Keep a reference to the kernel before defining the Python wrapper with the same name.\nflip_kernel = flip\n\n\ndef flip(*args, **kwargs):\n    # Parse arguments similar to torch.flip(input, dims)\n    # Supported calling patterns:\n    #   flip(input, dims)\n    #   flip(input, dims=...)\n    if len(args) == 0 and \"input\" in kwargs:\n        x = kwargs[\"input\"]\n    else:\n        x = args[0]\n\n    if len(args) >= 2:\n        dims = args[1]\n    else:\n        dims = kwargs.get(\"dims\", kwargs.get(\"dim\", None))\n\n    if dims is None:\n        raise ValueError(\"flip requires 'dims' argument\")\n\n    if isinstance(dims, int):\n        dims = (dims,)\n    else:\n        dims = tuple(int(d) for d in dims)\n\n    device = x.device\n    if device.type != \"cuda\":\n        raise RuntimeError(\"This Triton implementation of flip requires a CUDA device.\")\n\n    # Normalize dims\n    ndims = x.dim()\n    norm_dims = []\n    for d in dims:\n        if d < 0:\n            d = d + ndims\n        if not (0 <= d < ndims):\n            raise IndexError(f\"Dimension out of range (expected to be in range of [{-ndims}, {ndims - 1}], but got {d - ndims if d >= ndims else d})\")\n        norm_dims.append(d)\n    norm_dims = tuple(norm_dims)\n\n    n_elements = x.numel()\n    # Handle empty tensor quickly\n    if n_elements == 0:\n        return torch.empty_like(x)\n\n    # Prepare output as contiguous tensor\n    out = torch.empty(x.shape, dtype=x.dtype, device=device)\n\n    # Prepare metadata arrays\n    MAX_DIMS = 16\n    if ndims > MAX_DIMS:\n        raise RuntimeError(f\"flip kernel supports up to {MAX_DIMS} dimensions, got {ndims}\")\n\n    sizes = list(x.shape) + [1] * (MAX_DIMS - ndims)\n    in_strides = list(x.stride()) + [0] * (MAX_DIMS - ndims)\n    flip_mask = [0] * MAX_DIMS\n    for d in norm_dims:\n        flip_mask[d] = 1\n\n    sizes_t = torch.tensor(sizes, dtype=torch.int32, device=device)\n    strides_t = torch.tensor(in_strides, dtype=torch.int64, device=device)\n    flip_mask_t = torch.tensor(flip_mask, dtype=torch.int32, device=device)\n    storage_offset = torch.tensor(x.storage_offset(), dtype=torch.int64, device=device)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n\n    flip_kernel[grid](\n        x, out,\n        n_elements,\n        ndims,\n        sizes_t, strides_t, flip_mask_t,\n        storage_offset.item(),  # pass as Python int (will be converted)\n        BLOCK_SIZE=BLOCK_SIZE,\n        MAX_DIMS=MAX_DIMS,\n    )\n    return out\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.039092017213481794,
        "meets_threshold": false
      },
      "bitwise_and": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.03562061271858905,
            "flaggems_speedup_vs_cuda": 0.97993,
            "speedup_vs_flaggems": 0.036350160438591586
          }
        },
        "code": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Core Triton kernel for elementwise bitwise_and with broadcasting via strides\n@triton.jit\ndef bitwise_and_kernel(\n    x_ptr, y_ptr, out_ptr,\n    shape_ptr, x_strides_ptr, y_strides_ptr, out_strides_ptr,\n    ndim, n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    MAX_DIMS: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Work in int64 for index math\n    lin = offsets.to(tl.int64)\n\n    # Accumulate per-operand element offsets using strides\n    x_index = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    y_index = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n    out_index = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Decompose linear index into multi-dimensional indices and compute offsets\n    rem = lin\n    # Iterate from last dim to first\n    for d in tl.static_range(0, MAX_DIMS):\n        rd = MAX_DIMS - 1 - d  # reverse dimension index\n        active = rd < ndim\n        size_d = tl.load(shape_ptr + rd, mask=active, other=1).to(tl.int64)\n\n        idx_d = rem % size_d\n        rem = rem // size_d\n\n        xs = tl.load(x_strides_ptr + rd, mask=active, other=0).to(tl.int64)\n        ys = tl.load(y_strides_ptr + rd, mask=active, other=0).to(tl.int64)\n        os = tl.load(out_strides_ptr + rd, mask=active, other=0).to(tl.int64)\n\n        x_index += idx_d * xs\n        y_index += idx_d * ys\n        out_index += idx_d * os\n\n    x = tl.load(x_ptr + x_index, mask=mask, other=0)\n    y = tl.load(y_ptr + y_index, mask=mask, other=0)\n    r = x & y\n    tl.store(out_ptr + out_index, r, mask=mask)\n\n\n# Internal utility to prepare and launch the kernel\ndef _bitwise_and_impl(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor = None):\n    if not (x.is_cuda and (isinstance(y, torch.Tensor) and y.is_cuda or True)):\n        # y may be scalar-tensor; we ensure below for tensor inputs\n        pass\n\n    # Determine result dtype using PyTorch's promotion rules\n    # y can be a Tensor on any device at this point\n    try:\n        result_dtype = torch.result_type(x, y)\n    except Exception:\n        # If y isn't a Tensor (e.g., scalar), result_type might still work; else rethrow\n        result_dtype = torch.result_type(x, y)\n\n    # Disallow floating-point/complex dtypes\n    if result_dtype.is_floating_point or result_dtype.is_complex:\n        raise TypeError(\"bitwise_and is only supported for integer and boolean dtypes\")\n\n    # Device\n    device = x.device\n    # Ensure tensor y is on device and dtype-cast will be handled by callers\n\n    # Compute broadcasted shape\n    if isinstance(y, torch.Tensor):\n        out_shape = torch.broadcast_shapes(x.shape, y.shape)\n    else:\n        # Shouldn't happen here; scalar variants wrap y into Tensor before calling this\n        out_shape = x.shape\n\n    # Handle out tensor\n    if out is not None:\n        if out.device != device:\n            raise ValueError(\"out tensor must be on the same device as inputs\")\n        if tuple(out.shape) != tuple(out_shape):\n            raise ValueError(f\"out tensor has incorrect shape: expected {out_shape}, got {tuple(out.shape)}\")\n        if out.dtype != result_dtype:\n            raise ValueError(f\"out tensor has incorrect dtype: expected {result_dtype}, got {out.dtype}\")\n        out_t = out\n    else:\n        out_t = torch.empty(out_shape, dtype=result_dtype, device=device)\n\n    # Special handling for bool dtype to ensure robust Triton support\n    compute_dtype = result_dtype\n    use_bool_temp = False\n    if result_dtype == torch.bool:\n        compute_dtype = torch.uint8\n        use_bool_temp = True\n\n    # Cast inputs to compute dtype and move to device\n    x_t = x.to(dtype=compute_dtype, device=device, copy=False)\n    y_t = y.to(dtype=compute_dtype, device=device, copy=False) if isinstance(y, torch.Tensor) else y\n\n    # Expand to output shape to get broadcasting strides\n    x_exp = x_t.expand(out_shape)\n    if isinstance(y_t, torch.Tensor):\n        y_exp = y_t.expand(out_shape)\n    else:\n        # y is not a tensor here; scalar variants should have wrapped it earlier\n        raise RuntimeError(\"Internal error: y must be a Tensor in _bitwise_and_impl\")\n\n    out_compute = out_t if not use_bool_temp else torch.empty_like(out_t, dtype=torch.uint8, device=device)\n    out_exp = out_compute  # out tensor already has target shape\n\n    # Number of elements\n    n_elements = out_compute.numel()\n    if n_elements == 0:\n        if use_bool_temp:\n            if out is None:\n                return out_compute.to(torch.bool)\n            else:\n                out.copy_(out_compute.to(torch.bool))\n                return out\n        else:\n            return out_t\n\n    # Prepare shape and stride tensors\n    ndim = len(out_shape)\n    MAX_DIMS = 8\n    if ndim > MAX_DIMS:\n        raise ValueError(f\"Number of dimensions {ndim} exceeds MAX_DIMS={MAX_DIMS}\")\n\n    # Strides are in elements already in PyTorch\n    def to_i64_dev(arr):\n        return torch.tensor(arr, dtype=torch.int64, device=device)\n\n    shape_i64 = to_i64_dev(list(out_shape) + [1] * (MAX_DIMS - ndim))\n    x_strides_i64 = to_i64_dev(list(x_exp.stride()) + [0] * (MAX_DIMS - ndim))\n    y_strides_i64 = to_i64_dev(list(y_exp.stride()) + [0] * (MAX_DIMS - ndim))\n    out_strides_i64 = to_i64_dev(list(out_exp.stride()) + [0] * (MAX_DIMS - ndim))\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda META: (triton.cdiv(n_elements, META[\"BLOCK_SIZE\"]),)\n    bitwise_and_kernel[grid](\n        x_exp, y_exp, out_exp,\n        shape_i64, x_strides_i64, y_strides_i64, out_strides_i64,\n        ndim, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        MAX_DIMS=MAX_DIMS,\n    )\n\n    if use_bool_temp:\n        result = out_compute.to(torch.bool)\n        if out is None:\n            return result\n        else:\n            out.copy_(result)\n            return out\n    else:\n        return out_t\n\n\n# Wrapper for ('bitwise_and.Tensor', <Autograd.disable: False>)\ndef bitwise_and_Tensor(*args, **kwargs):\n    # Expect: (input: Tensor, other: Tensor, *, out: Optional[Tensor]=None)\n    if len(args) >= 2:\n        x, y = args[0], args[1]\n    else:\n        x = kwargs.get(\"input\", None)\n        y = kwargs.get(\"other\", None)\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n        raise TypeError(\"bitwise_and_Tensor expects two Tensor arguments\")\n    if not x.is_cuda or not y.is_cuda:\n        raise ValueError(\"Inputs must be CUDA tensors\")\n\n    # Validate dtypes\n    if (x.dtype.is_floating_point or x.dtype.is_complex) or (y.dtype.is_floating_point or y.dtype.is_complex):\n        raise TypeError(\"bitwise_and is only supported for integer and boolean dtypes\")\n\n    return _bitwise_and_impl(x, y, out=out)\n\n\n# Wrapper for ('bitwise_and.Scalar', <Autograd.disable: False>)  -> (Tensor, Scalar)\ndef bitwise_and_Scalar(*args, **kwargs):\n    # Expect: (input: Tensor, other: Scalar, *, out: Optional[Tensor]=None)\n    if len(args) >= 2:\n        x, scalar = args[0], args[1]\n    else:\n        x = kwargs.get(\"input\", None)\n        scalar = kwargs.get(\"other\", None)\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"bitwise_and_Scalar expects first argument to be a Tensor\")\n    if not x.is_cuda:\n        raise ValueError(\"Input tensor must be CUDA\")\n\n    # Determine result dtype as PyTorch would\n    scalar_cpu = torch.tensor(scalar)\n    result_dtype = torch.result_type(x, scalar_cpu)\n    if result_dtype.is_floating_point or result_dtype.is_complex:\n        raise TypeError(\"bitwise_and is only supported for integer and boolean dtypes\")\n\n    # Create a 1-element tensor for scalar on device with correct dtype\n    y = torch.tensor(scalar, dtype=result_dtype, device=x.device)\n\n    # Delegate to core implementation\n    return _bitwise_and_impl(x, y, out=out)\n\n\n# Wrapper for ('bitwise_and.Scalar_Tensor', <Autograd.disable: False>)  -> (Scalar, Tensor)\ndef bitwise_and_Scalar_Tensor(*args, **kwargs):\n    # Expect: (input: Scalar, other: Tensor, *, out: Optional[Tensor]=None)\n    if len(args) >= 2:\n        scalar, y = args[0], args[1]\n    else:\n        scalar = kwargs.get(\"input\", None)\n        y = kwargs.get(\"other\", None)\n    out = kwargs.get(\"out\", None)\n\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"bitwise_and_Scalar_Tensor expects second argument to be a Tensor\")\n    if not y.is_cuda:\n        raise ValueError(\"Input tensor must be CUDA\")\n\n    # Determine result dtype as PyTorch would\n    scalar_cpu = torch.tensor(scalar)\n    result_dtype = torch.result_type(scalar_cpu, y)\n    if result_dtype.is_floating_point or result_dtype.is_complex:\n        raise TypeError(\"bitwise_and is only supported for integer and boolean dtypes\")\n\n    # Create a 1-element tensor for scalar on device with correct dtype\n    x = torch.tensor(scalar, dtype=result_dtype, device=y.device)\n\n    # Delegate to core implementation\n    return _bitwise_and_impl(x, y, out=out)\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.036350160438591586,
        "meets_threshold": false
      },
      "hstack": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.02880040774173683,
            "flaggems_speedup_vs_cuda": 1.11853966666667,
            "speedup_vs_flaggems": 0.025748222079208107
          }
        },
        "code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef hstack(src_ptr,\n           dst_ptr,\n           src_numel,\n           rank,\n           concat_dim,\n           offset,\n           src_prods_ptr,\n           src_sizes_ptr,\n           out_prods_ptr,\n           BLOCK_SIZE: tl.constexpr,\n           MAX_RANK: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offs = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offs < src_numel\n\n    # Load source values\n    x = tl.load(src_ptr + offs, mask=mask)\n\n    # Compute output linear index for each source element\n    t = offs.to(tl.int64)\n    out_idx = tl.zeros([BLOCK_SIZE], dtype=tl.int64)\n\n    # Unrolled loop over dimensions up to MAX_RANK\n    for j in range(MAX_RANK):\n        p_src_j = tl.load(src_prods_ptr + j)\n        size_j = tl.load(src_sizes_ptr + j)\n        p_out_j = tl.load(out_prods_ptr + j)\n        ij = (t // p_src_j) % size_j\n        out_idx += ij * p_out_j\n\n    # Add offset for the concatenation dimension\n    p_out_concat = tl.load(out_prods_ptr + concat_dim)\n    out_idx += (offset.to(tl.int64)) * p_out_concat\n\n    tl.store(dst_ptr + out_idx, x, mask=mask)\n\n\n# Alias the kernel before defining the wrapper with the same name\nhstack_kernel = hstack\n\n\ndef hstack(*args, **kwargs):\n    # Parse inputs similar to torch.hstack: sequence of tensors as first positional arg or variadic\n    if len(args) == 0:\n        raise ValueError(\"hstack expects at least one tensor.\")\n    if len(args) == 1 and isinstance(args[0], (list, tuple)):\n        tensors = list(args[0])\n    else:\n        tensors = list(args)\n\n    if len(tensors) == 0:\n        raise ValueError(\"hstack expects at least one tensor.\")\n\n    # Handle optional out kwarg\n    out_kw = kwargs.get(\"out\", None)\n\n    # Ensure all inputs are tensors and on the same CUDA device\n    if not all(isinstance(t, torch.Tensor) for t in tensors):\n        raise TypeError(\"All inputs to hstack must be torch.Tensor.\")\n    device = tensors[0].device\n    if device.type != 'cuda':\n        raise RuntimeError(\"This Triton hstack implementation requires CUDA tensors.\")\n    if not all(t.device == device for t in tensors):\n        raise RuntimeError(\"All input tensors must be on the same device.\")\n    dtype = tensors[0].dtype\n    if not all(t.dtype == dtype for t in tensors):\n        raise RuntimeError(\"All input tensors must have the same dtype.\")\n\n    # Make tensors contiguous\n    xs = [t.contiguous() for t in tensors]\n\n    # Validate dimensions\n    r = xs[0].ndim\n    if not all(x.ndim == r for x in xs):\n        raise RuntimeError(\"All input tensors must have the same number of dimensions.\")\n    dim = 0 if r == 1 else 1\n\n    # Validate shapes (all dims except concat dim must match)\n    ref_shape = xs[0].shape\n    for x in xs[1:]:\n        for d in range(r):\n            if d == dim:\n                continue\n            if x.shape[d] != ref_shape[d]:\n                raise RuntimeError(\"Input shapes must match in all dimensions except the concatenation dimension.\")\n\n    # Compute output shape\n    out_shape = list(ref_shape)\n    out_shape[dim] = sum(x.shape[dim] for x in xs)\n\n    # Prepare output tensor\n    if out_kw is None:\n        out = torch.empty(out_shape, dtype=dtype, device=device)\n    else:\n        out = out_kw\n        if not isinstance(out, torch.Tensor):\n            raise TypeError(\"out must be a torch.Tensor.\")\n        if out.device != device or out.dtype != dtype:\n            raise RuntimeError(\"out must have the same device and dtype as inputs.\")\n        if list(out.shape) != out_shape:\n            raise RuntimeError(f\"out tensor has incorrect shape. Expected {out_shape}, got {list(out.shape)}.\")\n        out = out.contiguous()\n\n    # Helper to compute contiguous products (strides) for an arbitrary shape\n    def compute_products(shape, max_rank):\n        r_local = len(shape)\n        prods = [1] * max_rank\n        for j in range(r_local):\n            p = 1\n            for k in range(j + 1, r_local):\n                p *= int(shape[k])\n            prods[j] = int(p)\n        for j in range(r_local, max_rank):\n            prods[j] = 1\n        return prods\n\n    MAX_RANK = 8\n    out_prods = compute_products(out.shape, MAX_RANK)\n    out_prods_dev = torch.tensor(out_prods, dtype=torch.long, device=device)\n\n    # Launch kernel for each input tensor with appropriate offset along concat dimension\n    base_offset = 0\n    BLOCK_SIZE = 1024\n\n    for x in xs:\n        src_sizes = list(x.shape)\n        src_prods = compute_products(src_sizes, MAX_RANK)\n        src_sizes_dev = torch.tensor([int(s) for s in src_sizes] + [1] * (MAX_RANK - len(src_sizes)),\n                                     dtype=torch.long, device=device)\n        src_prods_dev = torch.tensor(src_prods, dtype=torch.long, device=device)\n\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n        hstack_kernel[grid](\n            x,\n            out,\n            n_elements,\n            r,\n            dim,\n            int(base_offset),\n            src_prods_dev,\n            src_sizes_dev,\n            out_prods_dev,\n            BLOCK_SIZE=BLOCK_SIZE,\n            MAX_RANK=MAX_RANK\n        )\n\n        base_offset += int(x.shape[dim])\n\n    return out",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.025748222079208107,
        "meets_threshold": false
      },
      "scaled_dot_product_attention": {
        "data_sources": {
          "batch1": {
            "gpt_speedup_vs_cuda": 0.012659094258894506,
            "flaggems_speedup_vs_cuda": 0.9044445,
            "speedup_vs_flaggems": 0.013996540704149902
          }
        },
        "code": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef scaled_dot_product_attention(\n    q_ptr, k_ptr, v_ptr, o_ptr,\n    B, H, Nq, Nk, D,\n    stride_qb, stride_qh, stride_qn, stride_qd,\n    stride_kb, stride_kh, stride_kn, stride_kd,\n    stride_vb, stride_vh, stride_vn, stride_vd,\n    stride_ob, stride_oh, stride_on, stride_od,\n    scale,\n    is_causal,\n    BLOCK_K: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n):\n    pid_n = tl.program_id(0)  # query index\n    pid_bh = tl.program_id(1)  # combined batch-head index\n\n    b = pid_bh // H\n    h = pid_bh % H\n\n    # Base pointers for this (b, h)\n    q_row_ptr = q_ptr + b * stride_qb + h * stride_qh + pid_n * stride_qn\n    k_base_ptr = k_ptr + b * stride_kb + h * stride_kh\n    v_base_ptr = v_ptr + b * stride_vb + h * stride_vh\n    o_row_ptr = o_ptr + b * stride_ob + h * stride_oh + pid_n * stride_on\n\n    d_offsets = tl.arange(0, BLOCK_D)\n    q_vec = tl.load(q_row_ptr + d_offsets * stride_qd, mask=d_offsets < D, other=0.0)\n    q_vec = q_vec.to(tl.float32)\n\n    # Streaming softmax variables\n    m_i = tl.full((), -float('inf'), tl.float32)\n    l_i = tl.zeros((), dtype=tl.float32)\n    o_acc = tl.zeros([BLOCK_D], dtype=tl.float32)\n\n    # Loop over key/value blocks\n    for k_start in range(0, Nk, BLOCK_K):\n        k_offsets = k_start + tl.arange(0, BLOCK_K)\n\n        # Load K block [BLOCK_K, BLOCK_D]\n        k_ptrs = k_base_ptr + k_offsets[:, None] * stride_kn + d_offsets[None, :] * stride_kd\n        k_block = tl.load(k_ptrs, mask=(k_offsets[:, None] < Nk) & (d_offsets[None, :] < D), other=0.0)\n        k_block = k_block.to(tl.float32)\n\n        # Compute scores: [BLOCK_K]\n        # s_j = sum_d Q[d] * K[j, d]\n        s = tl.sum(k_block * q_vec[None, :], axis=1)\n        s = s * scale\n\n        # Apply causal mask if needed\n        if is_causal != 0:\n            valid_keys = k_offsets <= pid_n\n            s = tl.where(valid_keys, s, -float('inf'))\n\n        # Also mask out keys beyond Nk\n        s = tl.where(k_offsets < Nk, s, -float('inf'))\n\n        # Compute max and update streaming softmax\n        max_s = tl.max(s, axis=0)\n        m_new = tl.maximum(m_i, max_s)\n        exp_s = tl.exp(s - m_new)\n        alpha = tl.exp(m_i - m_new)\n        l_i = l_i * alpha + tl.sum(exp_s, axis=0)\n\n        # Load V block and update output accumulator\n        v_ptrs = v_base_ptr + k_offsets[:, None] * stride_vn + d_offsets[None, :] * stride_vd\n        v_block = tl.load(v_ptrs, mask=(k_offsets[:, None] < Nk) & (d_offsets[None, :] < D), other=0.0)\n        v_block = v_block.to(tl.float32)\n\n        weighted_v = tl.sum(v_block * exp_s[:, None], axis=0)\n        o_acc = o_acc * alpha + weighted_v\n\n        m_i = m_new\n\n    # Normalize output\n    # Handle potential l_i == 0 to avoid NaN\n    inv_l_i = tl.where(l_i > 0, 1.0 / l_i, 0.0)\n    o_out = o_acc * inv_l_i\n\n    # Store\n    tl.store(o_row_ptr + d_offsets * stride_od, o_out, mask=d_offsets < D)\n\n\n_sdpa_triton_kernel = scaled_dot_product_attention\n\n\ndef scaled_dot_product_attention(*args, **kwargs):\n    # Parse inputs following torch.nn.functional.scaled_dot_product_attention signature\n    # Args: query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None\n    if len(args) >= 3:\n        q = args[0]\n        k = args[1]\n        v = args[2]\n        attn_mask = args[3] if len(args) > 3 else kwargs.get('attn_mask', None)\n        dropout_p = args[4] if len(args) > 4 else kwargs.get('dropout_p', 0.0)\n        is_causal = args[5] if len(args) > 5 else kwargs.get('is_causal', False)\n        scale = args[6] if len(args) > 6 else kwargs.get('scale', None)\n    else:\n        q = kwargs.get('query', None)\n        k = kwargs.get('key', None)\n        v = kwargs.get('value', None)\n        attn_mask = kwargs.get('attn_mask', None)\n        dropout_p = kwargs.get('dropout_p', 0.0)\n        is_causal = kwargs.get('is_causal', False)\n        scale = kwargs.get('scale', None)\n\n    # Fallback conditions\n    fallback = False\n    if q is None or k is None or v is None:\n        fallback = True\n    else:\n        if q.ndim != 4 or k.ndim != 4 or v.ndim != 4:\n            fallback = True\n        elif q.shape[0] != k.shape[0] or q.shape[1] != k.shape[1] or k.shape[2] != v.shape[2] or k.shape[3] != v.shape[3] or q.shape[3] != k.shape[3]:\n            fallback = True\n        elif attn_mask is not None:\n            fallback = True\n        elif dropout_p is not None and float(dropout_p) > 0.0:\n            fallback = True\n        elif q.device.type != 'cuda' or k.device.type != 'cuda' or v.device.type != 'cuda':\n            fallback = True\n\n    if fallback:\n        return torch.nn.functional.scaled_dot_product_attention(*args, **kwargs)\n\n    B, H, Nq, D = q.shape\n    Nk = k.shape[2]\n\n    # Choose block sizes; require D <= BLOCK_D\n    BLOCK_K = 128\n    BLOCK_D = 128\n    if D > BLOCK_D:\n        return torch.nn.functional.scaled_dot_product_attention(*args, **kwargs)\n\n    # Default scale if not provided\n    if scale is None:\n        scale = 1.0 / math.sqrt(D)\n    scale = float(scale)\n\n    # Output tensor\n    o = torch.empty_like(q)\n\n    # Cast compute dtype to float32 internally; output dtype matches q.dtype\n    # We will perform accumulation in float32 in the kernel and store to o.dtype\n\n    # Prepare strides (in elements)\n    stride_qb, stride_qh, stride_qn, stride_qd = q.stride()\n    stride_kb, stride_kh, stride_kn, stride_kd = k.stride()\n    stride_vb, stride_vh, stride_vn, stride_vd = v.stride()\n    stride_ob, stride_oh, stride_on, stride_od = o.stride()\n\n    grid = (Nq, B * H)\n    _sdpa_triton_kernel[grid](\n        q, k, v, o,\n        B, H, Nq, Nk, D,\n        stride_qb, stride_qh, stride_qn, stride_qd,\n        stride_kb, stride_kh, stride_kn, stride_kd,\n        stride_vb, stride_vh, stride_vn, stride_vd,\n        stride_ob, stride_oh, stride_on, stride_od,\n        scale,\n        int(bool(is_causal)),\n        BLOCK_K=BLOCK_K,\n        BLOCK_D=BLOCK_D,\n    )\n\n    return o\n",
        "has_code": true,
        "best_speedup_vs_flaggems": 0.013996540704149902,
        "meets_threshold": false
      }
    },
    "only_batch1": 67,
    "only_batch2": 24,
    "both_batches": 0
  }
}