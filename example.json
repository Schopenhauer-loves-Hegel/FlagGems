{
"op_name": "aten::huber_loss",
"success": true,
"traceback": null,
"params": {
    "delta": 2.0,
    "reduction": 2,
    "dtype": "torch.bfloat16",
    "shape": [
    512,
    512
    ]
},
"speedup": null,
"info": {
    "total": 162,
    "failed": 0,
    "success": 162
},
"code": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef huber_loss_element_kernel(\n    x_ptr,    # pointer to input tensor (broadcasted, contiguous, flattened)\n    y_ptr,    # pointer to target tensor (broadcasted, contiguous, flattened)\n    out_ptr,  # pointer to output tensor (contiguous, flattened)\n    n_elements,  # number of elements\n    delta,       # huber delta (scalar)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    diff = x - y\n    absdiff = tl.abs(diff)\n\n    # loss = 0.5 * diff^2 if absdiff <= delta else delta * (absdiff - 0.5 * delta)\n    loss_quad = 0.5 * diff * diff\n    loss_linear = delta * (absdiff - 0.5 * delta)\n    loss = tl.where(absdiff <= delta, loss_quad, loss_linear)\n\n    tl.store(out_ptr + offsets, loss, mask=mask)\n\n\n@triton.jit\ndef reduce_sum_kernel(\n    x_ptr,     # pointer to input tensor (contiguous, flattened)\n    out_ptr,   # pointer to single scalar (float32) to accumulate sum into\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    vals = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    vals_f32 = vals.to(tl.float32)\n    partial_sum = tl.sum(vals_f32, axis=0)\n    tl.atomic_add(out_ptr, partial_sum)\n\n\ndef _normalize_reduction(reduction):\n    if isinstance(reduction, str):\n        r = reduction.lower()\n        if r == 'none':\n            return 0\n        elif r == 'mean':\n            return 1\n        elif r == 'sum':\n            return 2\n        else:\n            raise ValueError(f\"Unsupported reduction: {reduction}\")\n    elif isinstance(reduction, int):\n        if reduction in (0, 1, 2):\n            return reduction\n        else:\n            raise ValueError(f\"Unsupported reduction: {reduction}\")\n    else:\n        raise ValueError(f\"Unsupported reduction type: {type(reduction)}\")\n\n\ndef huber_loss(input, target, reduction=1, delta=1.0):\n    reduction = _normalize_reduction(reduction)\n    if not (input.is_cuda and target.is_cuda):\n        raise AssertionError(\"Triton kernels require CUDA tensors\")\n    device = input.device\n    # Promote dtype similar to PyTorch type promotion rules\n    result_dtype = torch.result_type(input, target)\n\n    # Broadcast tensors to a common shape\n    x_b, y_b = torch.broadcast_tensors(input.to(result_dtype), target.to(result_dtype))\n    x_b = x_b.contiguous()\n    y_b = y_b.contiguous()\n    numel = x_b.numel()\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(numel, meta['BLOCK_SIZE']),)\n\n    if reduction == 0:  # 'none'\n        out = torch.empty_like(x_b, dtype=result_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, out, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        return out\n    else:\n        # Compute elementwise loss into a temporary buffer\n        tmp = torch.empty_like(x_b, dtype=result_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        # Reduce to scalar using float32 accumulator\n        acc = torch.zeros((), dtype=torch.float32, device=device)\n        reduce_sum_kernel[grid](tmp, acc, numel, BLOCK_SIZE=BLOCK_SIZE)\n        if reduction == 1:  # mean\n            val = (acc / numel).to(result_dtype)\n        else:  # sum\n            val = acc.to(result_dtype)\n        return val\n\n\ndef huber_loss_out(input, target, reduction=1, delta=1.0, out=None):\n    if out is None:\n        raise ValueError(\"huber_loss_out requires an 'out' tensor\")\n    reduction = _normalize_reduction(reduction)\n    if not (input.is_cuda and target.is_cuda and out.is_cuda):\n        raise AssertionError(\"Triton kernels require CUDA tensors\")\n\n    device = input.device\n    # Determine result dtype; use out.dtype if provided to match .out behavior\n    # but ensure it's compatible with promoted dtype\n    promoted_dtype = torch.result_type(input, target)\n    result_dtype = out.dtype\n\n    # Broadcast tensors\n    x_b, y_b = torch.broadcast_tensors(input.to(promoted_dtype), target.to(promoted_dtype))\n    x_b = x_b.contiguous()\n    y_b = y_b.contiguous()\n    numel = x_b.numel()\n\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(numel, meta['BLOCK_SIZE']),)\n\n    if reduction == 0:  # 'none'\n        # Ensure out has correct shape\n        if out.numel() != numel or out.shape != x_b.shape:\n            raise ValueError(f\"'out' tensor must have shape {tuple(x_b.shape)} for reduction='none'\")\n        # Compute into a temporary if out is not contiguous or dtype mismatches\n        needs_tmp = (not out.is_contiguous()) or (out.dtype != result_dtype)\n        if needs_tmp:\n            tmp = torch.empty_like(x_b, dtype=result_dtype, device=device)\n            huber_loss_element_kernel[grid](x_b.to(result_dtype), y_b.to(result_dtype), tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n            out.copy_(tmp)\n        else:\n            huber_loss_element_kernel[grid](x_b.to(result_dtype), y_b.to(result_dtype), out, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        return out\n    else:\n        # Compute elementwise loss into temporary (in promoted dtype), then reduce to scalar\n        tmp = torch.empty_like(x_b, dtype=promoted_dtype, device=device)\n        huber_loss_element_kernel[grid](x_b, y_b, tmp, numel, float(delta), BLOCK_SIZE=BLOCK_SIZE)\n        acc = torch.zeros((), dtype=torch.float32, device=device)\n        reduce_sum_kernel[grid](tmp, acc, numel, BLOCK_SIZE=BLOCK_SIZE)\n        if reduction == 1:  # mean\n            val = (acc / numel).to(result_dtype)\n        else:  # sum\n            val = acc.to(result_dtype)\n        # Ensure out is scalar/0-d\n        if out.numel() != 1 or out.dim() > 1:\n            raise ValueError(\"For reduction='mean' or 'sum', 'out' must be a scalar (0-d or 1-element) tensor\")\n        # Copy the scalar value into out\n        out.copy_(val)\n        return out",
"test_func": "import flagbench\nfrom sandbox.config import DEVICE as device\nfrom sandbox.verifier.test_parametrize import parametrize, label\nfrom sandbox.utils.accuracy_utils import gems_assert_close as assert_close\nfrom sandbox.utils.accuracy_utils import to_reference\nfrom sandbox.register import REGISTERED_OPS\nimport torch\n\n@label(\"huber_loss\")\n@parametrize(\"shape\", [(2, 3), (128, 256), (512, 512)])\n@parametrize(\"dtype\", [torch.float32, torch.float16, torch.bfloat16])\n@parametrize(\"reduction\", [0, 1, 2])\n@parametrize(\"delta\", [0.5, 1.0, 2.0])\ndef test_huber_loss_tensor(shape, dtype, reduction, delta):\n    self_tensor = torch.randn(shape, dtype=dtype, device=\"cuda\")\n    target_tensor = torch.randn(shape, dtype=dtype, device=\"cuda\")\n\n    ref_self = self_tensor.clone()\n    ref_target = target_tensor.clone()\n    ref_out = torch.ops.aten.huber_loss(ref_self, ref_target, reduction, float(delta))\n\n    with flagbench.use_gems(REGISTERED_OPS):\n        act_self = self_tensor.clone()\n        act_target = target_tensor.clone()\n        act_out = torch.ops.aten.huber_loss(act_self, act_target, reduction, float(delta))\n\n    assert_close(act_out, ref_out, dtype=dtype)\n\n\n@label(\"huber_loss\")\n@parametrize(\"shape\", [(2, 3), (128, 256), (512, 512)])\n@parametrize(\"dtype\", [torch.float32, torch.float16, torch.bfloat16])\n@parametrize(\"reduction\", [0, 1, 2])\n@parametrize(\"delta\", [0.5, 1.0, 2.0])\ndef test_huber_loss_out(shape, dtype, reduction, delta):\n    self_tensor = torch.randn(shape, dtype=dtype, device=\"cuda\")\n    target_tensor = torch.randn(shape, dtype=dtype, device=\"cuda\")\n\n    if reduction == 0:\n        out_shape = shape\n    else:\n        out_shape = ()\n\n    ref_self = self_tensor.clone()\n    ref_target = target_tensor.clone()\n    ref_out = torch.empty(out_shape, dtype=dtype, device=\"cuda\")\n    torch.ops.aten.huber_loss.out(ref_self, ref_target, reduction, float(delta), out=ref_out)\n\n    with flagbench.use_gems(REGISTERED_OPS):\n        act_self = self_tensor.clone()\n        act_target = target_tensor.clone()\n        act_out = torch.empty(out_shape, dtype=dtype, device=\"cuda\")\n        torch.ops.aten.huber_loss.out(act_self, act_target, reduction, float(delta), out=act_out)\n\n    assert_close(act_out, ref_out, dtype=dtype)"
}